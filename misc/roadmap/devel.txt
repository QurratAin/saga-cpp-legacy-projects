
SAGA-C++ Development Roadmap
----------------------------

 This document tries to lay the basis of a coordinated evolution
 of the SAGA-C++ implementation (including, and limited to,
 saga-core, its python bindings and C++ adaptors).  The document
 ignores several issues which are closely related to the code
 development, such as deployment, documentation, standardization
 and higher level tools -- those are discussed in separate
 documents.


 Status Oct 2011
 ---------------

  The SAGA C++ implementation has reached some maturity, and
  seems to be usable for real world projects.  It is in the
  process of getting globally deployed on several major DCIs,
  which will likely increase uptake.  The SAGA-C++ usage has
  focused on a relatively small subset of adaptors (globus, ssh,
  default, condor, pbs), with a small number of additional ones
  expected growth (gLite, BES).  Those adaptors seem to
  stabilize, most other adaptors have to be classified as
  experimental or dead.  The Python bindings over C++ have seen
  significant usage.

  The used IT infrastructure (repository, devel list, web site
  and ticketing system) are all hosted at CCT/LSU, but are
  expected to get migrated to Rutgers or elsewhere within the
  next couple of months.  That infrastructure seems to work
  reliable, but has some annoyances and deficiencies.

  The development team at the moment consists of 2.x core
  developers (Ole, Andre, Hartmut), and a relatively large set
  of adaptor developers (Students, Naregi, Steve, Hans).  The
  team is slowly but surely growing, but people outside of the
  core team tend to focus on specific modules.  Hartmut is
  slowly drifting off, development-wise.

  

 Potential for Evolution
 -----------------------

  Up to now, the SAGA evolution has been driven by releases,
  which, in general, targetet specific use cases or
  environments.  Long-term evolution has been somewhat neglected
  though, as the procedure only allowed to plan from
  release to release -- many tickets have been moved from one
  release to the next, even if marked 'Urgent', because they did
  not fit the specific release targets.  As a result, parts of
  the code base are very well maintained, and others fell behind
  for years.

  The SAGA-C++ implementation can, in principle, evolve in
  several aspects:

    - functional evolution:
      - additional API packages
      - additional adaptors
      - added engine functionality
      - API evolution (fix C++ API, evolve Python API)
    - non-functional 
      - code quality / stability / reliability / documentation
      - development tooling, build and packaging support
      - code and backend testing

  As the size of the devel team is restricted, a prioritization
  of those options (and on more fine grained sub-items) is
  required, to have any chance of a coordinated evolution.  It
  is difficult to see how that prioritization can be decoupled
  from the currently reigning release schedule, but that attempt
  must be made to allow for a more balanced code base evolution.


 Prioritization
 --------------

  [AM: Now, this section is where my bias will show, so please 
       feel free to discuss! ;-)]

  SAGA-C++ is the one and only C++ implementation of the SAGA
  API, and as such defines the C++ language bindings.  Several
  features of the API have not yet been (fully) implemented, due
  to the lack of prioritization.  One could argue that those
  features have then obviously not been missed, but (a) the set
  of long-standing open API level tickets suggests otherwise,
  and (b) it is a chicken-and-egg problem: users will rarely
  complain about missing specifica if they do not know those
  specifica could exist.  
  
  What we *do* see is that some groups stop using SAGA (most
  importantly BigJob), and one needs to understand the reason
  for that.  It can be claimed that the reasons are

    - lacking / insufficient support of asynchronous operations
    - performance and scalability problems
    - unsuitable programming abstractions
    - complex deployment dependencies and procedure

  Some of those issues are already being addressed (fast-advert, 
  binary packaging), others are not yet on the horizon (engine
  support for better async ops, additional programming
  abstractions like messaging and resource lifetime).

  We need to understand the above deficiencies in detail,
  and address them individually and explicitely -- the next
  subsections are an attempt to do so.

  
  Asynchronous Operations
  -----------------------

   The SAGA API comes with support for asynchronous operations
   for all remote calls, and with notifications for all stateful
   entities.  Those features are supposed to help the
   application programmer with latency hiding and event-driven
   execution models, and are also supposed to improve
   application scalability charcteristics (see task_container).

   As very few backends support asynchronous operations
   natively, the SAGA-C++ implementation provides the ability to
   wrap synchronous operations into threads, and to present them
   thus as asynchronous operations to the programmer, in a way,
   which is semantically similar to futures. That seems to work
   very well, although some use cases (see digedag) hit local
   resource limitations if the number of threads is becoming too
   large (e.g. >256 on default Linux kernels).

   SAGA's task container is supposed to support the management
   of collections of asycnhronous operations (aka tasks), but,
   at the moment, misses a number of features, such as a
!  usable collective wait(), so that it is not really usable,
   and in fact is rarely used at all.

   Related to asynchronous operations, the SAGA API supports
   event notification.  Similar to above, very few backends
   support notifications natively -- but other than for tasks,
   the engine support for implementing notifications on adaptor
   level is in fact missing.  As a result, almost no adaptor
   implements notification, even if the respective information
   are readily available on that level.


  Programming abstractions
  ------------------------

   The by-far most used SAGA API package is the job package,
   which seems to work stable and to solve most job related use
   cases -- with the notable exception of not being able to
   sensibly handle resources with finite lifetime, such as
   virtual machine instances.  As those use cases are becoming
   ever more prominent, that semantic gap needs to be closed,
   and the job API needs to be extended by the capability to
   manage and observe resource lifetime and state.  Note that a
   draft specification of a resource API exists, which is in
   fact backward compatible to the current job API, but
   implementing that package is as of now not a priority for the
   SAGA development group.
   
   Also very widely used is SAGA's advert package, which
   provides relative simple but powerful means to coordinate
   distributed application components.  It seems, however, that
   the advert API is sometimes misused as messaging API, which
   works, but has severe performance and scalability
   implications.  This indicates that a message abstraction is
   missing, and in fact, BigJob has recently replaced its advert
   API based cooordination model with a (non-SAGA based)
   messaging model (ZeroMQ).  Note that a message API
   specification exists since about 2 years, but an
   implementation of that API is not yet a priority of the SAGA
   development group.
   
   In general, it seems that the SAGA project (not necessarily
   the SAGA-C++ implementation) should strive to provide higher
   level programming abstractions -- the currently exposed ones
   are still relatively close to the middleware abstraction, and
   do not fully reflect application level semantics.  In
   particular, data management (not transport!) abstractions are
   missing, but also support for workflows, bag of tasks, etc.


  Deployment dependencies and procedures
  --------------------------------------

   The SAGA-C++ dependency is, in hindsight, unfortunate: while
   being very powerful, the real-life obstacles to interoperate
   with the variety of used boost deployments are relatively
   high.  Also, the user-level experience of compiling,
   deploying and using boost is, according to thye feedback we
   receive, demitrary to the experience *S*AGA tries to deliver.

   Additionally, the fact that SAGA-C++ consists of a number of
   inter-dependent modules is also complicating compile and
   deployment.  Finally, the ever failing unit tests make a bad
   first impression for any user who cares to run it.

   While there are no short term options for removing the boost
   dependency, the devel group should try to mitigate its
   effects.  In particular it should be reconsidered to include
   the required boost elements into the distribution, possibly
   in a separate namespace.  Another option is to statically
   link against boost libraries, to at least resolve the runtime
   problems.  Either way, one needs to continue to support the
   numberous and ever growing number of incompatible boost
   releases which can be found on real world systems.

   the current approach, to pre-install SAGA on major DCIs, and
   to provide binary packages for the major linux distributions,
   seems to be promising, but it remains to be seen if those
   will increase uptake, and reduce dependency problems.

 
  Performance and scalability
  ---------------------------

   First, as asynchronous operations and notification are, to a
   very large part, a performance optimization, the discussion
   of those items above also impact the expected application
   level performance discussed here.

   In general it must, however, be noted that the SAGA API is a
   relatively lean API, in that it does not add significant
   semantics to the backend operation, and thus should not add
   significant runtime overhead either (see hpdc09 paper).
   Several aspects of the SAGA-C++ architecture, however, can
   potentially increase call latency, sometimes significantly.

   Relatively obvious is the latency implication of the
   late-binding approach: failing adaptors add to the latencies.
   That problem is mostly irrelevant though, as the implemented
   adaptor resorting prefers successful adaptors, and the
   additional latency should only apply once, in general.
   Moreover, most application seem to use complete URL schemas,
   so that late binding can mostly use the correct adaptor even
   on the first call (but some schemas are overused, such as
   http).

   Additional latencies can also arise due to, for example, the
   two stage job submission (service creates job instance, job
   instance is then run, i.e. submitted).  Such latencies can be
   optimized away on adaptor level, for example by keeping
   instance specific state information, but there is no support
   on engine level, so such mechanisms are not consistenly
   used.  In general however it seems that engine level
   latencies are mostly negligable.

   So, performance is *mostly* (but not exclusively, as
   discussed) caused by backend interaction.  In some cases, it
   seems that the SAGA API semantics resuires more backend
   communication hops than the native backend libraries -- but
   it is hard to see how that could be resolved without breaking
   the abstraction layers, and without tuning the API semantic
   to a specific set of backends.  In other cases, it is the
   backend latency itself which is imposing the overhead.  As
   the SAGA development team has, in the general case, no
   influence over backend implementations, that cannot be helpd.
   The group should, however, strive to understand the sources
   of operation latencies, and should document the performance
   of the various backends, for example by providing the results
   of stress tests.  If the group can influence backend
   implementations, or has the choice between different backend
   access mechanisms or libraries, it should strive hard to
   improve the performance.

   There seems to be some unclarity on what level of caching is
   applicable for the backend adaptors, without breaking API
   semantics.  The general rule holds that read caches are not
   problematic *at* *all*, as those behave, in the worst case,
   like extended communication latencies -- the API caters for
   that.  As a prinziple, cache life times should not exceed the
   communication channel's latencie, as order of magnitiude.
   Read caches should be exploited wherever possible!

   Write caches are much more problematic, as they require an
   additional consistency protocol.  Also, the API expects state
   changes to happen before an API call returns, and does not
   cater for delayed error messaging.  Write caches should thus
   not be used.  If write performance is becoming a problem for
   specific applications, it should be addressed in application
   space, by either empoying SAGA's bulk operations, or by using
   asynchronous operations and notifications.  If the problem
   appears to apply to many application use cases, the API
   semantics needs to be revised -- that appears not to be the
   case for the time being.


 SAGA Development Roadmap
 ------------------------

   From the discussion and recommendations above, one can derive
   the following development roadmap.  Note that the
   prioritization is subjective, and reflects the authors
   experience and plans ;-)


   -   T.0:   implement remote unit and stress testing
   -   T.1:   fix the engine deficiencies
     - T.2.1:   task container
     - T.2.2:   notification polls (or find a different way)
     - T.2,3:   collection of results
   -   T.2:   add message  package
   -   T.3:   add resource package
   -   T.4:   improve advert service performance (stress tests!)
   -   T.5:   reduce boost dependencies, at compile and runtime
   -   T.6:   support associated developer groups by providing higher
   -   T.7:   level abstractions over SAGA
   -   T.8:   only then address other 'obvious' functional tasks:
     - T.8.1:   add scp adaptor
     - T.8.2:   add GlobusOnline adaptor
     - T.8.3:   add Genesis-II GFFS adaptor
     - T.8.4:   add message  zmq    adaptor
     - T.8.5:   add resource occi   adaptor
     - T.8.6:   add resource drmaa2 adaptor   


