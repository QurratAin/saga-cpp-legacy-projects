\documentclass{rspublic}   

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
%\pagestyle{empty}

%\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}    
\usepackage{multirow}    
\usepackage{listings}   
\usepackage{times}     
\usepackage{paralist}    
\usepackage{wrapfig}    
\usepackage[small,it]{caption}
\usepackage{multirow}
\usepackage{ifpdf}   
\usepackage{subfigure} 

                    
%Bibliography                     
\usepackage{natbib}   

\usepackage{listings}
\usepackage{keyval}  
\usepackage{color}
\definecolor{listinggray}{gray}{0.95}
\definecolor{darkgray}{gray}{0.7}
\definecolor{commentgreen}{rgb}{0, 0.4, 0}
\definecolor{darkblue}{rgb}{0, 0, 0.4}
\definecolor{middleblue}{rgb}{0, 0, 0.7}
\definecolor{darkred}{rgb}{0.4, 0, 0}
\definecolor{brown}{rgb}{0.5, 0.5, 0}



\title[Efficient Replica-Exchange Simulations on
  Large-Scale Production Infrastructure]{Efficient Replica-Exchange Simulations on
  Large-Scale Production Infrastructure}

\author[Thota, Luckow, Jha]{
  Abhinav Thota$^{1,2}$, Andr\'e Luckow$^{1}$ and Shantenu Jha$^{1,2,3}$\\
  \small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State University, Baton Rouge, LA 70803, USA}}\\
  \small{\emph{$^{2}$Department of Computer Science, Louisiana State
      University, Baton Rouge, LA 70803, USA}}\\
  \small{\emph{$^{3}$e-Science Institute, Edinburgh EH8 9AA, UK}}\\
}

%\date{}

\def\acknowledgementname{Acknowledgements}
\newenvironment{acknowledgement}%
{\section*{\acknowledgementname}%
\parindent=0pt%
}

\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***shantenu: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***andre: #1 }}}
\newcommand{\athotanote}[1]{ {\textcolor{green} { ***athota: #1 }}}
\else
\newcommand{\alnote}[1]{}
\newcommand{\athotanote}[1]{}
\newcommand{\jhanote}[1]{}
\fi

\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\T}[1]{\texttt{#1}}

\newcommand{\glidein}[1]{Glide-In }  
\newcommand{\replicaagent}[1]{Replica-Agent }         
\newcommand{\remanager}[1]{RE-Manager }

\begin{document} 


\maketitle    

\begin{abstract}{Replica-Exchange, SAGA, Large-Scale, Production}  

  Developing applications that are able to orchestrate heterogeneous
  resources across distributed resources is a complex
  task. Inevitably, the design and development of an application is
  influenced and constrained by the programming systems and the
  infrastructure it is developed against. Breaking this coupling
  between the development and the underlying infrastructure, to enable
  applications to be flexible (across infrastructure), extensible (to
  new methods of communication and coordination) and scalable is an
  important design objective of both logically and physically
  distributed applications.  In this work, we focus on the
  Replica-Exchange (RE) Methods which represent a class of algorithms
  that involve a large number of loosely-coupled ensembles. RE
  simulations are used to understand physical phenomena Ð ranging from
  protein folding dynamics to binding affinity calculations.  In this
  work, we develop a flexible, extensible and scalable implementation
  of RE that can utilise a range of infrastructure concurrently
  \jhanote{(and autonomically/adaptively): Do we?}, that supports
  different coordination mechanisms (publish-subscribe \jhanote{do
    we?}, centralised notification), different replica pairing
  mechanisms (synchronous versus asynchronous) and thereby different
  variants of the RE algorithm. We implement and demonstrate how a
  flexible and robust implementation enables the efficient use of a
  broad range of infrastructure.

% Developing applications that are able to orchestrate heterogeneous
% resources across distributed resources is a complex task. Inevitably,
% the design and development of an application is influenced and
% constrained by the programming systems and the infrastructure it is
% developed against. Breaking this coupling between the development and
% the underlying infrastructure, to enable applications to be flexible
% (across infrastructure), extensible (to new methods of communication
% and coordination) and scalable is an important design objective of
% distributed applications both logically distributed and physically
% distributed.  In this work, we focus on the Replica-Exchange (RE)
% Methods which represent a class of algorithms that involve a large
% number of loosely-coupled ensembles. RE simulations are used to
% understand physical phenomena Ð ranging from protein folding dynamics
% to binding affinity calculations.  In this work, we develop a
% flexible, extensible and scalable implementation of RE that can
% utilise a range of infrastructure concurrently (and
% autonomically/adaptively), that supports different coordination
% mechanisms (publish/subscribe, centralized notification), different
% replica pairing mechanisms (synchronous versus asynchronous) and
% thereby different variants of the RE algorithm. We implement and
% demonstrate how a flexible and robust implementation enables the
% efficient use of a broad range of infrastructure.

\end{abstract}

\jhanote{please use \citep{} and not \cite{}}

\section{Introduction}
Developing applications that are able to orchestrate heterogeneous
resources across distributed resources is a complex task.  Inevitably,
the design and development of an application is influenced and
constrained by the programming systems and the infrastructure it is
developed against. Breaking this coupling between the development and
the underlying infrastructure, to enable applications to be flexible
(across infrastructure), extensible (to new methods of communication
and coordination) and scalable is an important design objective of
distributed applications -- both logically distributed and physically
distributed.

In this work, we focus on the Replica-Exchange
(RE)~\citep{hansmann,Sugita:1999rm} methods -- which represent a class
of algorithms that involve a large number of loosely-coupled
ensembles.  RE simulations are used to understand physical phenomena
-- ranging from protein folding dynamics to binding affinity
calculations. Most RE implementations are either infrastructure
specific (Woods et al. 2005) or, if using multiple distributed
resources, they require prior co-scheduling (Manos et
al. 2008). ~\citep{Luckow:2008fp} takes it to the next level and is an
example of adaptive RE simulations on production-level grid resources,
while ~\citep{parashar_arepex} is an example of \emph{asynchronous} RE
simulations, which is based on
CometG~\citep{Li:2005:CSC:1090948.1091381}, a decentralised
computational infrastructure for Desktop Grid environments.

We develop a flexible, extensible and scalable implementation of RE
that can utilize a range of infrastructure concurrently that supports
different coordination mechanisms (publish-subscribe, centralised
notification), different replica pairing mechanisms (synchronous
versus asynchronous) and thereby different variants of the RE
algorithm.  Application formulations that are scalable while being
flexible and extensible are better suited to using the diverse range
of traditional and hybrid infrastructure (e.g., Grid-Cloud and
heteregeneous resources).  

Along with application formulations that facilitate the flexible
utilisation of a range of infrastructure, it is imperative to have the
correct run-time abstractions that support flexible deployment of
these applications.  Consequently, we utilize a flexible pilot-job
implementation (SAGA BigJob) to support the execution of our
formulations.  We implement and demonstrate how a flexible and robust
implementation enables the efficient use of a broad range of
infrastructure. We compare and analyze the performance of the
different RE models (synchronous and asynchronous) when we scale-up to
256 replicas and scale-out to 4 machines. We present results using
which the reader can understand which model of RE is most suited for a
particular set of resources (distributed, local etc.,)

The rest of the paper is organized as follows. Section II sketches out
the different RE algorithms that we investigate. Section III outlines
the architecture of the SAGA BigJob and how it supports the dynamic
execution of multiple replicas (ensembles). In Section IV, we present
the implementation details of the RE algorithms and understand
the primary determinants of peformance; we also present
an approximate mathematical model for the different algorithms.
% We also present
% different implementations of the asynchronous RE algorithm. 
In Section V, we describe the experiments performed with a view to
understanding the performance when scaling-up (on a single machine) as
the number of replicas increases. Section V also investigates the
scaling-out characteristics, namely performance as the number of
replicas are increased while the (distributed) resources employed
increases. %  while keeping the number of replicas on local and
% distributed resources. 
\jhanote{Currently we have only Section 6 and no 7}. In Section VI, we
present the results and analysis.  Section VII concludes the paper and
discusses future work.

% \alnote{Should we add a section with some scientific background: HIV,
%   Hepatitis...?}  \jhanote{given the tightness of space, I think we
%   should try to avoid it. OK?}

\section{Replica-Exchange Approach}
\label{sec:repex-approach}
The RE algorithm involves the concurrent execution of multiple similar
simulations, the \emph{replicas}.  There is a loose-coupling between
the replicas in form of periodic exchange attempts between paired
replicas. The traditional approach to RE is the synchronous model,
which works well in an ideal scenario with a well defined model of
resource availability. But with heterogenous systems and fluctuating
resource availability, the asynchronous RE model could be more
effectively used to conduct simulations. Here we first provide a basic mathematical model for different RE models which explains the terms involved.


%Previously, we demonstrated the usage of the SAGA Pilot-Job
%framework~\citep{saga_bigjob_condor_cloud} -- called the BigJob, to run
%RE simulations across multiple, heterogeneous distributed Grid and
%Cloud infrastructures~\citep{Luckow:2008fp}.
%\alnote{maybe we should also intro SAGA at some point} \jhanote{Yes} The Simple API for Grid Applications (SAGA)~\citep{saga_gfd90} is an API standardization effort within the Open Grid Forum (OGF)~\citep{ogf_web}, an international standards development body concerned primarily with standards for distributed computing. The various tasks that are carried out using the SAGA APIs include file staging, job spawning and the conduction of the exchange attempts.
%Further, we introduced several adaptivity modes, e.\,g.\ adaptive
%sampling that are able to react to dynamic changes in resource
%availabilities.

%\alnote{Not sure how many technical we need to provide...}  

%Traditionally, depending
%on the number of processes \texttt{N}, the manager creates \texttt{N/2} pairs
%of replicas.  Before launching a job, the manager ensures that all
%required input files are transferred to the respective resource. For
%this purpose, the SAGA File API and the GridFTP adaptor are used. The
%replica jobs are then submitted to the resource using the SAGA CPR
%API and the MIGOL/GRAM middleware.

\jhanote{Mention that these are SAGA-based implementations. Something
  else would be implemented differently}
\alnote{Proposed structure: a) math. model b) sync c) async. We should give the reader some orientation in this section.}

\subsection{Mathematical Model}
\alnote{I think we should discuss the mathematical model before going into the specifics of sync/async RE.}

In this section, we aim to develop an equation for total time to run any RE simulation. If $t$ is the average time between successful exchanges, and $p$ is the probability of a successful exchange, 
\begin{eqnarray}
t=  {1 \over p} \times {[T_{MD} + T_{X} + T_{W}]} 
\label{eq:timebtw}
\end{eqnarray}

where $T_{X}$ is the time to make a pairwise exchange, which is comprised of (i) finding a partner, (ii) exchanging
states including file transfer, (iii) book-keeping and $T_{W}$ is the time spent waiting for all replicas to complete running (synchronous model), and (iv) (re)starting the replica.

Therefore, ${T_{X}} = {T_F + T_{ex} + T_{coord}}$ 

and for ${T_{X}}^{async}, T_W = 0$

The time ($T$) for N$_{X}$ exchanges is therefore $N_{X} \times t$

If there are $\eta$ independent exchange events occurring concurrently, then the time $T$ for 
N$_x$ exchanges is $T \over \eta$.

%Note $\eta_{sync}$ = 1 and $\eta_{async}$ = $N_R \over 2 $. \\


%Define $t$ to be the average time between successful exchanges

%Define $p$ to be the probability of an successful exchange

%$t=  {1 \over p} \times {[T_s+T_{MD} + T_{X} + T_{W}]} $

%where $T_{X}$ is comprised of (i) Finding a partner, (ii) exchanging
%states including file transfer, and (iii) book-keeping, status update
%et c. 

%Therefore ${T_{X}} = {T_F + T_{ex} + T_{coord}}$ 


%and for ${T_{X}}^{async}, T_W = 0$


%The time ($T$) for N$_{X}$ exchanges is therefore $N_{X} \times t$

%If there are $\eta$ independent exchange events then the time $T$ for 
%N$_x$ exchanges is $T \over \eta$.

%Note $\eta_{sync}$ = $N_R \over 2$ and $\eta_{async}$ = $N_R $. \\
\athotanote{I think $\eta_{sync,async-cent}$ should be 1 as only one exchange is done at a time(single master process) and $\eta_{async-decent}$ should be $N_R \over 2$, as each pair is involved in negotiating an exchange. since this depends on implementation, i am not including the $\eta$ values here.}
%\athotanote{I propose that we add  $T_{s}$ (time to start a replica) to $t$. would you agree?}
%\athotanote{one more thing: sync RE is slower because all the replicas are started one after the other at each exchange step. where as in cent RE, pairs of replicas are started after each exchange. how do we show this difference?}

%Note: $t_{sync} = {1 \over p} \times {[T_{MD} + {T_{X}}^{sync}]} $

%Note: $t = {1 \over p} \times {[T_{MD} + {T_{X}}]} $  \\


\subsection{Synchronous Replica Exchange}

\alnote{Are we already introducing cases in this sec?} \athotanote{nope, i don't think so!}
In the synchronous (traditional) model of RE, depending upon
on the number of replicas ${N_R}$, the manager creates ${N_R/2}$ pairs
of replicas. When the replicas reach a
pre-determined state (e.g. the NAMD job finishes after a fixed number
of steps), a decision as to whether to exchange temperatures between
previously paired replicas is determined using the Metropolis scheme.
The run of an ensemble of replicas concurrently and the subsequent
pairwise exchange attempt are referred to as generation. No two
replicas can belong to different generations. If the exchange attempt
is successful, parameters such as the temperature are swapped. Both
jobs are then relaunched~\citep{Luckow:2008fp}.

As mentioned in the equation (~\ref{eq:timebtw}), if $t$ is the average time between successful exchanges and $p$ the probability of a successful exchange, then:

$t =  {1 \over p} \times {[T_{MD} + T_{X} + T_{W}]}$

It should be noted that we are discussing the average values of each of the term in the equation above.
The time ($T$) for N$_{X}$ exchanges is therefore $N_{X} \times t$
If there are $\eta$ independent exchange events occurring concurrently, then the time $T$ for 
%N$_x$ exchanges is $T \over \eta$.

%Note $\eta_{sync}$ = 1.


%\alnote{Do we need $T_{s}$ or can we subsume it into $T_{x}$?}
%\athotanote{i think it matters when we vary the number of replicas. also, the exchange is done by the master, while the replica is started by the agent.}
%\alnote{We should point out that we are discussing averages.}%\athotanote{i am not sure, because we are only introducing the terms; we quantify the terms in section 4..?}
%where $T_{s}$ is the time required to start a replica, $T_{MD}$ is the time 
%it takes one replica to complete a particular number of time steps,
%and $T_{X}$ is comprised of (i) finding a partner, (ii) exchanging
%states including file transfer, and (iii) book-keeping: ${T_{X}} = {T_F + T_{ex} + T_{coord}}$ \alnote{We should either name itbook keeping or coordination.}\athotanote{done} $T_{W}$ is the time spent waiting for the completion
%of all other replicas at the end of each generation.

%The time-to-completion $T_{c}$ for N$_{X}$ exchanges is therefore: $N_{X} \times t$.
%If there are $\eta$ independent exchange events then the time $T$ for 
%N$_x$ exchanges is $T \over \eta$. 
\alnote{We should give a number for $\eta$ for sync RE}\athotanote{wouldn't that be implementation dependent?}
%Note $\eta_{sync}$ = $N_R \over 2$ and $\eta_{async}$ = $N_R $. \\


%The total time to completion ($T_{C}$) of the simulation
\jhanote{let us be consistent. Is the entire RE process/workflow a  simulation or is an invidivual replica/task a simulation? I prefer  the latter, in which case the T$_C$ is not of a simulation but that  of a RE run/work/process?}  %is the sum of two components; the first
%component is the simple sum of the following terms(?) (i) the time to
%launch the BigJobs \jhanote{BigJob has not been defined or explained
 % in the paper yet?!}  ($T_{L)}$, (ii) the queue wait time
%($T_{QW}$). \athotanote{i removed references to the BigJob here, but lost in conflict-resolution}These terms not dependent on the the specific case
%employed, or the the number of replicas and thus as a whole neither is
%the first component. In contrast, the second component contains terms,
%which in general are dependent on specific case and number of
%replicas.  The second component can be defined as
% the following components: (i) the time to launch the BigJobs
% \jhanote{BigJob has not been defined or explained in the paper
%   yet?!}  ($T_{L)}$, (ii) the queue wait time ($T_{QW}$), and
%the product of the following terms and the number of exchange steps
%($N_{X}$): (iii) the actual runtime of NAMD \jhanote{Why are we making
  %it NAMD specific. We really want to say simulation -- which could be
 % MD or MC, or NAMD or any other code} at each generation ($T_{MD}$),
%(iv) time spent waiting for all replicas to be done ($T_{W}$), (v) the
%time to carry out the exchanges at the end of each generation($T_{X}$), and (vi) the time required to restart the simulations after each exchange step ($T_{r}$). We only include steps (i) and (ii) for completeness, which occur only at the beginning of the simulation. Steps (iii) to (vi) repeat after each exchange. We consider the start time as the time at which the replicas are launched for the first time. We also think that it is safe to say that, irrespective of the implementation details, steps (iii) to (vi) are common to all replica-exchange simulations. \alnote{we should put  behind each step which $T_{x}$ we refer to} \jhanote{not sure if you  are referring to here (in text) or in the equation?} Therefore, the actual time to solution is the sum of (iii), (iv), (v) and (vi) multiplied by the number of exchange steps ($N_{X}$). So, the time to completion, without (i) and (ii) would be: \alnote{Why don't we  include $T_{L}$ and $T_{QW}$?}  \jhanote{Maybe because they dont  help understand the scaling behaviour and the fact none of these are  a function of the replica-exchange algorithms..}


% The total time to completion ($T_{C}$) of the simulation can be said
% to be the sum of the following components: \athotanote{(i) the time to
%   launch the BigJobs ($T_{L)}$, (ii) the queue wait time ($T_{QW}$) I
%   don't think I should include BJ/QW time here, as they depend on the
%   implementation. we were only supposed to talk about the algos here?}
% (i) the time to start the replicas ($T_{s}$), (ii) the actual runtime
% of NAMD at each generation ($T_{MD}$), (iii) time spent waiting for
% all replicas to be done ($T_{W}$), (iv) the time to make the exchanges
% at the end of each generation ($T_{X}$) and its product with the
% number of exchange steps ($N_{X}$).  As steps (i) to (iv) repeat after
% each exchange, we take the product of the sum of those steps with the
% number of exchange steps as the total time to completion. We consider
% the start time as the time at which the replicas are launched for the
% first time. We also think that it is safe to say that, irrespective of
% the implementation details, steps (i) to (iv) are present in a typical
% replica-exchange simulation. The equation for $T_{C}$ would then be:
% \alnote{we should put behind each step which $T_{x}$ we refer to}
% \jhanote{not sure if you are referring to here (in text) or in the
%   equation?}  \alnote{Why don't we include $T_{L}$ and $T_{QW}$?}
% \jhanote{Maybe because they dont help understand the scaling behaviour
%   and the fact none of these are a function of the replica-exchange
%   algorithms..}  

%\begin{eqnarray}
%T_{C} &=& [T_{s}+T_{MD} + T_{W} + T_{X}] \times N_{X}
%\label{eq:equation 1}
%\end{eqnarray}

%Here, the steps (v) and (vi) occur in a serial manner.
 
% 1 para limitation on traditional replica exchange
A major limitation of this model is that the replicas are paired in
fixed groups.  Exchanges can only take place between these paired
replicas.  This, while limiting the number of replicas which are
available for an exchange, also inhibits exchanges between replicas
with non-nearest temperatures. This also negates the possibility of
crosswalks. A crosswalk is said to occur when a replica originally
with a low temperature reaches the upper temperature range and then
returns to the lower temperature
range. %Moreover, the replicas are attached to their partners,
%sometimes waiting for them to complete while there are possibly other replicas available which are paired to their partners.
%This also reduces the number of exchanges that can take place within a given time.
Replica pairing works well in an ideal scenario but with heterogeneous
systems, where the resource availability and performance fluctuates,
it is far from ideal. It is important to have a scheme that does not
depend on a static  set of
resources that are pre-defined at the time of workload submission.
% availability, where the resources and the resource availability are
% well defined. 
This forms the motivation for coming up with a formulation that makes
it possible to run RE simulations that can dynamically utilise a range
of infrastructures.
 %\jhanote{The point is really the following: Paired-replicas are Ok if
  %it can be guaranteed that equal resources will be available, or the
  %resource availabilty can be predicted in advance. However, in
  %distributed systems, whereby definition, resource availability
  %fluctates it is important to have a scheme/implementation that does
  %not depend on a static, well-defined model of resource availability
  %and execution. This forms the motivation for coming up with a
  %formulation of a well known algorithm that makes it suitable for a
  %range of infrastruture.}



  
\subsection{Asynchronous Replica Exchange}
%- Introduce asynchronous Replica Exchange --  1 para on case II and case III (algorithmically)

%To overcome these limitations

We propose an asynchronous RE algorithm similar
to~\citep{parashar_arepex}, where a replica can perform exchanges
asynchronously with any other replica in the ensemble. Here by asynchronously, we mean that no replica has to wait for all the replicas to finish running. Whenever a pair of replicas are available for exchange, the exchange is made. %This eliminates the need to pair the replicas and limit exchanges to fixed pairs of replicas. 
We differ from the model described in \citep{parashar_arepex}
in some important ways. \alnote{Are we talking about a model here or 
more about implementation aspects? Sounds more like impl.}\athotanote{you are right..}
The asynchronous RE model we developed runs on
production level grids such as the Teragrid and LONI~\citep{LONI_web},
unlike a specialized infrastructure such as CometG.
\jhanote{Careful. Comet is only a coordination infrastructure which
  can be overlayed over production CI} Also, we run the replicas as
MPI jobs. \jhanote{have not defined asynchronous implementation yet!} \athotanote{fixed above, please check.}

% We propose an asynchronous RE algorithm similar to~\cite{parashar_arepex},
% where a replica can perform exchanges asynchronously with any other replica in the ensemble. This eliminates the need to pair the replicas and limit exchanges to fixed pairs of replicas. We differ from the model described in 
% ~\cite{parashar_arepex} in some important ways. The asynchronous RE model we developed runs on production level grids such as the Teragrid and LONI~\cite{LONI_web}, unlike a specialized infrastructure such as CometG. Also, we run the replicas as MPI jobs. 

The different components which make up the total time to completion of 
an asynchronous RE run would be similar to the traditional RE run, 
except that there would not be a $T_W$. \alnote{This is too general. 
We should drop the redundancy of explaining every component and rather 
analyze the differences.} \athotanote{you are right; i am adding a couple of lines here. does it help?}But also, an asynchronous RE algorithm has the potential to perform better than the traditional RE: (i) when we scale-up the number of replicas and (ii) when we scale-out across many machines.

Again, as mentioned in the equation (~\ref{eq:timebtw}), if $t$ is the average time between successful exchanges and $p$ the probability of a successful exchange, then:

$t =  {1 \over p} \times {[T_{MD} + T_{X} + T_{W}]}$

It should be noted that we are discussing the average values of each of the term in the equation above.
The time ($T$) for N$_{X}$ exchanges is therefore $N_{X} \times t$
%Therefore,  $t=  {1 \over p} \times {[T_s+T_{MD} + T_{X}]} $
%where $T_{s}$ is the time required to start a replica, $T_{MD}$ is the time it takes one replica to complete a particular number of time steps,
%and $T_{X}$ is comprised of (i) finding a partner, (ii) exchanging
%states including file transfer, and (iii) book-keeping, status update
%etc.: ${T_{X}} = {T_F + T_{ex} + T_{coord}}$ 
%The time ($T$) for N$_{X}$ exchanges is therefore $N_{X} \times t$
If there are $\eta$ independent exchange events then the time $T$ for 
N$_x$ exchanges is $T \over \eta$. \alnote{What is the value of $\eta$ for async RE?} \athotanote{as mentioned earlier, i the value depends on the implementation; centralised=1, decentralised= $N_R\over2$}
\athotanote{do we need the equation here again?}

Also, we implement the asynchronous RE algorithm in two ways: (i) centralised and (ii) decentralised.

\athotanote{i think it would be better if we just describe sync and async verbally in 2.1 and 2.2 and then introduce the math model in 2.3. just a reordering of the current sections.}

  
%While (i) the time to launch the BigJobs ($T_{L}$) and (ii) the queue wait time ($T_{QW}$) are sequential events, (iii) the runtime of NAMD ($T_{MD}$), (iv) the time to make the exchanges at the end of the run of a pair of replicas ($T_{X}$), and (v) the time required to restart the replicas ($T_{r}$) after each exchange are not. But we know that, even in the asynchronous RE model, for a particular number of exchanges, each replica needs to restart and run a particular number of times. For example, consider simulation with 4 replicas conducting 16 exchanges. That would mean that, on an average, each replica has to restart 8 times. \jhanote{I don't see this. Is it 16 exchanges on   average per replica? Or 16 exchange in total?} In this manner, we can calculate the absolute replica runtime as product of the runtime of a replica and the number of restarts ($N_{R}$). We can also calculate (iv) and (v) for each exchange. In a homogeneous environment this would be true.


% Since this is an asynchronous implementation, it is not easy to
% separate the total simulation time into smaller components. The reason
% is that many events can occur
% concurrently. %There is a need to distinguish between serial and parallel events.
% %While (i) the time to launch the BigJobs ($T_{L}$) and (ii) the queue wait time ($T_{QW}$) are sequential events,
% The following processes, (i) the time to start the replicas ($T_{s}$), (ii) the runtime of NAMD ($T_{MD}$) and (iii) the time to make the exchanges at the end of the run of a pair of replicas ($T_{X}$) after each exchange might occur concurrently. But we know that, even in the asynchronous RE model, for a particular number of exchanges, each replica needs to restart and run a particular number of times. For example, consider simulation with 4 replicas conducting 16 exchanges. That would mean that, on an average, each replica has to restart 8 times. In this manner, we can calculate the absolute replica runtime as product of the runtime of a replica and the number of NAMD restarts ($N_{R}$). %We can also calculate (ii) and (iii) for each exchange. 
% In a homogeneous environment this would be true. 

%Further details depend on the implementation, but we can already see some advantages over the traditional model. with the asynchronous approach. There are also some scientific advantages - making exchanges with non-nearest partners, crosswalks etc. 
%If $N_{X}$ is the total number of exchanges, the total time to completion($T_{C}$) would be: \athotanote{how to write the equation with overlapping components?} \alnote{I would propose to separate the total time-to-solution and the time for a generation.  In a homogeneous environment we could easily add the times for a generation. In a heterogeneous  environment we will need something like a speed factor to model this precisely. We should discuss this.}   \begin{eqnarray} T_{C} &=& T_{MD} \times N_{R} + [T_{s} + T_{X}] \times N_{X} \label{eq:equation 1} \end{eqnarray}

%In a heterogeneous environment, $T_{MD}$ might not be a constant. In that case, we could estimate the number of times the replicas on different machines might restart and calculate the total number of restarts required to complete the exchanges. \athotanote{we can have an equation for the heterogeneous situation too, but it would depend on the number of dissimilar machines.}

%\athotanote{how do the control flow diagrams
%  look? }
%\alnote{We need to highlight how we particularly differ from Parashar
  %et al: no comet, no MPI jobs, production Grids, understand
  %performance sentence...}
%The asynchronous replica exchange framework builds upon the SAGA BigJob and RE frameworks discussed previously.
%The architecture is shown in Figure 1.
%We present two variants of the Asynchronous RE algorithm. The first is a centralized 
%mechanism where all the replicas are managed by a master. %The master closely monitors the replicas and makes the exchanges when appropriate. 
%The second is a decentralized mechanism where each replica is managed independently, in a decoupled manner.% An agent is launched in place of the replica and will perform the the required actions on behalf of the replica.
%Describe how we implement Case II and Case III (you can use figures)
%using SAGA and the advantages



\section{SAGA BigJob - A Pilot-job Framework}
\label{sec:BigJob}

The Simple API for Grid Applications (SAGA)(~\citep{saga_gfd90}) is an API that provides the basic functionality required to build distributed applications, tools and frameworks so as to be independent of the details of the underlying infrastructure. SAGA is an API standardization effort within the Open Grid Forum (OGF)~\citep{ogf_web}, an international standards development body concerned primarily with standards for distributed computing. The various tasks that are carried out using the SAGA APIs include file staging, job spawning and the conduction of the exchange attempts.

Previously, we demonstrated the usage of the SAGA Pilot-Job framework(~\citep{saga_bigjob_condor_cloud}) -- called the BigJob, to run RE simulations across multiple, heterogeneous, distributed grid and cloud infrastructure(~\citep{Luckow:2008fp}). Here we are using the SAGA BigJob to efficiently request and manage computational resources. 

%%%%% FIGURE %%%%%
\begin{figure}[t]
      \centering
          \includegraphics[width=0.8\textwidth]{Bigjob_arch.pdf}
          \caption{\footnotesize SAGA/BigJob Architecture
              }
      \label{fig:bigjob}
\end{figure}

Figure ~\ref{fig:bigjob} shows the architecture of SAGA BigJob.
It consists of three components: (i) the SAGA-BigJob Manager, (ii) the BigJob Agent and (iii) the advert service. The SAGA-BigJob manager submits the BigJobs to the resource manager and the sub-job descriptions to the \emph{advert server}. The advert server can be on any machine and is a central key/value store which is used for communication between the SAGA-BigJob Manager and the BigJob agent. There is a separate BigJob agent for each BigJob. Thus, every BigJobs on every machine has a local BigJob agent. Once the BigJobs become active, the BigJob agent retrieves the job descriptions from the advert server, allocates the required number of nodes and launches the sub-jobs on each resource. The BigJob agent continuously monitors the running sub-jobs and updates the sub-job states in the advert server. Once a sub-job finishes running, the nodes are freed and marked as available. The BigJob agent periodically polls the advert server for new jobs.


\section{Implementation of Synchronous and Asynchronous RE}
We present the implementation details of the synchronous RE and the two 
implementations of the asynchronous RE algorithm here. Synchronous RE is 
only implemented in a centralised manner, where the simulation and all the 
replicas are managed by a master. 
\alnote{The following needs refinement}\athotanote{any better now?} Because there is a synchronization step at the end of each exchange step, we don't think it will help even if the replicas are managed in a decentralized manner. %Hence we only implement the synchronous RE in a centralized manner.
%Due to the synchronization involved and the pairing mechanism, it is not beneficial to have each replica managed independently. 
But, in the case of asynchronous RE, we implement it in both 
centralised and decentralised  fashions. And, we do see that the decentralised 
implementation is much more efficient than the centralised implementation.
The synchronous and asynchronous RE have been successfully deployed on production 
level infrastructure such as the Teragrid and LONI. 

We are primarily interested in learning the scale-up and scale-out properties of synchronous and asynchronous RE and hence are not bothered by the queue wait time. In the following sentences, we will explain how we implement the synchronous RE simulations. To be able to quantify the various terms involved, let us consider the following configuration. Let the total number of replicas ($N_R$) be 32 and the total number of pairwise exchnges ($N_X$) be 128. We did the experiments on LONI and Teragrid shared resource \emph{QueenBee}. Each replica is configured to run 500 time-steps and allocated 16 processors. It should be noted that we are talking about the average values whenever we mention quantities. Each replica $T_{MD}$ takes 90 seconds to complete the 500 time-steps from a fresh start and 70 seconds every time it is restarted. Thus, if each replica is restarted 7 times, the average $T_{MD}$ is 72 seconds. Also in this section, when we say replica, we mean NAMD.

\subsection{Synchronous RE}

The synchronous RE is implemented in a centralised manner where a
master, the RE-Manager controls all the replicas and the simulation
(see Figure~\ref{fig:coordination}a). 

As we have already explained how the the BigJob works in section (~\ref{sec:BigJob}), we will only explain the additional functionality that is in place to make the RE simulations. We will now call the extended SAGA-BigJob Manager the \emph{RE Manager}. The RE Manager now receives various data about each replica, such as the state (running, done, etc.,), temeperature and energy from the BigJob agent after each run and based on that makes the exchanges. This is in addition to the typical management tasks carried out by the SAGA-BigJob Manager.

The RE Manager first submits the BigJob to resource manager. While the BigJob may wait in the queue at the resource manager, the RE Manager submits each of the replica job descriptions to the advert server. This action almost always is completed before the BigJob becomes active in the queue. Therefore, we are not bothered by the time it takes to complete. But once the BigJob is active, the BigJob agent starts each of the jobs and the the average time it takes to retrieve the job-description and start one replica ($T_s$) is 0.3 seconds. For 32 replicas, it is 9.6 seconds. The replicas are now running, and the RE Manager periodically queries the advert server for the latest replica states and marks the replicas which finish running as done. The replicas typically don't finish running exactly at the same time. This is because of two reasons: (i) even though we are using  a homogenous infrastructure, we observed that replica run time varies by 1 or 2 seconds ($\delta$), and (ii) the replicas are started in a serial manner and if there are 32 replicas in the ensemble, there is a gap of 10 seconds between the time the first and last replicas are started. And we see this gap again between the time the first and last replica completion. We are calling this $T_W$ and for 32 replicas it is: $T_s \times N_R+\delta$ = 11.6 seconds. 

Since this is a synchronous RE, all the replicas finish running before the exchanges are started. Hence, the RE manager won't spend any time searching for partners and $T_F$ is 0 seconds. $T_{ex}$ includes updating configuration files and transferring the them to their respective directories, in this case, locally. It takes 0.2 seconds to write and copy a file locally. $T_{ex} = 0.2 \times 2=0.4$ seconds. $T_{coord}$ is the time it takes to update the states of the pair of replicas in the advert server, which is $0.11\times 2 = 0.22$ seconds. Therefore, $T_X$ is $0+0.4+0.22=0.62$ seconds. Since at each exchange step, there will be 16 pairwise exchanges, for 32 replicas, $T_X=0.62 \times 16 = 9.92$ seconds.

If $t$ is the average time between successful exchanges, and $p$ is the probability of a successful exchange, 
Quantifying equation ~\ref{eq:timebtw}, we get
\begin{eqnarray}
t=  {1 \over p} \times {[72 + 9.92 + 11.6]} = {1 \over p} \times 91.52
\label{eq:timebt}
\end{eqnarray}

The time ($T$) for N$_{X}$ exchanges is therefore $N_{X} \times t = 91.52 \times 8 = 732 seconds$


%The BigJob agent also monitors the replicas it
%launched and posts the statuses of the replicas to the advert
%server. The master queries the advert server for the latest job states
%and when it finds that \emph{all} the replicas in the generation are
%'Done' ($T_{MD}+T_{W}$), it starts making the exchanges ($T_{X}$).
%\alnote{We should clearly differ between describing the coordination pattern and analyzing the time} $T_{X}$ is comprised of the time to find a partner $T_{F}$, exchanging states/file-staging $T_{ex}$ and status updates $T_{coord}$. \alnote{repetition with sec 2} After all the exchanges are done in a particular generation, the replica job descriptions are posted to the advert server and the BigJob agent restarts the replicas. This process is repeated until the required number of exchanges are made. The time to make one exchange would be: \begin{eqnarray} t &=&  {1 \over p} \times {[T_s+T_{MD} + T_{X} + T_{W}]}  \label{eq:sy} \end{eqnarray} where $p$ is the probability of a successful exchange.

%If the number of exchanges is ($N_{X}$), the total time to completion ($T_{C}$) would be:
%\begin{eqnarray}
%T_{C} &=& N_{X} \times t 
%\label{eq:synch}
%\end{eqnarray}
%If there are $\eta$ independent exchange events then the time $T$ for 
%N$_x$ exchanges is $T \over \eta$. For this synchronous RE model, 
%$\eta$ is 1\alnote{1 or 2?}, as all the exchanges are carried out 
%in a serial manner. Also, assuming $p$ is 1,\alnote{unrealistic} 
%$T =  [T_s+T_{MD} + T_{X} + T_{W}] \times N_{X}$.
%Quantifying each of the terms above with experimental values observed 
%on QueenBee, we have:
%\alnote{$T_{MD}$ is for what scenario?}
%$T_s = 1.5\,sec; T_{MD}=90\,sec; T_{X}=T_{F}+T_{ex}+T_{coord}=0.3+1+0.6=1.9\,sec; T_W=varies.$ 
%\alnote{$T_{F}$ == 0.3\,sec? independent of number replicas? 
%Do we have an average value of $T_{W}$ with respect to different
%number of replicas maybe incl. stddev? table?}
%The value of $T_W$ increases with the number of replicas, because 
%the gap between the time the first and last replicas are 
%started increases. 


% \begin{figure}[t]
%       \centering
%           \includegraphics[width=0.8\textwidth]{synchronous.pdf}
%           \caption{\footnotesize Control Flow: Synchronous Replica Exchange
%               }
%       \label{fig:sync}
% \end{figure}

\begin{figure}%
\centering
\subfigure[Central]{\includegraphics[width=0.47\textwidth]{../figures/central_AL.pdf}}\qquad
\subfigure[Decentral]{\includegraphics[width=0.47\textwidth]{../figures/decentral_AL.pdf}}\\
\caption{\textbf{Centralised vs. Decentralised Coordination:} Both central coordination style
is used both by the synchronous RE (case I) and a version of the asynchronous RE (case II). 
In the decentral style -- used by another version of the asynchronous RE (case III)-- the master is only required
for initially setting up all replicas. The later coordination is done peer-to-peer via the Advert Service.}
\label{fig:coordination}
\end{figure}


\subsection{Centralised Asynchronous RE}
\alnote{please don't just copy paste. Only focus on the differences}

%%%%% FIGURE %%%%%
% \begin{figure}
% \centering
% %\subfigure[Control Flow: Centralized Replica Exchange]{
% \includegraphics[width=1\textwidth]{centralized.pdf}
% %\label{fig:async:b}
% \caption{\small  In the centralized asynchronous RE, all the replicas are managed by the master.} \athotanote{Do we need this figure? control-flow is quite} 
% \label{fig:cent}
% %\vspace{-1em}
% \end{figure}

We implemented asynchronous RE in centralised and decentralised models 
(cmp. Figure~\ref{fig:coordination}). Here we describe the centralised 
implementation. The implementation is very similar to that of the 
synchronous RE, except that the exchanges are done in an asynchronous manner. 

After the RE Manager launches the BigJob and BigJob becomes active, the BigJob agent polls the advert server for sub-jobs and starts the replicas. As before, we do not consider the time to launch the BigJobs and the queue wait-time when calculating the total time.  The BigJob agent also monitors the 
replicas it launched and posts the statuses of the replicas to the advert 
server. The master queries the advert server for the latest job 
states and when it finds that \emph{a pair} of replicas are 'Done' ($T_{MD}$), it 
makes the exchange ($T_{X}$). Here, $T_{X}$ is comprised of the time to find a partner ($T_{F}$), exchanging states/file-staging ($T_{ex}$ and status updates ($T_{coord}$). 
After each exchange is done, the replica job descriptions 
are posted to the advert server and the BigJob agent restarts the pair of
replicas. This process is repeated until the required number of 
exchanges are made. The time to make one exchange would be:
\begin{eqnarray}
t &=&  {1 \over p} \times {[T_s+T_{MD} + T_{X}]} 
\label{eq:cent}
\end{eqnarray}

If the number of exchanges is ($N_{X}$), the total time to completion ($T_{C}$) would be:
\begin{eqnarray}
T_{C} &=& N_{X} \times t 
\label{eq:centr}
\end{eqnarray}
If there are $\eta$ independent exchange events then the time $T$ for
N$_x$ exchanges is $T \over \eta$. For this asynchronous centralized
RE model, $\eta$ is 1, as all the exchanges are carried out in a
serial manner. Also, assuming $p$ is 1, $T = [T_s+T_{MD} + T_{X} ]
\times N_{X}$ Quantifying each of the terms above with experimental
values observed on QueenBee, we have: $T_s = 1.5 s, T_MD=90 s,
T_X=T_F+T_ex+T_coord=0.3+1+0.6=1.9 s; $ Here, $T_F$ varies with the
number of replicas. \alnote{these are exactly the same numbers as in
  the sync RE scenario. Won't help us in the analysis of our
  experiments} \athotanote{i think we are not able to show here the speed-up due to making the exchanges asynchronously. the bigjob agent will only have pairs of replicas to start rather than all the replicas every time.}

 \alnote{some more details explaining steps 1-6 would be good.}%The decision as to make the exchange or not is made using the Metropolis scheme.  

\subsection{Decentralised Asynchronous RE}

The decentralised implementation is very different from the implementation 
of cases I and II. Although the RE-Manager starts the simulation and launches 
the BigJobs, the replicas are managed in a decentralised manner 
(see Figure~\ref{fig:coordination}b)). We launch
a 'replica-agent' in place of each replica which manages the replicas in turn.
Here, the RE Manager's work is limited to submitting a BigJob request to the 
resource manager ($T_{L}$), and keeping the track of the number of exchanges made. 
Previously, we had the replica as a sub-job. But now, instead of the NAMD executable, 
we launch a replica-agent.
The replica-agent is nothing but a wrapper script for the replica. 
Once BigJobs become active, the BigJob agent starts the replica agents ($T_{s}$), 
the replica agents in turn start the replicas ($T_{r}$). The work of BigJob agent 
monitoring and restarting the replicas is now done by the replica agent. The replica-agents 
monitor the replicas and post the state to the advert server. When one of the 
replicas is ready to exchange ($T_{MD}$), the replica agent searches for another replica which is 
available and negotiates the exchange ($T_{X}$). Here, $T_{X}$ is comprised of the time to find a partner ($T_{F}$), exchanging states/file-staging ($T_{ex}$) and status updates ($T_{coord}$). 
After each exchange is done, the replica agents restart their replicas. This process is repeated until the required number of 
exchanges are made. The temperatures are exchanged over the advert server and the replica agent writes a new configuration file locally. This removes the need to stage the 
configuration files to different machines, which is much more inefficient. \alnote {one could ask why is the 
config file staged in case I and II then?} Where as, in the other two implementations the exchanges happen sequentially. But in the decentralised implementation, there would be many pairs of replica agents negotiating the exchanges. It should be noted that while this causes the time taken per unit exchange to go up, more exchanges occur in unit time. 


The time to make one exchange would be:
\begin{eqnarray}
t &=&  {1 \over p} \times {[T_s+T_{MD} + T_{X}]} 
\label{eq:decent}
\end{eqnarray}

If the number of exchanges is ($N_{X}$), the total time to completion ($T_{C}$) would be:
\begin{eqnarray}
T &=& N_{X} \times t 
\label{eq:decentr}
\end{eqnarray}
If there are $\eta$ independent exchange events then the time $T$ for 
N$_X$ exchanges is $T \over \eta$. For this asynchronous centralized RE model, $\eta$ is $N_R \over 2$, as at any time there could be a maximum of $N_R \over 2$ pairs of replicas negotiating.  \alnote{We really need to discuss $\eta$ tomorrow on the call!}
Also, assuming $p$ is 1, $T = [T_r+T_{MD} + T_{X} ] \times N_{X} \over \eta$
Quantifying each of the terms above with experimental values observed on QueenBee, we have:
$T_r = 0.3 s, T_MD=90 s, T_X=T_F+T_ex+T_coord=1+0.5+0.3=1.8 s; $ \alnote{Why is $T_{coord}$ shorter than for central version?}
Here, $T_F$ varies with the number of replicas. 
%$T &=& 


%If $N_{r}$ is the number of replicas, the total time to completion ($T_{C}$) would then be: 

  %\begin{eqnarray}
%T_{C} &=& T_{L}+T_{QW}+T_{r}+T_{MD} \times N_{R} + [(T_{s} + T_{X}) \times N_{X}+T_{w}]/N_{r}/2
%\label{eq:equation}
%\end{eqnarray}
%We can remove $T_{L}$ and $T_{QW}$ from the equation and neglecting $T_{w}$ and $T_{s}$, we get:

 % \begin{eqnarray}
%T_{C} &=&T_{r}+T_{MD} \times N_{R} + (T_{X} \times N_{X})/N_{r}/2
%\label{eq:equation}
%\end{eqnarray}

The decentralised implementation depends on the advert server much more than the other two implementations. Where as, in the other two implementations there are only a handful of connections to the advert server (one connection from each of the machines). But in the decentralised implementation, each replica agent has an open connection to the advert server. If the number of replicas is 256, then there would be approximately that many connections open with the advert server. This does not seriously impact the communication times with the advert server, and the overhead is unnoticeable.

Although both case II and case III implement the asynchronous RE
algorithm, they differ in the way they are implemented. The replicas
are either managed by a master (case II) or each replica is managed
individually (case III). We propose case III so as to not let the
master in case II to become a bottleneck, which can happen very
quickly with a large number of replicas. \jhanote{If we believe this
  is the case then we need to document and show how/much the slow down
  with increasing replica is.} We do see in our experiments that the
decentralised implementation scales up and scales out better than the
other two implementations.

% \begin{figure}
% \centering
% %\subfigure[Control Flow: Decentralized Replica Exchange]{
% \includegraphics[width=0.9\textwidth]{asyncre.pdf}
% %\label{fig:async:b}
% \caption{\small Decentralized control flow: In the decentralized asynchronous RE, for  each replica there is a replica agent which individually manages the replica.}
% \label{fig:decent}
% %\vspace{-1em}
% \end{figure}

\alnote{we should write Case consistently with small or capital letter}
% We have to bear in mind that while Case II and Case III both implement the same asynchronous RE algorithm, they do it differently.
% At first glance it appears to be a question of philosophy, whether to
% let the replicas be managed by a master or to let each replica be
% managed individually.
%There could be implications effecting the performance of the
%algorithm. Where as in Case II, the master has to manage all the
%replicas and since it can only manage one replica at a time, although negligible, it is a cause for concern with large number of replicas. %The effect could be negligible and might now effect the overall performance.
%But the decentralized version (Case III) has no
%such issues as each replica is managed individually. % \jhanote{The distinction between Case 3 and 2 needs to
%  be made more clear. The following is ``implementation detail''. What
%  is the conceptual difference between Case 3 and Case 2?}

\section{Scale-Up and Scale-Out: Experiments and Results}

%
%%%%% FIGURE %%%%%
\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{../data/scale_up.pdf}
\caption{\small The graph shows the mean run times of synchronous and asynchronous - centralised/decentralised RE simulations on one machine. We repeated the experiments with up to 256 replicas. Up to 128 replicas, the experiments were conducted on QB and 256 replicas experiments were conducted on Ranger.}
\label{fig:graph}
\vspace{-1em}
\end{figure}

%While the traditional RE limits the exchanges to only neighboring temperatures, an asynchronous RE does not. This is not a concern when the number of replicas is small and there is little chance of an exchange between non-adjacent temperatures. However, as the number of replicas increases, the difference between the target temperatures becomes small enough to allow exchanges between non-adjacent temperatures. This also allows for 'crosswalks' to happen. The larger the number of crosswalks, the better is the performance of the simulation.

%To evaluate the performance of the various models of RE we have discussed in the abstract, we conducted several experiments on Teragrid and LONI resources. 
%In the following sentences we will analyze the performance of synchronous RE(Case I) with centralized asynchronous RE(Case II). 

\subsection{Scale-Up}

\subsubsection{Experiments}
In this section we describe the experiments we did increasing the number of replicas on a single machine. We configured the Cases I, II \& III to run parallel NAMD simulations with 4, 8, 16, 32, 64, 128 and 256 replicas sampling a temperature between 300 and 3000 K on Teragrid machines QueenBee and Ranger. \alnote{What resource?} In each simulation, one BigJob is launched with sufficient cores while each replica uses 16 MPI processes and runs 500 time steps between exchange attempts. Up to 128 replicas, all simulations were conducted on QueenBee, but the 256 replica simulations were conducted on Ranger as QueenBee allocates a maximum of 2048 cores per job. The metric used for comparison of 4, 8, 16, 32, 64, 128 and 256 replicas is the time to complete 16, 32, 64, 128, 256, 512 and 1024 attempted exchanges, respectively. It should be noted that the ratio between the number of replicas and the number of attempted exchanges is kept constant, for the purpose of comparison between each of these cases. It means that we are keeping the number of times each replica restarts, approximately, a constant. 

The mean run time is obtained by subtracting the queue wait time of the BigJob from the total time. As the ratio between the number of replicas and number of exchanges is kept constant, ideally, the runtime must remain constant too. In the graph (Figure~\ref{fig:graph}), we see an increase in the time to completion as the number of replicas increases. The increase in the completion time is not uniform across the three cases. We see the most slow down in case I and the least in case III.

\subsubsection{Scale up performance: Results and Analysis}

{\it Results:}\\

{\it Analysis: } From the graph (Figure~\ref{fig:graph}), we see that
Case I does not scale very well when we increase the number of
replicas. This is due to an inability to start and end all the
replicas at the same time and also due to the centralized control of the workflow. With more and more replicas, the time to start the replicas increases  and 
synchronization cost tends to increase too.  In Case II, the exchanges happen in
an asynchronous manner, but they are still conducted by the master
alone. As the number of replicas increases, the master quickly becomes
a bottleneck. But in Case III, the exchanges are all carried out in a
decentralised manner. Many exchanges can occur between different pairs
of replicas at the same time. Therefore, we say that the asynchronous
RE algorithm scales better and that the decentralised implementation
is better than the centralised implementation.

%
%%%%% FIGURE %%%%%
\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{../data/8rep.pdf}
\caption{\small The graph shows the mean run times of synchronous and
  asynchronous - centralised/decentralised RE simulations across two
  and four machines.}
\label{fig:2machines}
\vspace{-1em}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{../data/16rep.pdf}
\caption{\small The graph shows the mean run times of synchronous and
  asynchronous - centralised/decentralised RE simulations across two
  and four machines.}
\label{fig:2machines}
\vspace{-1em}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{../data/32rep.pdf}
\caption{\small The graph shows the mean run times of synchronous and
  asynchronous - centralised/decentralised RE simulations across two
  and four machines. }
\label{fig:2machines}
\vspace{-1em}
\end{figure}

\subsection{Scale out}

\subsubsection{Experiments}
In this section, we describe the experiments we did while increasing
the number of machines across which the experiments were done. We
chose LONI and Teragrid machines to run the experiments. We did
experiments distributed across two and four machines with 8, 16 and 32
replicas distributed equally across the machines.
Figure~\ref{fig:2machines} and Figure~\ref{fig:4machines} show the
behavior of Cases I, II and III across one, two and four machines,
respectively. Overall, there is not much difference between the local
and distributed runs in any case. The reason being that there is very
little interaction between the replicas. A very small configuration
file is staged to the remote machines after an exchange, which will
not add more than a couple of seconds per exchange. The rest of the
co-ordination is done via the advert service, which is usually located
on a remote machine in any case. Each query to the advert server is in
the order of milli seconds and does not produce a noticeable effect on
the performance.

\jhanote{There are several parts of the above paragraph that should be
  in the results sub-section. Similarly, there are parts in the
  experiments section for 'scale-up' that should be in the results
  section}

\subsubsection{Scale out performance: Results and Analysis}

{\it Results:}\\


{\it Analysis: } In Figure~\ref{fig:2machines}, we see the performance of all three cases when run in a distributed manner across two machines. As the data that is exchanged between replicas is very small, the Cases I, II and III behave in a similar manner to the way they behave on a single machine. \alnote{we should add some numbers and maybe a graph for an example scenario: x: machines y: time-to-solution} Again, asynchronous RE is better suited for distributed runs and the decentralised implementation scales best. The reason is, since each replica has its own replica agent, there is no need to transfer any files between machines. The required configuration files are created locally by the replica agent.


%In Case I, the pair-wise replica exchange can occur only between replicas of the same generation. Therefore, each exchange step is attempted only after all the replicas have finished running. After the exchange, all the replicas are restarted sequentially. This inserts a delay between the start time of the first replica, the last replica and the replicas in between. %As more resources become available at different times, the replicas already running or done are forced to wait for the newly running replicas to finish before moving on to the next exchange step. %Each exchange step is counted as an exchange.
%In Case II, the pair-wise replica exchange can take place between any two replicas in the ensemble irrespective of generation. As more resources become available, the new replicas join the ensemble immediately and the replicas already running are not restrained from attempting exchanges or restarting. This gives the asynchronous or synchronous RE a slight advantage. But with a large number of replicas we could easily see large difference.
%\athotanote{Further, we show performance gains by running across more than one machine. By running across more than one machine, we demonstrate the ability to divide the jobs into smaller sub-jobs and then distribute them across a number of machines, thereby reducing the risk of long queue wait times on an over-crowded resource. In Figure~\ref{fig:graph}, it can be seen that the asynchronous RE time to completion improves almost by a factor of 3 when moving from one machine to four machines. This was caused due to the fact that when the experiment was done on one machine, by the time the experiment ended, only 64 cores were allocated by the resource manager. But on the other hand, when the experiment was launched across four machines, it received an allocation of 64 cores on each of the four resources. The improvement that is seen in the case of synchronous RE from one to four machines is also due to a similar reason.} %The asynchronous RE appears faster by a couple of minutes due to the fact that when the BigJobs become available randomly, the synchronous RE has to wait for the newly running replicas to finish.

%slightly over 2 machines, but again increases over 4 machines. This is due to the fact that the experiments have been run only a handful of times but, over time, it can be assumed that it will result in reduced queue wait times.

\section{Conclusion}



%\athotanote{is this right? }
% We are also going to have a wider group of replicas to look at for
% each replica as we are not pairing the replicas.

% Also, we have the usual advantages of using a pilot-job,
% such as reduced queue wait times by not having to submit to the queue
% at every step.  We also provide major advantages when compared to
% Parashar et al.

%  to run the asynchronous RE simulations,
% including the ability to run MPI
% jobs.
% ??We need to evaluate the performance of our models and compare with other models for conducting replica exchange simulations.


%%%%% FIGURE %%%%%
%\begin{figure}
%\centering
%\subfigure[Time to complete 64 exchanges on QB with two 64 core BigJobs and on both QB/Louie jointly with a 64 core BigJob on each machine.]{
%\includegraphics[width=0.40\textwidth]{figures/graph1.pdf}
%\label{fig:subfig3}
%}
%\hspace{0.5cm}
%\subfigure[Time to complete different number of exchanges on QB/Louie with a 64 core BigJob on each machine.]{
%\includegraphics[width=0.40\textwidth]{figures/graph2.pdf}
%\label{fig:subfig4}
%}
%\caption{\small In Figure 2(a), we can see the improvement in performance when run on more than one machine. It is due to the fact that usually the first queued job becomes active before the second on a machine and running jobs on more than one machine solves this problem. In Figure 2(b), we can see consistent performance over prolonged runs, making 32, 64 and 128 exchanges.}
%\label{fig:graphs}
%\vspace{-1em}
%\end{figure}
%%%%% FIGURE %%%%%

An important motivation for this work is to implement a scheme that does not depend on a
static, well defined model of resource availability. %We test and scale
%our implementation on production level grids such as Teragrid and
%LONI~\citep{LONI_web}.
Preliminary results, shown in Figure~\ref{fig:graph}, indicate the
most important advantages of asynchronous RE and SAGA/BigJob over
traditional RE: (a) allows for exchanges to occur between replicas
with non-nearest temperatures, which in turn allows crosswalks to
happen (b) a reduced time to completion when running on more than one
machine due to improved resource availabilities, (c) the advantage of
using a pilot-job mechanism, which eliminates the waiting times at the
local resource manager, \alnote{have shown this in prev. papers, but
  we don't really have data in this one} and (d) the ability to scale
out across different production level infrastructure, such as, the
Teragrid and LONI.
% It performs well even after doubling and quadrupling the number of
% exchanges required to complete the simulation. The time to
% completion only increases by 35\% after doubling and 117\% after
% quadrupling the number of exchanges.
%\athotanote{should the results
%  be included in the conclusion or in a separate results section? Do
%  you agree with the \# of exchanges scheme to show the data?}
% Unfortunately we have results only for Case II currently, but 

%In summary, we have established the ability to scale-out across different
%infrastructure and compared the performance of the asynchronous
%RE with the synchronous RE at large scales. 
Further, we compare the traditional RE model (case I) and the centralised (case II) and decentralised (case III) models of the asynchronous replica exchange by modeling and repeating the experiments a reasonable number of times, so as to accurately quantify the scientific and performance gains. %We also propose to measure the frequency with which crosswalks occur with increasing number of replicas and measure the advantages due to a decentralized implementation in the full paper.


%With this asynchronous replica exchange mechanism we can improve the
%number of exchanges per unit time, a key parameter in judging the
%performance of a replica-exchange mechanism. \athotanote{is this
 % right? }  We are also going to have a wider group of replicas to
%look at for each replica as we are not pairing the replicas. Also, we
%have the usual advantages of using a pilot-job, such as reduced queue
%wait times by not having to submit to the queue.  Unfortunately we
%dont have results \jhanote{What results can we present -- any? some?},
%so we will say, (i) we establish the ability to scale-out (distributed
%and exa-scale) across different infrastructure (ii) compare the Async
%versus sync formulation at unprecedented scales \jhanote{At least
%  outline what infrastructure we / you are planning to use?} (iii)
%compare different implementations of the Async version
 

\begin{acknowledgement}
  ACK
\end{acknowledgement}

\bibliographystyle{kluwer}
\bibliography{saga,literature}    
\end{document}

