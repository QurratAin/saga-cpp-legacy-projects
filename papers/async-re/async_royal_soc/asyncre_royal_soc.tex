\documentclass{rspublic}   

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
%\pagestyle{empty}

%\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}    
\usepackage{multirow}    
\usepackage{listings}   
\usepackage{times}     
\usepackage{paralist}    
\usepackage{wrapfig}    
\usepackage[small,it]{caption}
\usepackage{multirow}
\usepackage{ifpdf}    

                    
%Bibliography                     
\usepackage{natbib}   

\usepackage{listings}
\usepackage{keyval}  
\usepackage{color}
\definecolor{listinggray}{gray}{0.95}
\definecolor{darkgray}{gray}{0.7}
\definecolor{commentgreen}{rgb}{0, 0.4, 0}
\definecolor{darkblue}{rgb}{0, 0, 0.4}
\definecolor{middleblue}{rgb}{0, 0, 0.7}
\definecolor{darkred}{rgb}{0.4, 0, 0}
\definecolor{brown}{rgb}{0.5, 0.5, 0}



\title[Efficient Replica-Exchange Simulations on
  Large-Scale Production Infrastructure]{Efficient Replica-Exchange Simulations on
  Large-Scale Production Infrastructure}

\author[Thota, Luckow, Jha]{
  Abhinav Thota$^{1,2}$, Andr\'e Luckow$^{1}$ and Shantenu Jha$^{1,2,3}$\\
  \small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State University, Baton Rouge, LA 70803, USA}}\\
  \small{\emph{$^{2}$Department of Computer Science, Louisiana State
      University, Baton Rouge, LA 70803, USA}}\\
  \small{\emph{$^{3}$e-Science Institute, Edinburgh EH8 9AA, UK}}\\
}

%\date{}

\def\acknowledgementname{Acknowledgements}
\newenvironment{acknowledgement}%
{\section*{\acknowledgementname}%
\parindent=0pt%
}

\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***shantenu: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***andre: #1 }}}
\newcommand{\athotanote}[1]{ {\textcolor{green} { ***athota: #1 }}}
\else
\newcommand{\alnote}[1]{}
\newcommand{\athotanote}[1]{}
\newcommand{\jhanote}[1]{}
\fi

\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\T}[1]{\texttt{#1}}

\newcommand{\glidein}[1]{Glide-In }  
\newcommand{\replicaagent}[1]{Replica-Agent }         
\newcommand{\remanager}[1]{RE-Manager }

\begin{document} 


\maketitle    

\begin{abstract}{Replica-Exchange, SAGA, Large-Scale, Production}  

Developing applications that are able to orchestrate heterogeneous resources across distributed resources is a complex
task. Inevitably, the design and development of an application is influenced and constrained by the programming systems
and the infrastructure it is developed against. Breaking this coupling between the development and the underlying
infrastructure, to enable applications to be flexible (across infrastructure), extensible (to new methods of communication
and coordination) and scalable is an important design objective of distributed applications both logically
distributed and physically distributed.
In this work, we focus on the Replica-Exchange (RE) Methods which represent a class of algorithms that
involve a large number of loosely-coupled ensembles. RE simulations are used to understand physical phenomena √ê
ranging from protein folding dynamics to binding affinity calculations.
In this work, we develop a flexible, extensible and scalable implementation of RE that can utilise a range of infrastructure
concurrently (and autonomically/adaptively), that supports different coordination mechanisms (publishsubscribe,
centralised notification), different replica pairing mechanisms (synchronous versus asynchronous) and
thereby different variants of the RE algorithm. We implement and demonstrate how a flexible and robust implementation
enables the efficient use of a broad range of infrastructure.
\end{abstract}

\section{Introduction}
Developing applications that are able to orchestrate heterogeneous
resources across distributed resources is a complex task.  Inevitably,
the design and development of an application is influenced and
constrained by the programming systems and the infrastructure it is
developed against. Breaking this coupling between the development and
the underlying infrastructure, to enable applications to be flexible
(across infrastructure), extensible (to new methods of communication
and coordination) and scalable is an important design objective of
distributed applications -- both logically distributed and physically
distributed.

In this work, we focus on the Replica-Exchange (RE)~\cite{hansmann,Sugita:1999rm} 
methods -- which represent a class of
algorithms that involve a large number of loosely-coupled ensembles.
RE simulations are used to understand physical phenomena -- ranging
from protein folding dynamics to binding affinity calculations. Most RE implementations are either infrastructure specific (Woods et al. 2005)
or, if using multiple distributed resources, they require prior co-scheduling
(Manos et al. 2008). ~\cite{Luckow:2008fp} takes it to the next level and is an example of adaptive RE simulations on production-level grid resources, while ~\cite{parashar_arepex} is an example of \emph{asynchronous} RE simulations, which is based on CometG~\cite{Li:2005:CSC:1090948.1091381}, a decentralized computational infrastructure for Desktop Grid environments.

We develop a flexible, extensible and scalable
implementation of RE that can utilize a range of infrastructure
concurrently (and autonomically/adaptively), that supports different
coordination mechanisms (publish-subscribe, centralised notification),
different replica pairing mechanisms (synchronous versus asynchronous)
and thereby different variants of the RE algorithm. We implement and
demonstrate how a flexible and robust implementation enables the
efficient use of a broad range of infrastructure. We compare and analyze the performance of the different RE models (synchronous and asynchronous) when we scale-up to 256 replicas and scale-out to 4 machines. 
We present results using which the reader can understand which model of RE is most suited for a particular set of resources (distributed, local etc.,)

\alnote{Should we add a section with some scientific background: HIV,
  Hepatitis...?}  \jhanote{given the tightness of space, I think we
  should try to avoid it. OK?} The rest of the paper is organized as
follows. Section II presents the different algorithms of RE. Section
III presents the SAGA BigJob architecture which we use to conduct the
simulations. In Section IV, we present how each of the RE algorithms
are implemented. We also present different implementations of the
asynchronous RE algorithm. In Section V, we describe the experiments
that we conducted on local and distributed resources. In Section VI,
we present the results and analysis. Section VII concludes the paper
and discusses future work.

\section{Replica-Exchange Approach}
\label{sec:repex-approach}
The RE algorithm involves the concurrent execution of multiple similar
simulations, the \emph{replicas}.  There is a loose-coupling between
the replicas in form of periodic exchange attempts between paired
replicas. The traditional approach to RE is the synchronous model, which works well in an ideal scenario with a well defined model of resource availability. But with heterogenous systems and fluctuating resource availability, the asynchronous RE model could be more effectively used to conduct simulations.

%Previously, we demonstrated the usage of the SAGA Pilot-Job
%framework~\cite{saga_bigjob_condor_cloud} -- called the BigJob, to run
%RE simulations across multiple, heterogeneous distributed Grid and
%Cloud infrastructures~\cite{Luckow:2008fp}.
%\alnote{maybe we should also intro SAGA at some point} \jhanote{Yes} The Simple API for Grid Applications (SAGA)~\cite{saga_gfd90} is an API standardization effort within the Open Grid Forum (OGF)~\cite{ogf_web}, an international standards development body concerned primarily with standards for distributed computing. The various tasks that are carried out using the SAGA APIs include file staging, job spawning and the conduction of the exchange attempts.
%Further, we introduced several adaptivity modes, e.\,g.\ adaptive
%sampling that are able to react to dynamic changes in resource
%availabilities.

%\alnote{Not sure how many technical we need to provide...}  

%Traditionally, depending
%on the number of processes \texttt{N}, the manager creates \texttt{N/2} pairs
%of replicas.  Before launching a job, the manager ensures that all
%required input files are transferred to the respective resource. For
%this purpose, the SAGA File API and the GridFTP adaptor are used. The
%replica jobs are then submitted to the resource using the SAGA CPR
%API and the MIGOL/GRAM middleware.

\subsection{Traditional Replica Exchange}
In the traditional model of RE (Case I), depending upon
on the number of processes \texttt{N}, the manager creates \texttt{N/2} pairs
of replicas. When the replicas reach a
pre-determined state (e.g. the NAMD job finishes after a fixed number
of steps), a decision as to whether to exchange temperatures between
previously paired replicas is determined using the Metropolis scheme.
The run of an ensemble of replicas in parallel and the subsequent
pairwise exchange attempt are referred to as generation. No two
replicas can belong to different generations. If the exchange attempt
is successful, parameters such as the temperature are swapped. Both
jobs are then relaunched~\cite{Luckow:2008fp}.

The total time to completion ($T_{C}$) of the simulation is the sum of
the following components: (i) the time to launch the BigJobs
($T_{L)}$, (ii) the queue wait time ($T_{QW}$), and the product of the
following components and the number of exchange steps ($N_{X}$): (iii)
the actual runtime of NAMD at each generation ($T_{MD}$), (iv) time
spent waiting for all replicas to be done ($T_{W}$), (v) the time to
make the exchanges at the end of each generation ($T_{X}$), and (vi)
the time required to restart the NAMD jobs after each exchange step
($T_{r}$). We only include steps (i) and (ii) for completeness, which
occur only at the beginning of the simulation. Steps (iii) to (vi)
repeat after each exchange. We consider the start time as the time at
which the replicas are launched for the first time. We also think that
it is safe to say that, irrespective of the implementation details,
steps (iii) to (vi) are common to all replica-exchange
simulations. \alnote{we should put behind each step which $T_{x}$ we
  refer to} \jhanote{not sure if you are referring to here (in text)
  or in the equation?} Therefore, the actual time to solution is the
sum of (iii), (iv), (v) and (vi) multiplied by the number of exchange
steps ($N_{X}$). So, the time to completion, without (i) and (ii) would be:
\alnote{Why don't we include $T_{L}$ and $T_{QW}$?} \jhanote{Maybe because
they dont help understand the scaling behaviour and the fact none
of these are a function of the replica-exchange algorithms..}

\begin{eqnarray}
T_{C} &=& [T_{MD} + T_{W} + T_{X} + T_{r}] \times N_{X}
\label{eq:equation 1}
\end{eqnarray}

Here, the steps (v) and (vi) occur in a serial manner.
 
% 1 para limitation on traditional replica exchange
A major limitation of this model is that the replicas are paired in fixed groups. 
Exchanges can only take place between these paired replicas.
This, while limiting the number of replicas which are available for an exchange, also inhibits exchanges between replicas with non-nearest temperatures. This also negates the possibility of crosswalks. A crosswalk is said to occur when a replica originally with a low temperature reaches the upper temperature range and then returns to the lower temperature range. %Moreover, the replicas are attached to their partners, 
%sometimes waiting for them to complete while there are possibly other replicas available which are paired to their partners.
%This also reduces the number of exchanges that can take place within a given time.
Replica pairing works well in an ideal scenario but with heterogeneous
systems, where the resource availability and performance fluctuates,
it is far from ideal. It is important to have a scheme that does not
depend on a static \alnote{I think we should explain what well-defined
  resource model means.} \jhanote{fixed. Please check} set of
resources that are pre-defined at the time of workload submission.
% availability, where the resources and the resource availability are
% well defined. 
This forms the motivation for coming up with a formulation that makes
it possible to run RE simulations that can dynamically utilise a range
of infrastructures.
 %\jhanote{The point is really the following: Paired-replicas are Ok if
  %it can be guaranteed that equal resources will be available, or the
  %resource availabilty can be predicted in advance. However, in
  %distributed systems, whereby definition, resource availability
  %fluctates it is important to have a scheme/implementation that does
  %not depend on a static, well-defined model of resource availability
  %and execution. This forms the motivation for coming up with a
  %formulation of a well known algorithm that makes it suitable for a
  %range of infrastruture.}
  

  
\subsection{Asynchronous Replica Exchange}
%- Introduce asynchronous Replica Exchange --  1 para on case II and case III (algorithmically)

%To overcome these limitations
We propose an asynchronous RE algorithm similar to~\cite{parashar_arepex},
where a replica can perform exchanges asynchronously with any other replica in the ensemble. This eliminates the need to pair the replicas and limit exchanges to fixed pairs of replicas. We differ from the model described in Parashar et al. in some important ways. The asynchronous RE model we developed runs on production level grids such as the Teragrid and LONI~\cite{LONI_web}, unlike a specialized infrastructure such as CometG. Also, we run the replicas as MPI jobs. 

Since this is an asynchronous implementation, it is not easy to separate the total simulation time into smaller components. The reason is that many events can occur concurrently. %There is a need to distinguish between serial and parallel events. 
While (i) the time to launch the BigJobs ($T_{L}$) and (ii) the queue wait time ($T_{QW}$) are sequential events, (iii) the runtime of NAMD ($T_{MD}$), (iv) the time to make the exchanges at the end of the run of a pair of replicas ($T_{X}$), and (v) the time required to restart the replicas ($T_{r}$) after each exchange are not. But we know that, even in the asynchronous RE model, for a particular number of exchanges, each replica needs to restart and run a particular number of times. For example, consider simulation with 4 replicas conducting 16 exchanges. That would mean that, on an average, each replica has to restart 8 times. In this manner, we can calculate the absolute replica runtime as product of the runtime of a replica and the number of restarts ($N_{R}$). We can also calculate (iv) and (v) for each exchange. In a homogeneous environment this would be true. 
%Further details depend on the implementation, but we can already see some advantages over the traditional model. with the asynchronous approach. There are also some scientific advantages - making exchanges with non-nearest partners, crosswalks etc. 
If $N_{X}$ is the total number of exchanges, an equation for total time, excluding (i) and (ii), would be:
\athotanote{how to write the equation with overlapping components?}
\alnote{I would propose to separate the total time-to-solution and the time for a generation. 
In a homogeneous environment we could easily add the times for a generation. In a heterogeneous 
environment we will need something like a speed factor to model this precisely. We should discuss this.}
  \begin{eqnarray}
T_{C} &=& T_{MD} \times N_{R} + [T_{X} + T_{r}] \times N_{X}
\label{eq:equation 1}
\end{eqnarray}

In a heterogeneous environment, $T_{MD}$ might not be a constant. In that case, we could estimate the number of times the replicas on different machines might restart and calculate the total number of restarts required to complete the exchanges. \athotanote{we can have an equation for the heterogeneous situation too, but it would depend on the number of dissimilar machines.}

We implement the asynchronous RE algorithm in two ways: (i) centralized (case II) and (ii) decentralized (case III).

%\athotanote{how do the control flow diagrams
%  look? }
%\alnote{We need to highlight how we particularly differ from Parashar
  %et al: no comet, no MPI jobs, production Grids, understand
  %performance sentence...}
%The asynchronous replica exchange framework builds upon the SAGA BigJob and RE frameworks discussed previously.
%The architecture is shown in Figure 1.
%We present two variants of the Asynchronous RE algorithm. The first is a centralized 
%mechanism where all the replicas are managed by a master. %The master closely monitors the replicas and makes the exchanges when appropriate. 
%The second is a decentralized mechanism where each replica is managed independently, in a decoupled manner.% An agent is launched in place of the replica and will perform the the required actions on behalf of the replica.
%Describe how we implement Case II and Case III (you can use figures)
%using SAGA and the advantages



\section{SAGA BigJob - A Pilot-job Framework}

The Simple API for Grid Applications (SAGA)(~\cite{saga_gfd90}) is an API that provides the basic functionality required to build distributed applications, tools and frameworks so as to be independent of the details of the underlying infrastructure. SAGA is an API standardization effort within the Open Grid Forum (OGF)~\cite{ogf_web}, an international standards development body concerned primarily with standards for distributed computing. The various tasks that are carried out using the SAGA APIs include file staging, job spawning and the conduction of the exchange attempts.

Previously, we demonstrated the usage of the SAGA Pilot-Job framework(~\cite{saga_bigjob_condor_cloud}) -- called the BigJob, to run RE simulations across multiple, heterogeneous, distributed grid and cloud infrastructure(~\cite{Luckow:2008fp}). Here we are using the SAGA BigJob to efficiently request and manage computational resources. 

%%%%% FIGURE %%%%%
\begin{figure}[t]
      \centering
          \includegraphics[width=0.8\textwidth]{Bigjob_arch.pdf}
          \caption{\footnotesize SAGA/BigJob Architecture
              }
      \label{fig:bigjob}
\end{figure}

Figure ~\ref{fig:bigjob} shows the architecture of SAGA BigJob.
It consists of three components: (i) the SAGA-BigJob Manager, (ii) the BigJob Agent and (iii) the advert service. The SAGA-BigJob manager submits the BigJobs to the resource manager and the sub-job descriptions to the \emph{advert server}. The advert server can be on any machine and is a central key/value store which is used for communication between the RE Manager and the BigJob agent. Once the BigJobs become active, the BigJob agent retrieves the job descriptions from the advert server, allocates the required number of nodes and launches the sub-jobs on each resource. The BigJob agent continuously monitors the running sub-jobs and updates the sub-job states in the advert server. Once a sub-job finishes running, the nodes are freed and marked as available. The BigJob agent also polls the advert server for new jobs.


\section{Implementation of Synchronous and Asynchronous RE}
We present the implementation details of the Synchronous RE and the two different implementations of the Asynchronous RE algorithm here. Synchronous RE is only implemented in a centralized manner, where simulation and all the replicas are managed by a master. Due to the synchronization involved and the pairing mechanism, it is not beneficial to have each replica managed independently. But, in the case of asynchronous RE, we implement it in both centralized and decentralized  fashions. And, we do see that the decentralized implementation is much more efficient than the centralized implementation.

\subsection{Synchronous RE}

In synchronous RE, the master controls all the replicas and the simulation. 
First, the master submits a request to the resource manager for a  
number of cores sufficient for all the replicas (BigJob). A connection to the 
advert server is opened to store the job descriptions. Second, 
the necessary files are staged to the individual working directories and 
the job descriptions of each of the replicas are 
posted to the advert server. When the BigJob becomes active, 
the BigJob agent polls the advert server for new replicas 
and starts those replicas. The BigJob agent also monitors the 
replicas it launched and posts the statuses of the replicas to the advert 
server. The master queries the advert server for the latest job 
states and when it finds that all the replicas are 'Done', it 
starts making the exchanges. After choosing the partners and the exchange is made by exchanging the temperatures of the replicas and writing and staging the new configuration files 
to the respective working directories. After all the exchanges 
are done in a particular generation, the replica job descriptions 
are posted to the advert server and the bigjob agent restarts the 
replicas. This process is repeated until the required number of 
exchanges are made. 

\begin{figure}[t]
      \centering
          \includegraphics[width=0.8\textwidth]{synchronous.pdf}
          \caption{\footnotesize Control Flow: Synchronous Replica Exchange
              }
      \label{fig:sync}
\end{figure}


\subsection{Centralized Asynchronous RE}


%%%%% FIGURE %%%%%
\begin{figure}
\centering
%\subfigure[Control Flow: Centralized Replica Exchange]{
\includegraphics[width=1\textwidth]{centralized.pdf}
%\label{fig:async:b}
\caption{\small  In the centralized asynchronous RE, all the replicas are managed by the master.} \athotanote{Do we need this figure?} 
\label{fig:cent}
%\vspace{-1em}
\end{figure}

In the centralized version (Figure~\ref{fig:cent}) of the asynchronous RE, the master launches and manages the replicas. The implementation is very similar to that of the synchronous RE, except that the exchanges are done in a asynchronous manner. 

First, the master submits a request to the resource manager for a  
number of cores sufficient for all the replicas (BigJob). A connection to the 
advert server is opened to store the job descriptions. Second, 
the necessary files are staged to the individual working directories and 
the job descriptions of each of the replicas are 
posted to the advert server. When the BigJob becomes active, 
the BigJob agent polls the advert server for new replicas 
and starts those replicas. The BigJob agent also monitors the 
replicas it launched and posts the statuses of the replicas to the advert 
server. The master queries the advert server for the latest job 
states and when it replica in 'Done' state, it 
starts searches for a partner to make the exchange. After choosing the partner, the exchange is made by exchanging the temperatures of the replicas and writing and staging the new configuration files 
to the respective working directories. After each exchange, the master repeats the posts the replica job descriptions 
the advert server and the bigjob agent restarts the 
replicas. This process is repeated until the required number of 
exchanges are made.

 \alnote{some more details explaining steps 1-6 would be good.}%The decision as to make the exchange or not is made using the Metropolis scheme.  

\subsection{Decentralized Asynchronous RE}

The decentralized implementation is very different from the implementation of cases I and II. We introduce
a 'replica-agent' for each replica. Each replica-agent manages a single replica. 
Here, the master work is limited to submitting a BigJob request to the resource manager, 
staging the files and keeping the track of the number of exchanges made. Previously, we had the replica as a sub-job. But now, instead of the NAMD executable, we launch a replica-agent.
The replica-agent is nothing but a wrapper script for the replica. 
Once BigJobs become active and the BigJob agent starts the replica agents, the replica agents in turn start the replicas. The work of BigJob agent monitoring and restarting the replicas is now done by the replica agent. The replica-agents 
monitor the replicas and post the state to the advert server. When one of the 
replicas is ready to exchange, the replica agent searches for another replica which is 
available and negotiates the exchange. The temperatures are exchanged over the advert server and the replica agent writes a new configuration file locally. This removes the need to stage the 
configuration files to different machines, which is much more inefficient. \alnote {one could ask why is the 
config file staged in case I and II then?} 

Although both case II and case III implement the asynchronous RE algorithm, they differ in the way they are implemented. The replicas are either managed by a master (case II) or each replica is managed individually (case III). We propose case III so as to not let the master in case II to become a bottleneck, which can happen very quickly with a large number of replicas. We do see in our experiments that the decentralized implementation scales up and scales out better than the other two implementations.

\begin{figure}
\centering
%\subfigure[Control Flow: Decentralized Replica Exchange]{
\includegraphics[width=1\textwidth]{asyncre.pdf}
%\label{fig:async:b}
\caption{\small Decentralized control fow: In the decentralized asynchronous RE, for  each replica there is a replica agent which individually manages the replica.}
\label{fig:decent}
%\vspace{-1em}
\end{figure}

\alnote{we should write Case consistently with small or capital letter}
% We have to bear in mind that while Case II and Case III both implement the same asynchronous RE algorithm, they do it differently.
% At first glance it appears to be a question of philosophy, whether to
% let the replicas be managed by a master or to let each replica be
% managed individually.
%There could be implications effecting the performance of the
%algorithm. Where as in Case II, the master has to manage all the
%replicas and since it can only manage one replica at a time, although negligible, it is a cause for concern with large number of replicas. %The effect could be negligible and might now effect the overall performance.
%But the decentralized version (Case III) has no
%such issues as each replica is managed individually. % \jhanote{The distinction between Case 3 and 2 needs to
%  be made more clear. The following is ``implementation detail''. What
%  is the conceptual difference between Case 3 and Case 2?}

\section{Scale-Up and Scale-Out: Experiments and Results}

%
%%%%% FIGURE %%%%%
\begin{figure}
\centering
\includegraphics[scale=0.50]{scale_up.pdf}
\caption{\small The graph shows the mean run times of synchronous and asynchronous - centralized/decentralized RE simulations on one machine. We repeated the experiments with up to 256 replicas. Up to 128 replicas, the experiments were conducted on QB and 256 replicas experiments were conducted on Ranger.}
\label{fig:graph}
\vspace{-1em}
\end{figure}


%While the traditional RE limits the exchanges to only neighboring temperatures, an asynchronous RE does not. This is not a concern when the number of replicas is small and there is little chance of an exchange between non-adjacent temperatures. However, as the number of replicas increases, the difference between the target temperatures becomes small enough to allow exchanges between non-adjacent temperatures. This also allows for 'crosswalks' to happen. The larger the number of crosswalks, the better is the performance of the simulation.

%To evaluate the performance of the various models of RE we have discussed in the abstract, we conducted several experiments on Teragrid and LONI resources. 
%In the following sentences we will analyze the performance of synchronous RE(Case I) with centralized asynchronous RE(Case II). 

\subsection{Scale-Up}

\subsubsection{Experiments}
We configured the Cases I, II \& III to run parallel NAMD simulations with 4, 8, 16, 32, 64, 128 and 256 replicas sampling a temperature between 300 and 3000 K on Teragrid machines QueenBee and Ranger. \alnote{What resource?} In each simulation, one BigJob is launched with sufficient cores while each replica uses 16 MPI processes and runs 500 time steps between exchange attempts. Up to 128 replicas, all simulations were conducted on QueenBee, but the 256 replica simulations were conducted on Ranger as QueenBee allocates a maximum of 2048 cores per job. The metric used for comparison of 4, 8, 16, 32, 64, 128 and 256 replicas is the time to complete 16, 32, 64, 128, 256, 512 and 1024 attempted exchanges, respectively. It should be noted that the ratio between the number of replicas and the number of attempted exchanges is kept constant, for the purpose of comparison between each of these cases. It means that we are keeping the number of times each replica restarts, approximately, a constant. 

The mean run time is obtained by subtracting the queue wait time of the BigJob from the total time. As the ratio between the number of replicas and number of exchanges is kept constant, ideally, the runtime must remain constant too. In the graph (Figure~\ref{fig:graph}), we see an increase in the time to completion as the number of replicas increases. The increase in the completion time is not uniform across the three cases. We see the most slow down in Case I and the least in Case III.

\subsubsection{Scale up performance: Results and Analysis}

{\it Results:}\\

{\it Analysis: } From the graph (Figure~\ref{fig:graph}), we see that
Case I does not scale very well when we increase the number of
replicas. This is because of an inability to start and end all the
replicas at the same time. With more and more replicas, the
synchronization cost increases.  In Case II, the exchanges happen in
an asynchronous manner, but they are still conducted by the master
alone. As the number of replicas increases, the master quickly becomes
a bottleneck. But in Case III, the exchanges are all carried out in a
decentralized manner. Many exchanges can occur between different pairs
of replicas at the same time. Therefore, we say that the asynchronous
RE algorithm scales better and that the decentralized implementation
is better than the centralized implementation.

%
%%%%% FIGURE %%%%%
\begin{figure}
\centering
\includegraphics[scale=0.80]{2Machines.pdf}
\caption{\small The graph shows the mean run times of synchronous and asynchronous - centralized/decentralized RE simulations across two machines.}
\label{fig:2machines}
\vspace{-1em}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.80]{4Machines.pdf}
\caption{\small The graph shows the mean run times of synchronous and asynchronous - centralized/decentralized RE simulations across four machines.}
\alnote{maybe it would be good to present both 2m/4m scenario in one graph. Makes the comparison a little easier.}
\label{fig:4machines}
\vspace{-1em}
\end{figure}

\subsection{Scale out}
\subsubsection{Experiments}
We chose LONI and Teragrid machines to run Cases I, II \& III. We did experiments distributed across two and four machines. We did these experiments with 8, 16 and 32 replicas distributed equally across two and four machines. 
Figure~\ref{fig:2machines} and Figure~\ref{fig:4machines} show the behavior of Cases I, II and III across one, two and four machines, respectively. On the whole, there is not much difference between the local and distributed runs in any case. The reason being that there is very little interaction between the replicas. A very small configuration file is staged to the remote machines after an exchange, which will not add more than a couple of seconds per exchange. The rest of the co-ordination is done via the advert service, which is usually located on a remote machine in any case. Each query to the advert server is in the order of milli seconds and does not produce a noticeable effect on the performance. 

\subsubsection{Scale out performance: Results and Analysis}

{\it Results:}\\


{\it Analysis: } In Figure~\ref{fig:2machines}, we see the performance of all three cases when run in a distributed manner across two machines. As the data that is exchanged between replicas is very small, the Cases I, II and III behave in a similar manner to the way they behave on a single machine. \alnote{we should add some numbers and maybe a graph for an example scenario: x: machines y: time-to-solution} Again, asynchronous RE is better suited for distributed runs and the decentralized implementation scales best. The reason is, since each replica has its own replica agent, there is no need to transfer any files between machines. The required configuration files are created locally by the replica agent.


%In Case I, the pair-wise replica exchange can occur only between replicas of the same generation. Therefore, each exchange step is attempted only after all the replicas have finished running. After the exchange, all the replicas are restarted sequentially. This inserts a delay between the start time of the first replica, the last replica and the replicas in between. %As more resources become available at different times, the replicas already running or done are forced to wait for the newly running replicas to finish before moving on to the next exchange step. %Each exchange step is counted as an exchange.
%In Case II, the pair-wise replica exchange can take place between any two replicas in the ensemble irrespective of generation. As more resources become available, the new replicas join the ensemble immediately and the replicas already running are not restrained from attempting exchanges or restarting. This gives the asynchronous or synchronous RE a slight advantage. But with a large number of replicas we could easily see large difference.
%\athotanote{Further, we show performance gains by running across more than one machine. By running across more than one machine, we demonstrate the ability to divide the jobs into smaller sub-jobs and then distribute them across a number of machines, thereby reducing the risk of long queue wait times on an over-crowded resource. In Figure~\ref{fig:graph}, it can be seen that the asynchronous RE time to completion improves almost by a factor of 3 when moving from one machine to four machines. This was caused due to the fact that when the experiment was done on one machine, by the time the experiment ended, only 64 cores were allocated by the resource manager. But on the other hand, when the experiment was launched across four machines, it received an allocation of 64 cores on each of the four resources. The improvement that is seen in the case of synchronous RE from one to four machines is also due to a similar reason.} %The asynchronous RE appears faster by a couple of minutes due to the fact that when the BigJobs become available randomly, the synchronous RE has to wait for the newly running replicas to finish.

%slightly over 2 machines, but again increases over 4 machines. This is due to the fact that the experiments have been run only a handful of times but, over time, it can be assumed that it will result in reduced queue wait times.

\section{Conclusion}



%\athotanote{is this right? }
% We are also going to have a wider group of replicas to look at for
% each replica as we are not pairing the replicas.

% Also, we have the usual advantages of using a pilot-job,
% such as reduced queue wait times by not having to submit to the queue
% at every step.  We also provide major advantages when compared to
% Parashar et al.

%  to run the asynchronous RE simulations,
% including the ability to run MPI
% jobs.
% ??We need to evaluate the performance of our models and compare with other models for conducting replica exchange simulations.


%%%%% FIGURE %%%%%
%\begin{figure}
%\centering
%\subfigure[Time to complete 64 exchanges on QB with two 64 core BigJobs and on both QB/Louie jointly with a 64 core BigJob on each machine.]{
%\includegraphics[width=0.40\textwidth]{figures/graph1.pdf}
%\label{fig:subfig3}
%}
%\hspace{0.5cm}
%\subfigure[Time to complete different number of exchanges on QB/Louie with a 64 core BigJob on each machine.]{
%\includegraphics[width=0.40\textwidth]{figures/graph2.pdf}
%\label{fig:subfig4}
%}
%\caption{\small In Figure 2(a), we can see the improvement in performance when run on more than one machine. It is due to the fact that usually the first queued job becomes active before the second on a machine and running jobs on more than one machine solves this problem. In Figure 2(b), we can see consistent performance over prolonged runs, making 32, 64 and 128 exchanges.}
%\label{fig:graphs}
%\vspace{-1em}
%\end{figure}
%%%%% FIGURE %%%%%

An important motivation for this work is to implement a scheme that does not depend on a
static, well defined model of resource availability. %We test and scale
%our implementation on production level grids such as Teragrid and
%LONI~\cite{LONI_web}.
Preliminary results, shown in Figure~\ref{fig:graph}, indicate the most important advantages of asynchronous RE and SAGA/BigJob over traditional RE: (a) allows for exchanges to occur between replicas with non-nearest temperatures, which in turn allows crosswalks to happen (b) a reduced time to completion when running on more than one machine due to improved resource availabilities, (c) the advantage of using a pilot-job mechanism, which eliminates the waiting times at the local resource manager, \alnote{have shown this in prev. papers, but we don't really have data in this one} and (d) the ability to scale out across different production level infrastructure, such as, the Teragrid and LONI.
% It performs well even after doubling and quadrupling the number of
% exchanges required to complete the simulation. The time to
% completion only increases by 35\% after doubling and 117\% after
% quadrupling the number of exchanges.
%\athotanote{should the results
%  be included in the conclusion or in a separate results section? Do
%  you agree with the \# of exchanges scheme to show the data?}
% Unfortunately we have results only for Case II currently, but 

%In summary, we have established the ability to scale-out across different
%infrastructure and compared the performance of the asynchronous
%RE with the synchronous RE at large scales. 
Further, we compare the traditional RE model (Case I) and the centralized (Case II) and decentralized (Case III) models of the asynchronous replica exchange by modeling and repeating the experiments a reasonable number of times, so as to accurately quantify the scientific and performance gains. %We also propose to measure the frequency with which crosswalks occur with increasing number of replicas and measure the advantages due to a decentralized implementation in the full paper.


%With this asynchronous replica exchange mechanism we can improve the
%number of exchanges per unit time, a key parameter in judging the
%performance of a replica-exchange mechanism. \athotanote{is this
 % right? }  We are also going to have a wider group of replicas to
%look at for each replica as we are not pairing the replicas. Also, we
%have the usual advantages of using a pilot-job, such as reduced queue
%wait times by not having to submit to the queue.  Unfortunately we
%dont have results \jhanote{What results can we present -- any? some?},
%so we will say, (i) we establish the ability to scale-out (distributed
%and exa-scale) across different infrastructure (ii) compare the Async
%versus sync formulation at unprecedented scales \jhanote{At least
%  outline what infrastructure we / you are planning to use?} (iii)
%compare different implementations of the Async version
 

\begin{acknowledgement}
  ACK
\end{acknowledgement}

\bibliographystyle{kluwer}
\bibliography{saga,literature}    
\end{document}

