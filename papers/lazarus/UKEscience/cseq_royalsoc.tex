%\documentclass[conference,final]{IEEEtran}
%\documentclass[a4paper]{article}

\documentclass{rspublic}

\usepackage[utf8]{inputenc}
%\usepackage{graphicx}                                                                                             
\usepackage{url}
\usepackage{float}
\usepackage{times}
\usepackage{multirow}
\usepackage{listings}
\usepackage{times}
\usepackage{paralist}
\usepackage{wrapfig}
\usepackage[small,it]{caption}
\usepackage{multirow}
\usepackage{ifpdf}
\usepackage{subfig}
\usepackage[pdftex]{graphicx}
\usepackage{natbib}
\usepackage{listings}
\usepackage{keyval}
\usepackage{color}


\long\def\comment#1{{ \bf \textcolor{magenta}{\bf #1}}}
\long\def\ccomment#1{{ \bf \textcolor{blue}{\bf #1 (SJ)}}}
\newcommand{\F}[1]{\B{\textcolor{red}{FIXME: #1}}}
\newcommand{\C}{\comment}
\newcommand{\CC}{\ccomment}
\newcommand{\fix}[1]{\textcolor{red}{\bf #1}}
\newcommand{\tc}{$T_c$ }
\newcommand{\tcnsp}{$T_c$}

\setlength\parskip{-0.15em}
\setlength\parsep{-0.0em}
\newcommand{\upup}{\vspace*{-0.6em}}
\newcommand{\upp}{\vspace*{-0.6em}}
\newcommand{\up}{\vspace*{-0.3em}}

\newif\ifdraft
\drafttrue

\ifdraft
\newcommand{\fixme}[1]{ { \bf{ ***FIXME: #1 }} }
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***Jha: #1 }}}
\newcommand{\yyenote}[1]{ {\textcolor{blue} { ***yye00: #1 }}}
\else
\newcommand{\jhanote}[1]{}
\newcommand{\yyenote}[1]{}
\newcommand{\fixme}[1]{}
\fi

\newcommand{\jitter}[1]{{$\sigma(\alpha)$}}

\newif\ifpdf
\ifx\pdfoutput\undefined
  \pdffalse
\else
  \ifnum\pdfoutput=1
    \pdftrue
  \else
    \pdffalse
  \fi
\fi

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg}
\else
\DeclareGraphicsExtensions{.eps}
\fi

\title{Modelling Data-driven CO$_{2}$ Sequestration Using Distributed HPC Cyberinfrastructure}

\author[el-khamra, jha]{Yaakoub el-Khamra$^{1}$,Shantenu Jha$^{2,3,4*}$ \\
  \small{\emph{$^{1}$TACC, Austin, USA} \\ \emph{$^{2}$Center for
      Computation \& Technology, Louisiana
      State University, USA}\\
    \emph{$^{3}$Department of Computer Science, Louisiana State University, USA}\\
    \emph{$^{4}$e-Science Institute, Edinburgh, UK}} \\
{\footnotesize {\hspace{0.0 in} $^*$Corresponding Author sjha@cct.lsu.edu}}}

\begin{document}

\maketitle

\begin{abstract}{ensemble simulations, sequestration, distributed
    applications, scale-out} In this paper, we layout the
  computational challenges involved in effectively simulating complex
  phenomenon such as sequestrating CO$_2$ in existing oil
  reservoirs. The challenges arise at multiple levels: (i) the
  computational complexity of simulating the fundamental processes;
  (ii) the resource requirements of the computationally demanding
  simulations; (iii) the need for integrating real-time data
  (intensive) and computational intensive simulations; (iv) and the
  need to implement all of these in a robust, scalable and extensible
  approach. We will outline the architecture and implementation of the
  solution we develop in response to these requirements, and discuss
  results to validate claims that our solution scales to effectively
  solve desired problems sizes and thus provides the capability to
  generate novel scientific insight.\end{abstract}

\section{Introduction and Motivation}

Global energy needs today present serious challenges: the increasing
demand for energy must be met, however at the same time the emissions
of greenhouse gases into the atmosphere must be reduced. Even as
alternative energy sources continue to develop and gain popularity,
the fact remains that fossil carbon resources will continue to be in
heavy use (in both developing and industrialized countries) and
consequently generate large volumes of carbon dioxide
~\citep{GeoRPT}. The atmospheric impact of this greenhouse gas can be
abated through capturing and sequestering significant fractions of the
produced CO$_2$.

\jhanote{should we insert a graphic here of time dependence of energy
  demand?}

For long-term storage of large volumes of CO$_2$, porous subsurface
geologic formations are ideal candidates: these are the same
formations responsible for the existence of oil and gas
reservoirs. Indeed much the technology behind carbon dioxide
sequestration (including drilling, gas injection, reservoir management
and of course reservoir simulation) stems from drilling, petroleum and
reservoir engineering. Injecting CO$_2$ into an oil-gas reservoir can
also lead to improved oil recovery by ``pushing out'' the oil and gas
for production, this allows for reduced net cost through increased
revenues from oil and gas production ~\citep{EORBook}.

One of the major areas of research in this field is the
characterization of reservoirs that are safe and secure,
environmentally and geologically and are therefore promising
candidates for CO$_2$ sequestration ~\citep{GeoRPT,Luigi}. Our efforts
are directed towards developing cyberinfrastructure tools,
technologies and abstractions that facilitate large scale reservoir
characterization and forecasting studies.

\jhanote{Yaakoub: This is where the back-of-the-envelope quantitative
  estimates of computational requirements will be placed. Having
  established the scientific (and moral) imperative), we will now
  establish the computational challenge of the problem/solution at
  hand.}

Since the amount of information obtained directly from reservoirs is
very small compared to the actual size of the reservoir, history
matching techniques have been developed to match actual reservoir
production with simulated reservoir production, therefore obtaining a
more ``satisfactory'' set of models. One of the promising approaches
to history matching is the use of Ensemble Kalman filters (EnKF)
~\citep{KalmanPaper, DO2007, LiEnKF07, DO2006}.

Ensemble Kalman filters are recursive filters that can be used to
handle large, noisy data; the data in this case would be the results
and parameters from ensembles of reservoir models that are sent
through the filter to obtain the ``true '' state of the data. Since
the reservoir model varies from one ensemble to another, the run-time
characteristics of the ensemble simulation are irregular and hard to
predict. Furthermore, at simulation times when real historical data is
available, all the data from the different ensembles at that
simulation time must be compared to the actual production data, before
the simulations are allowed to proceed. This translates into a global
synchronization point for all ensembles; hence performing large scale
studies for complex reservoirs in a reasonable amount of time would
benefit greatly from the use of distributed, high performance, high
throughput and on-demand computing resources.

\begin{figure}
\begin{center}
\includegraphics*[scale=0.33,angle=0]{figures/3StageKalmanFilter}
\end{center}
\caption{Schematic illustrating the variability between stages of a typical
  ensemble Kalman filter based simulation. The end-to-end
  application consists of several stages; in general at each stage the
  number of models generated varies in size and duration.}
\label{fig:irregular_execution}
\end{figure}


\section{Meeting the Requirements: Design and Architecture}

Due to the complexity of the problem, involving reservoir simulation,
geostatistics on one end, and simulation management, workflows, grid
and high-throughput computing on another, a design decision was made
early in development to make full use of existing tools and
abstractions whenever possible. This resulted in a rapid development
cycle and faster turn-over. The components we developed and use are as
follows:


\section{Single Machine Versus Distributed Infrastructure}
\begin{figure}
\begin{center}
\includegraphics*[scale=0.5,angle=0]{figures/Figure7.png}
\end{center}
\caption{Time to completion with Lazarus. From Left to Right: (i)
  Ranger (ii) Ranger when using BQP, (iii) QueenBee, (iii) Ranger
  and QueenBee, (iv) Ranger, QueenBee and Abe, (v) Ranger, QueenBee 
  and Abe when using BQP.}
\label{fig:SingleVsDistributed}
\end{figure}

Consider a practical reservoir study that involves characterization, production forecast and sequestration forecast. Assuming we solve a one million grid cell model with our reservoir simulator, one iteration on 16 cores will require roughly 30 seconds. Also assume we will be performing data assimilation with observational data collected every month or 30 days. With 10 iterations per day and 30 days in a month, each model will require 2.5 hours on 16 cores. With 100 ensemble members, this equates to 250 hours (10 days) with a submitted BigJob size of 16 cores running one ensemble at a time or 2.5 hours with a submitted BigJob size of 1600 cores, all running concurrently.

Assume for now that we are working with the ranger supercomputing system at TACC. We would have access to 1600 cores for a maximum of 48 hours, which should allow us to run roughly 19 months worth of simulations. With 15 years of history matching, 15 years of forecasts and another 15 years of sequestration simulation, this totals to 540 months, or 29 BigJobs with 1600 core size and 48 hour duration. The wait time in the queue for such a job is, on average, 30 hours on ranger. If we submit the subsequent job to the queuing system after the current job is finished, the total duration of the entire study will therefore be roughly 95 days.

Certain improvements are possible obviously. Using three machines as a distributed infrastructure instead of just one machine, based on \ref{fig:SingleVsDistributed}, we can expect a reduction of the total time to completion by about a third, roughly 65 days. Futher optimization is of course possible.

\yyenote{need to give examples on further optimization}	


\section{Sensors}


To perform realtime reservoir characterization and forecast, live sensor data must be retrieved from the field and used in the data assimilation stage. The availability of new data also triggers a new EnKF stage in the workflow. To that end we implemented a communication layer based on data streaming from the data acquisition (sensor) platforms to a data spool in the form of a MySQL database ~\ref{fig:SensorRelay}. The EnKF application queries the data spool for the latest available sensor platform (field observations) data. Once the new field observations are retrieved they are labeled as ``assimilated'' to differentiate them from the next--in--line observations. It is worth mentioning that while currently we use a MySQL database, a more consistent solution would be to use the SAGA advert service, which would remove the need to access MySQL in Lazarus.

%With a high performance reservoir simulator, a parallel, scalable ensemble Kalman filter, and a workflow manager that provides access to an abundance of high performance and high throughput resources optimized for lowest total--time--to--completion, we are well equipped to perform live, real--time reservoir characterization based on direct sensor data.


\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{./figures/SensorRelay.png}
\end{center}
\caption[Sensors to Simulations data flow]{Real-time transfer of field sensor data to the EnKF application is a two stage process: streaming from sensor platforms to database and queries from the EnKF.}
\label{fig:SensorRelay}
\end{figure}

The use of an intermediate data--spool is necessary for many reasons: it is difficult to guarantee resource availability and synchronization with the EnKF. It is also prohibitively costly to maintain an active EnKF session throughout the life--time of the reservoir. With a data--spool we can also adjust the update rate at the EnKF session without losing any data from the reservoir. Updates to the models can be as immediate as the moment data is broadcast from the sensors in the field, or as late as when the reservoir is being abandoned. With immediate model updates, we can run forecast studies.

% \begin{figure}
% \begin{center}
% \includegraphics[scale=0.65]{./figures/LabView.png}
% \end{center}
% \caption[Sensor side LabVIEW vi]{Sensor side LabVIEW vi. To stream data from sensor platforms to data spools we use a LabVIEW vi that makes calls to a shared library containing an interface to the database (MySQL). Initial tests were conducted using a Wii mote 3--axis accelerometer. Image courtesy of Richard G. Duff }
% \label{fig:LabView}
% \up
% \up
% \end{figure}

%Special care had to be taken when interfacing to sensor platforms: there is no unified specification for communicating abstract sensor information. The simplest approach was to develop an interface to a common data acquisition application: LabVIEW from National Instruments ~\cite{LabVIEW}. To that end we developed a virtual instrument (vi) that makes function calls to a shared library which in turn interfaces to the MySQL database (Figure ~\ref{fig:LabView}). The vi has an adjustable streaming rate for polling sensor information and relaying it to the database, and is modular so it can be integrated in existing vi's as a sub--vi. This translates to painless integration with existing field and laboratory sensor platforms, in particular, the UCoMs sand tank experiment ~\cite{UCOMS}.

%It is worth mentioning that the intention here was to develop an abstract interface to sensor data that can accommodate everything from drill--string modelling ~\cite{DrillingLab}, well--heads, weather stations and buoys. This is one of the reasons why a ProdML ~\cite{ProdML} was not the first choice of interfaces. However, should the need arise, we can develop a direct ProdML to PETSc interface which will have no significant impact on the closed loop performance.





\begin{itemize}

\item The Reservoir Simulator: Based on the Cactus Code
  ~\citep{cactus_web} high performance scientific computing framework
  and the Portable Extensible Toolkit for Scientific Computing: PETSc
  ~\citep{PETSc}, the BlackOil reservoir simulator solves the
  equations for multiphase fluid flow through porous media, allowing
  us to simulate the movement of oil and gas in subsurface
  formations. With a few parameter changes, BlackOil is also used for
  modelling the flow of CO$_2$, however it cannot simulate any
  geochemical interaction. While adequate as a first order
  approximation, it is still under heavy development to enable it to
  satisfactorily model geochemical phenomena.

\item The Ensemble Kalman filter: Also based on Cactus and PETSc, it
  computes the Kalman gain matrix and updates the model parameters of
  the ensembles. The Kalman filter requires live production data from
  the reservoir for it to update the reservoir models in real-time,
  and launch the subsequent long-term forecast, enhanced oil recovery
  and CO$_2$ sequestration studies

\item Lazarus: Is a SAGA (Simple API for Grid Applications) based
  autonomic computing framework for scientific pplications. Lazarus
  utilizes SAGA , SAGA-based abstractions (BigJob) and provides an
  autonomic, self-configuring, self optimizing, self monitoring and
  self-healing job manager that facilitates the launch of history
  matching studies on distributed high performance, high throughput
  and grid computing resources ~\citep{gmac}

\item SensorsToSimulations (S2S): Developed, using Cactus and database
  interfaces, initially to send measurement-while-drilling (MWD) data
  from drilling bottom-hole-assemblies to drill-string simulators, the
  S2S framework has been extended to accommodate sending sensor data
  to the BlackOil reservoir simulator (production data required for
  well modelling) and the Ensemble Kalman filter for history
  matching. The S2S framework also announces the arrival of new
  history data to Lazarus to trigger the launching of a new stage of
  ensemble simulations ~\citep{Duff1,Duff2}.

\end{itemize}

\begin{figure}
\begin{center}
\includegraphics*[scale=0.7,angle=0]{figures/Picture2.png}
\end{center}
\caption{Schematic illustrating the interaction of BlackOil, EnKF, S2S
  and Lazarus. The blue arrows represent the flow of simulation data
  while the red arrows represent the flow of sensor data. \jhanote{We
    need a better figure than this. Where is the graffle figure that
    you promised? Chris White is a step ahead of you in that he uses
    graffle.. ahem}}
\label{fig:irregular_execution}
\end{figure}

When used in conjunction, these components allow us to perform
closed-loop large scale reservoir characterization studies as well as
long term forecast, enhanced oil recovery and CO$_2$ sequestration
studies. As more reservoir history data is generated during enhanced
oil recovery and CO$_2$ injection, the closed-loop can continue to run
and provide insight as the reservoir geological models are being
updated by the EnKF.  Continuously running the latest models in
forecast simulations with the latest sensor data will hopefully allow
us to predict un-intended movement of CO$_2$, leaks into aquifers,
critical pressure build-ups or huge pressure drops (fractures,
blow-outs etc.). To that end we use the BlackOil reservoir simulator
as an early prototype of what will eventually be an accurate simulator
that can model CO$_2$ sequestration, geochemical and geomechanical
phenomena. The S2S framework is also under heavy development to add
sensor metadata allowing Lazarus, through autonomic logic, to
distinguish between critical and non-critical sensor data, and
ultimately making use of on-demand computing resources (should the
sensor data prove critical).

% \jhanote{Need some details of how this ``autonomic decision making'' is supported?}
% \yyenote{haven't figure that bit out yet, but I am guessing I will just remove it}

\jhanote{Motivate why we need to use BigJob and Condor-GlideIn. The
  variability of sub-job sizes and requirements. We use BigJob on TG
  and Condor Glide-in on Condor resources}.

\section{Performance Data and Analysis:} 

We have previously run an EnKF based application on several TeraGrid
resources and demonstrated effective {\it Scale-out} across three
machines~\citep{gmac}.  Additionally, we incorporate the use of Condor
pools in Lazarus for even higher throughput, demonstrating possible
advantages to using multiple pilot job mechanisms.

\subsection{Performance Data} \jhanote{We could either reproduce the
  above data plot as is, or we could extend the data plot to include
  $T_c$ when Purdue's Condor Pool is introduced into the mix. I think
  there is some merit in extending to scale-out onto 4 resources (R,
  Q, A + Purdue) -- obviously without worrying about BQP. We don't
  have to use BigJob on the Purdue-Condor pool}

In this work, in addition to a different scientific problem, we
integrate (simulated) sensor data into the computational loop; the
time-dependent sensor-data dynamically drives the application, and
influences the controls and execution trajectory in phase-space.

These are amongst the first simulations
that use general purpose frameworks (as distinguished from specialized
frameworks, for example LEAD) on production infrastructure.

\subsection{Analysis} 

\jhanote{In this section we revisit the ``computational requirements
  of real problems'' as set out near the beginning. We then calibrate
  requirements based upon our results here -- showing (i) that we have
  met a fraction of the requirements, but, (ii) that we need to do
  much more in order to be able to meet the requirements laid out for
  real problems.}
  
{\it Conclusion: } A triad of unique aspects are developed in this
paper: (i) We incorporate the use of EnKF in reservoir
characterization for carbon dioxide sequestration studies, combining
two developing research areas and investigating the forefront (ii) We
extend our EnKF based approach further with the incorporation of live
sensor data that influences the entire study (iii) We introduce Condor
pools to our resources and use Condor's native pilot-job (Glide-In)
for Condor resources and SAGA's BigJob abstraction for Teragrid
resources transparently through Lazarus. 

\bibliographystyle{kluwer}
\bibliography{cseq_abstract}
\end{document}


\item \yyenote{ The problem is performing reservoir studies that are
    updated real-time, the application is the history-matching,
    forecasting and CO2 sequestration scenario. We want to use
    cyberinfrastructure because of several reasons: we have many
    simulations of complex reservoirs, that means we have to go
    parallel for higher throughput and use parallel processing. To be
    able to do this in real-time i.e. as the sensor data streams in,
    we have to have very fast turn-around for simulations that means
    high throughput, so we use everything at our disposal including
    grids which so far works. Other motivation, and this is more of a
    petroleum engineering perspective: if we cannot make use of
    existing tools and technologies today, chances are we will not be
    able to use new tools and technologies of tomorrow. We can
    elaborate on how we need cyberinfrastructure not just cycles: HPC
    for the simulator, grid and high throughput for many simulations
    and bigjob, databases for the sensor data i.e. data driven and
    data aware for Lazarus, as well as restart/fault-tolerance
    mechanisms, and of course sensors in the field. Now all I have to
    do is structure this so that it resembles a half defence
    paragraph. }

 \item Sensor driven application and workflow: the number of stages is controlled by 
sensors and the simulations themselves use sensor data

 \item Highly unpredictable: we have workflows that are changing dynamically and 
applications that use parameters from live data that we do not see before, everything will 
vary wildly


\item The sensor data can be quite critical: if you are injecting CO2 and suddenly (i.e. real-time suddenly) you lose well-head pressure: the CO2 is going somewhere, you want to find out where! It could be building up around the casing or going into a fracture, really nasty stuff can happen \jhanote{elaborate  addressed} \yyenote{The sensor data can contain critical information: pressure went to the roof or pressure went to zero, i.e. bad things will happen. We need to be able to identify these situations because for those cases obtaining simulation results ASAP is very important}

\item Sensor data can be simply faulty: a blown out fuse, a bad relay, and you get let's say pressure of -5 billion psi, quite nonphysical. Instead of wasting SU's on these bad things we need to halt and report the issue. Hence we need some form of intelligence to the sensor data: either through XML description of allowable/permissible ranges of the data, what range is normal, what range is critical and what range is abnormal (or faulty) and so on. Instrumentation engineering is a big discipline and what we are trying to do here is apply autonomic instrumentation awareness: the simulation and EnKF do not just pull data, they pull information, and some autonomy in reasoning what the information is is highly desirable (but lacking right now). This probably needs to be moved later on and put in the future work for the actual paper for e-science and maybe if I get time to work on this for the eventual paper for UK-escience.

\item While the application in this case is CO2 seq, this can be really anything in the real world EnKF for atmospherics, EnKF for radar, or simply hurricane simulations 

\item Keeping tabs, book-keeping is incredibly difficult, you will need to archive data effectively for larger runs (list this in the challenges) \item In terms of autonomics, we have: sensor driven workflow (i.e. throughput), sensor driven simulations, i.e. data-aware workflows and data-aware simulations. We do not have this now but a natural extension is checking whether the data is critical or not, this would make the application data-criticality aware (not just availability of data but nature of the data... need better term for that one) \jhanote{this needs a bit of clarification} \yyenote{a pressure loss of let's say -5 psi is not the same as -5000 psi, one would not cause panic the other would. It is a good idea to have a person at the helm who would notice that big of a drop and realize: hey we need the results of those simulations immediately, and jump the queues or do something special. What would be even great: Lazarus is made aware of this, does things so that everything is optimized for faster turnover and results are on hand asap. If the clarification is the application of EnKF: the method has its origins in atmospherics: radar, hurricane tracking, meteorology that sort of thing. This not being my domain I do not know much about it but all books on EnKF say this}

\item Need for autonomics is driven by the requirement to either respond to external sensors or internal compute data-output. \jhanote{Elaborate on how this arises, what this means..}

\item Input data drives the priority of the next state of simulations, based upon state of the application

\item We have (or will have soon once the admins fix it) condor, that should help with the on-demand computing issue by allowing us to harvest cycles for regular jobs and put the critical ones in the high-priority queues

\item One interesting feature: when we run this we are doing 2 things: history match then forecast, i.e. we are updating our ``study'' of the reservoir behavior dynamically as it evolves in realtime

\item Need to formulate the problem in terms of Application Objective, Mechanisms and Strategy.

\item Need for distributed computing when urgent -- ``little bit on
  all machines''. Need for autonomic decision making in determining
  the resource: Lazarus determines where to send the
  simulation. \jhanote{We need to say something about how this
    decision could be made in practise} \yyenote{Was hoping Ole's
    scheduling stuff would help, I have no idea how to begin to
    optimize this, perhaps a reverse scheduler? I don't know honestly}
  \jhanote{OK let us leave this out}

\item Application objective: match-making for ; mechanism:  Lazarus.. etc ; strategy: 

\end{itemize}

\section{Introduction}

The sensor data can be quite critical: if you are injecting CO2 and suddenly (i.e. real-time suddenly) you lose well-head pressure: the CO2 is going somewhere, you want to find out where! It could be building up around the casing or going into a fracture, really nasty stuff can happen
\begin{itemize}
 \item Based on that, we will need several things: frequent and dynamic updates of available sensor data (i.e. sensor platform, which we sort of have but needs more work, this is LabVIEW side...)
 \item On demand computing: You want to know if the CO2 is just moving to new places normally (i.e. you have low permeability and have to cross a threshold to get CO2 through there) or you broke the rock and CO2 is building around the casing of the well and the whole damn thing will explode
 \item Special handling of critical sensor data: we might need some
   sort of mechanism to distinguish low priority sensor data from
   critical sensor data, based on that, throw a token for spruce or
   switch to dedicated queues or something like that
 \item The data is sensitive and confidential, security is a big issue that we will need to address, same goes for database redundancy (you don't want all ensembles hitting a single DB for latest sensor data all at once, you want redundancy...)
\item Based on sensor data, we might want to run fire control models, emergency response models, kill-well/well-shutdown models etc... So analysis of sensor data before we jam it in the workflow might be important
\end{itemize}
}

