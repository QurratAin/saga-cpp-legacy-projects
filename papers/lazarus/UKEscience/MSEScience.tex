\documentclass[10pt,conference,final]{IEEEtran}
%\documentclass[a4paper]{article}
\linespread{1.3}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}

\clubpenalty=10000
\widowpenalty = 10000

%\usepackage[pdftex]{graphicx}
\usepackage{graphicx}
%\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[hypertex]{hyperref}
\usepackage{subfigure}
\usepackage{color}
\usepackage{pdfsync}
% \usepackage{draftcopy}

\usepackage[small,it]{caption}

\usepackage{multirow}
\usepackage{ifpdf}

\long\def\comment#1{{ \bf \textcolor{magenta}{\bf #1}}}
\long\def\ccomment#1{{ \bf \textcolor{blue}{\bf #1 (SJ)}}}
\newcommand{\F}[1]{\B{\textcolor{red}{FIXME: #1}}}
\newcommand{\C}{\comment}
\newcommand{\CC}{\ccomment}
\newcommand{\fix}[1]{\textcolor{red}{\bf #1}}
\newcommand{\tc}{$T_c$ }
\newcommand{\tcnsp}{$T_c$}

\setlength\parskip{-0.15em}
\setlength\parsep{-0.0em}
\newcommand{\upup}{\vspace*{-0.6em}}
\newcommand{\upp}{\vspace*{-0.6em}}
\newcommand{\up}{\vspace*{-0.3em}}

\newif\ifdraft
%\drafttrue

\ifdraft
\newcommand{\fixme}[1]{ { \bf{ ***FIXME: #1 }} }
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***Jha: #1 }}}
\newcommand{\yyenote}[1]{ {\textcolor{blue} { ***yye00: #1 }}}
\else
\newcommand{\jhanote}[1]{}
\newcommand{\yyenote}[1]{}
\newcommand{\fixme}[1]{}
\fi

\newcommand{\jitter}[1]{{$\sigma(\alpha)$}}

\newif\ifpdf
\ifx\pdfoutput\undefined
  \pdffalse
\else
  \ifnum\pdfoutput=1
    \pdftrue
  \else
    \pdffalse
  \fi
\fi

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg}
\else
\DeclareGraphicsExtensions{.eps}
\fi

\begin{document}

\title{Modelling Data-driven CO$_{2}$ Sequestration Using Distributed HPC CyberInfrastructure}

 \author{\authorblockN{Yaakoub El Khamra\authorrefmark{1}\authorrefmark{2}, Shantenu Jha\authorrefmark{2}\authorrefmark{3}\authorrefmark{4}},
   \authorblockA{\authorrefmark{2} Center for Computation and
     Technology, Louisiana State University, Baton Rouge, 70803}
   \authorblockA{\authorrefmark{3} Department of Computer Science,
     Louisiana State University, Baton Rouge, 70803}
   \authorblockA{\authorrefmark{4} e-Science Institute, Edinburgh, UK}
   \authorblockA{\authorrefmark{1} Texas Advanced Computing Center,
     University of Texas, Austin, 78758} }


\maketitle

\begin{abstract} As carbon dioxide emissions rise due to soaring energy production, research in efficient, environmentally safe carbon dioxide sequestration technology has intensified.  One active area of research is the sequesteration of CO$_2$ in subsurface geologic formations, namely oil and gas reservoirs. To model the sequestration process, an accurate set of models of the reservoirs have to be generated and modified to agree with the reservoirs' production history, ideally as the production data is streamed live from sensors in the field. Thus the effective simulation of such complex processes is both computationally intensive and requires heteregenous computing models and infrastructure.  Our work is centered on developing the CyberInfrastructure -- tools, technologies and abstractions, that enable large-scale reservoir characterization and CO$_2$ sequestration studies, % as well as This infrastructure enables the
while seamlessly and concurrently using distributed, heterogeneous high-throughput computing resources and live, real-time sensor data.  
\end{abstract}


\section*{Carbon Dioxide Sequestration}
Global energy needs today present serious challenges: the increasing demand for energy 
must be met, while at the same time the 
emissions of greenhouse gases into the atmosphere must be reduced.
Even as alternative energy sources continue to develop and gain 
popularity, the fact remains that fossil carbon resources will continue to be in heavy use 
(in both developing and industrialized countries) and consequently generate 
large volumes of carbon dioxide ~\cite{GeoRPT}. The atmospheric impact of this greenhouse gas 
can be abated through capturing and sequestering significant fractions of the produced 
CO$_2$.

For long-term storage of large volumes of CO$_2$, porous subsurface geologic formations are ideal candidates: these are the same formations responsible for the existence of oil and gas reservoirs. Indeed much of the technology behind carbon dioxide sequestration (including drilling, gas injection, reservoir management and reservoir simulation) stems from drilling, petroleum and reservoir engineering. Injecting CO$_2$ into an oil-gas reservoir also leads to improved oil recovery by ``pushing out'' the oil and gas thereby reducing the net cost through increased revenues from oil and gas production ~\cite{EORBook}.

One of the major areas of research in this field is the characterization of reservoirs that are safe and secure, environmentally and geologically, \jhanote{Yaakoub, this sentence does not make sense: please check?} \yyenote{Yes it makes sense: the research area is characterization of reservoirs that meet the criteria: are safe and secure both environmentally and geologically: they will keep the co2 in place and will not fracture}  and are therefore promising candidates for CO$_2$ sequestration ~\cite{GeoRPT,Luigi}. Our efforts are directed towards developing CyberInfrastructure -- the tools, technologies and abstractions, that facilitate large scale reservoir characterization and forecasting studies.

\section*{Reservoir Characterization}
\yyenote{Maybe this is better} Direct information about any given reservoir is usually gathered through logging and measurement tools including core samples, thus restricted to a small portion of the actual reservoir size, namely the well-bore. For this reason, ``history matching'' techniques have been developed to ``match'' actual reservoir production with simulated reservoir production, therefore obtaining a more ``satisfactory'' set of reservoir models.  Ensemble Kalman filters (EnKF) represent a promising approach to history matching ~\cite{KalmanPaper, DO2007, LiEnKF07, DO2006}.

% One of the promising approaches to history
% matching is the use of Ensemble Kalman filters (EnKF) ~\cite{KalmanPaper, DO2007, LiEnKF07, DO2006}.


%Since the amount of information obtained directly from reservoirs (through logging, core samples and so on) is very small compared to the actual size of the reservoir, \jhanote{previous sentence doesn't read well, and isn't clear to me} \yyenote{The actual info we get directly, i.e. through measurement of the reservoir is small at best we get a few core samples from when we drill. The drill bit diameter is a few inches, the size of the reservoir is in acres} history matching techniques have been developed to match actual reservoir production with simulated reservoir production, therefore obtaining a more ``satisfactory'' set of reservoir models. One of the promising approaches to history matching is the use of Ensemble Kalman filters (EnKF) ~\cite{KalmanPaper, DO2007, LiEnKF07, DO2006}.

Ensemble Kalman filters are recursive filters that can be used to handle large, noisy data; the data in this case are the results and parameters from ensembles of reservoir models that are sent through the filter to obtain the ``true state'' of the data. Since the reservoir model varies from one ensemble to another, the run-time characteristics of the ensemble simulation are irregular and hard to predict. Furthermore, during simulations when real historical data is available, all the data from the different ensembles at that simulation time must be compared to the actual production data, before the simulations are allowed to proceed. This translates into a global synchronization point for all ensemble-members in any given stage. Due to this fundamental limit on task-parallelism, performing large scale studies for complex reservoirs in a reasonable amount of time would benefit greatly from the use of distributed, high-performance, high-throughput and on-demand computing resources.

\begin{figure}
\begin{center}
\includegraphics*[scale=0.33,angle=0]{figures/3StageKalmanFilter}
\end{center}
\caption{Schematic illustrating the variability between stages of a typical
  ensemble Kalman filter based simulation. The end-to-end
  application consists of several stages; in general at each stage the
  number of models generated varies in size and duration.}
\label{fig:irregular_execution}
\end{figure}

\section*{CyberInfrastructure used}

Due to the complexity of the problem -- involving reservoir simulation and geostatistics on one hand and simulation management, workflows, grid and high-throughput computing on the other, a design decision was made early in development to make full use of existing tools and abstractions whenever possible. This resulted in a rapid development cycle and faster turn-over. The components we developed and integrated are as follows:

\begin{itemize}
\item The Reservoir Simulator: The BlackOil reservoir simulator \jhanote{BlackOil simulator is never fully defined. Is that a functional description? Is that just a name?} \yyenote{It is both Black Oil i.e the black oil model and the rest is described in the following sentence: it solves...} solves the equations for multiphase fluid flow through porous media, allowing us to simulate the movement of oil and gas in subsurface formations. It is based on the Cactus Code ~\cite{cactus_web} high performance scientific computing framework and the Portable Extensible Toolkit for Scientific Computing: PETSc ~\cite{PETSc}. With a few parameter changes, BlackOil is also used for modelling the flow of CO$_2$; however it does not simulate any geochemical interactions. While adequate as a first order approximation, it is still under intense development to enable it to satisfactorily model geochemical phenomena.

\item The Ensemble Kalman filter: Also based on Cactus and PETSc, it computes the Kalman 
gain matrix and updates the model parameters of the ensembles. The Kalman filter requires 
live production data from the reservoir for it to update the reservoir models in 
real-time, and launch the subsequent long-term forecast, enhanced oil recovery and CO$_2$ 
sequestration studies.

\item Lazarus: Is a SAGA (Simple API for Grid Applications) based autonomic computing 
framework for scientific applications. Lazarus utilizes SAGA, SAGA-based 
abstractions (BigJob) and provides an autonomic, self-configuring, self optimizing, self 
monitoring and self-healing job manager that facilitates the launch of history matching 
studies on distributed high performance, high throughput and grid computing resources 
~\cite{gmac}.

\item SensorsToSimulations (S2S) framework: Developed initially using Cactus and database interfaces, to send measurement-while-drilling (MWD) data from drilling bottom-hole-assemblies to drill-string simulators, the S2S framework has been extended to accommodate sending sensor data to the BlackOil reservoir simulator (production data required for well modelling) and the Ensemble Kalman filter for history matching. The S2S framework also announces the arrival of new history data to Lazarus to launch a new stage of ensemble simulations ~\cite{Duff2,Duff1}.

\end{itemize}

\begin{figure}
\begin{center}
\includegraphics*[scale=0.65,angle=0]{figures/Picture2.png}
\end{center}
\caption{Schematic illustrating the interaction of BlackOil, EnKF, S2S and Lazarus. The 
blue arrows represent the flow of 
simulation data while the red arrows represent the flow of sensor data.}
\label{fig:Data Flow}
\end{figure}

\section*{The Closed Loop}

When used in conjunction, these components allow us to perform closed-loop large scale reservoir characterization studies as well as long term forecast, enhanced oil recovery and CO$_2$ sequestration studies. As more reservoir history data is generated during enhanced oil recovery and CO$_2$ injection, this closed-loop can continue to run and provide insight as the reservoir geological models are being updated by the EnKF.  Continuously running the latest models in forecast simulations with the latest sensor data will hopefully enable predicting un-intended movement of CO$_2$, leaks into aquifers, critical pressure build-ups or huge pressure drops (fractures, blow-outs etc.). To that end we use the BlackOil reservoir simulator as an early prototype of what will eventually be an accurate simulator that can model CO$_2$ sequestration, geochemical and geomechanical phenomena. The S2S framework is also under intense development to include sensor metadata allowing Lazarus, through autonomic logic, to distinguish between critical and non-critical sensor data, and ultimately making use of on-demand computing resources (should the sensor data prove critical).

% \jhanote{Need some details of how this ``autonomic decision making'' is supported?}
% \yyenote{haven't figure that bit out yet, but I am guessing I will just remove it}

% \jhanote{This does not sound good for an abstract. We should use words like, ``early prototype is available''..}  \yyenote{fixed}

% \jhanote{Motivate why we need to use BigJob and Condor-GlideIn. The variability of
% sub-job sizes and requirements. We use BigJob on TG and Condor Glide-in on Condor
% resources}.

\section*{Conclusions and Future Work}

%{\it Performance Data: }
We have previously run an EnKF based application on several TeraGrid resources and demonstrated effective {\it Scale-out} across three machines~\cite{gmac}. In this work, in addition to a different scientific problem, we integrated (simulated) sensor data into the computational loop. The time-dependent sensor-data dynamically drives the application, and influences the controls and execution trajectory in phase-space. Additionally, to support higher throughput for a larger distribution of 
job sizes, we incorporated Condor pools as part of the infrastructure used,
% we incorporate the use of Condor pools
% in Lazarus for even higher throughput, 
demonstrating possible advantages to using multiple pilot job mechanisms. These are amongst the first simulations that use general purpose frameworks (as distinguished from specialized frameworks, for example LEAD) on production infrastructure.

% \jhanote{We know that there will be dynamically driven data, but we should establish here, how our performance data/tests will be different from GMAC.  Will we just use the same metric, Time-to-solution as the number of resources are increased? More than 3 TG resources? Will we use BQP? What are the newer/different Autonomic techniques that we will support, implement} \yyenote{what scenario do you want to run here? Condor+TG versus TG alone?}

%{\it Conclusion: }
A triad of unique aspects were developed : (i) We incorporate the use of EnKF in reservoir characterization for carbon dioxide sequestration studies, % combining two developing research areas and investigating the forefront
(ii) The EnKF based approach further with the incorporation of live sensor data thus making it capable of simulating more realistic scenarios, (iii) The infrastructure
utilized for these simulations transcends HPC and HTC Grids, as evidenced by the
concurrent usage of multiple pilot-jobs on Condor pools and TeraGrid resources via
the SAGA-based BigJob abstraction.
% We introduce Condor pools to our resources and use Condor's native pilot-job (Glide-In) for Condor resources and SAGA's BigJob abstraction for Teragrid resources transparently through Lazarus. 
We believe this is a novel if not the first demonstration of the use of two-different pilot job mechanisms towards the solution of the same problem.

% \begin{keywords}
%     Distributed and Autonomic Applications, Distributed Application
%     Programming, SAGA
% \end{keywords}

\bibliographystyle{IEEEtran} 
\bibliography{cseq_abstract}
\end{document}

\section*{Notes (this will become the abstract)}

\begin{itemize}


\item \yyenote{ The problem is performing reservoir studies that are updated real-time, 
the application is the history-matching, forecasting and CO2 sequestration scenario. We 
want to use cyberinfrastructure because of several reasons: we have many simulations of 
complex reservoirs, that means we have to go parallel for higher throughput and use parallel processing. To be able to do this in real-time i.e. as the sensor data streams 
in, we have to have very fast turn-around for simulations that means high throughput, so 
we use everything at our disposal including grids which so far works. Other motivation, 
and this is more of a petroleum engineering perspective: if we cannot make use of existing 
tools and technologies today, chances are we will not be able to use new tools and 
technologies of tomorrow. We can elaborate on how we need cyberinfrastructure not just 
cycles: HPC for the simulator, grid and high throughput for many simulations and bigjob, 
databases for the sensor data i.e. data driven and data aware for Lazarus, as well as 
restart/fault-tolerance mechanisms, and of course sensors in the field. Now all I have to 
do is structure this so that it resembles a half defence paragraph. }

 \item Sensor driven application and workflow: the number of stages is controlled by 
sensors and the simulations themselves use sensor data

 \item Highly unpredictable: we have workflows that are changing dynamically and 
applications that use parameters from live data that we do not see before, everything will 
vary wildly

% \item \yyenote{ The simulation components are as follows:}
%   \begin{itemize}
%     \item Reservoir Simulator: a blackoil reservoir simulator that simulates three components in three phases: oil, water gas system. The Reservoir Simulator is used for forecasting production and for simulating CO2 sequestration by making the assumption that CO2 is soluble in oil but not water. This simplification, while crude, is a first order approximation and for our purposes will do for now. In the future, the blackoil reservoir simulator will be replaced with a compositional reservoir simulator that will be more accurate and allow us to resolve more physics. This does not affect anything in the workflow, it just gives us better results. The Reservoir simulator also needs to pull live sensor data since it constitutes part of the model parameters (eg. bottom hole pressure for production)
%     \item The EnKF: the ensemble Kalman Filter executable uses historical production data (and later on live, real-time sensor data) to compute the Kalman gain matrix and update the ensemble parameters (that are used in the Reservoir simulator)
%     \item The workflow manager: a mechansim for launching jobs and retrieving data, orchestrating new stages based on availability of new sensor data. The workflow also updates the forecast: i.e. runs models until reservoir is depleted and then runs the CO2 sequestration study
%   \end{itemize}

\item The sensor data can be quite critical: if you are injecting CO2 and suddenly (i.e. real-time suddenly) you lose well-head pressure: the CO2 is going somewhere, you want to find out where! It could be building up around the casing or going into a fracture, really nasty stuff can happen \jhanote{elaborate  addressed} \yyenote{The sensor data can contain critical information: pressure went to the roof or pressure went to zero, i.e. bad things will happen. We need to be able to identify these situations because for those cases obtaining simulation results ASAP is very important}

\item Sensor data can be simply faulty: a blown out fuse, a bad relay, and you get let's say pressure of -5 billion psi, quite nonphysical. Instead of wasting SU's on these bad things we need to halt and report the issue. Hence we need some form of intelligence to the sensor data: either through XML description of allowable/permissible ranges of the data, what range is normal, what range is critical and what range is abnormal (or faulty) and so on. Instrumentation engineering is a big discipline and what we are trying to do here is apply autonomic instrumentation awareness: the simulation and EnKF do not just pull data, they pull information, and some autonomy in reasoning what the information is is highly desirable (but lacking right now). This probably needs to be moved later on and put in the future work for the actual paper for e-science and maybe if I get time to work on this for the eventual paper for UK-escience.

\item While the application in this case is CO2 seq, this can be really anything in the real world EnKF for atmospherics, EnKF for radar, or simply hurricane simulations 

\item Keeping tabs, book-keeping is incredibly difficult, you will need to archive data effectively for larger runs (list this in the challenges) \item In terms of autonomics, we have: sensor driven workflow (i.e. throughput), sensor driven simulations, i.e. data-aware workflows and data-aware simulations. We do not have this now but a natural extension is checking whether the data is critical or not, this would make the application data-criticality aware (not just availability of data but nature of the data... need better term for that one) \jhanote{this needs a bit of clarification} \yyenote{a pressure loss of let's say -5 psi is not the same as -5000 psi, one would not cause panic the other would. It is a good idea to have a person at the helm who would notice that big of a drop and realize: hey we need the results of those simulations immediately, and jump the queues or do something special. What would be even great: Lazarus is made aware of this, does things so that everything is optimized for faster turnover and results are on hand asap. If the clarification is the application of EnKF: the method has its origins in atmospherics: radar, hurricane tracking, meteorology that sort of thing. This not being my domain I do not know much about it but all books on EnKF say this}

\item Need for autonomics is driven by the requirement to either respond to external sensors or internal compute data-output. \jhanote{Elaborate on how this arises, what this means..}

\item Input data drives the priority of the next state of simulations, based upon state of the application

\item We have (or will have soon once the admins fix it) condor, that should help with the on-demand computing issue by allowing us to harvest cycles for regular jobs and put the critical ones in the high-priority queues

\item One interesting feature: when we run this we are doing 2 things: history match then forecast, i.e. we are updating our ``study'' of the reservoir behavior dynamically as it evolves in realtime

\item Need to formulate the problem in terms of Application Objective, Mechanisms and Strategy.

% \item The three Unique things about this paper are (i) The fact that we are studying C02 Sequestration using EnKF (EnKF-CS) one of the first to do so, (ii) Effective/proper solution of EnKF-CS, imposes interesting additional requirements, over and above EnKF for History Matching -- that of sensors \& experimentally driven data. We incorporate this additional requirement, using the same programming system (SAGA) as used for EnkF-HM thus justifying claims of extensibility, and (iii) Due to distribution of sub-problems (task), we introduce Condor pools in the mix of resources we utilize and interestingly, we utilize Condor's native pilot-job (Glide-In) when using Condor resources but SAGA's BigJob abstraction when using TG resources (which opens the question, why can't we use Condor's Pilot-Job on Ranger, QueenBee ?). We beleieve this is a novel if not the first demonstration of the use of two-different pilot job mechanims towards the solution of the same problem.

\item Need for distributed computing when urgent -- ``little bit on all machines''. Need for autonomic decision making in determining the resource: Lazarus determines where to send the simulation. \jhanote{We need to say something about how this decision could be made in practise} \yyenote{Was hoping Ole's scheduling stuff would help, I have no idea how to begin to optimize this, perhaps a reverse scheduler? I don't know honestly} \jhanote{OK let us leave this out}

\item Application objective: match-making for ; mechanism:  Lazarus.. etc ; strategy: 

\end{itemize}

\section{Introduction}

The sensor data can be quite critical: if you are injecting CO2 and suddenly (i.e. real-time suddenly) you lose well-head pressure: the CO2 is going somewhere, you want to find out where! It could be building up around the casing or going into a fracture, really nasty stuff can happen
\begin{itemize}
 \item Based on that, we will need several things: frequent and dynamic updates of available sensor data (i.e. sensor platform, which we sort of have but needs more work, this is LabVIEW side...)
 \item On demand computing: You want to know if the CO2 is just moving to new places normally (i.e. you have low permeability and have to cross a threshold to get CO2 through there) or you broke the rock and CO2 is building around the casing of the well and the whole damn thing will explode
 \item Special handling of critical sensor data: we might need some sort of mechanism to distinguish low priority sensor data from critical sensor data, based on that, throw a token for spruce or switch to dedicated queues or something like that
 \item The data is sensitive and confidential, security is a big issue that we will need to address, same goes for database redundancy (you don't want all ensembles hitting a single DB for latest sensor data all at once, you want redundancy...)
\item Based on sensor data, we might want to run fire control models, emergency response models, kill-well/well-shutdown models etc... So analysis of sensor data before we jam it in the workflow might be important
\end{itemize}

\jhanote{Introduction will not be submitted}

