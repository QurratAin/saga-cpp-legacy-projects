\relax 
\citation{Thompson}
\citation{Nie}
\citation{Yen}
\citation{Steijl}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction and Motivation}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Steady Couette Flow Solution by Pure Molecular Dynamic Simulation; In normal condition (with the unique potential well depth between flow particle and surface material, denoted by $\epsilon $), the solution by MD is identical to the result of continuum approach. In hydrophobic case (smaller $\epsilon $) the fluid shows a slight slip on the wall, while the fluid particle strongly attempts to stick to the wall in hydrophilic case (bigger $\epsilon $).}}{2}}
\newlabel{MD_Solution_New}{{1}{2}}
\citation{Thompson}
\citation{Nie}
\citation{Yen}
\citation{Steijl}
\citation{Flekkoy}
\citation{Wagner}
\citation{Hadjicon1}
\citation{Hadjicon2}
\citation{Wang}
\@writefile{toc}{\contentsline {section}{\numberline {II}A Hybrid CFD-MD Approach - Numerical Technique for Multi-scale Flow Simulation}{3}}
\citation{ogf_web}
\@writefile{toc}{\contentsline {section}{\numberline {III}SAGA and SAGA-based Frameworks - An Efficient Runtime Environment for Coupled Multi-component Computations}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Schematic Diagram of the Hybrid Domain with Detailed View of Overlapping Zone; Overall continuum/atomistic computational domain including overlap region is shown on left figure. Detailed layer by layer explanation of overlapping region is indicated by right figure.}}{4}}
\newlabel{Fig:Couette}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Layered schematic of the different components of the SAGA landscape. At the topmost level is the simple integrated API which provides the basic functionality for distributed computing. Our BigJob abstraction is built upon this SAGA layer using Python API wrapper}}{4}}
\newlabel{Fig:SAGA1}{{3}{4}}
\citation{saga_royalsoc}
\citation{PseudoCompressibility}
\citation{LU-SGS}
\citation{Osher}
\citation{MUSCL}
\citation{Allen}
\citation{Haile}
\citation{Rapaport}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Development of A Hybrid CFD-MD Simulation Toolkit}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Features of Baseline CFD and MD Codes}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Architecture of the Controller/Manager and Control Flow: Application manager is responsible for job management including BigJob and sub-job submission, their status monitoring functions. We implement a load balancing module, and migration service based on job information. Application agent system resides on each HPC resource and conducts job information gathering and also communicates with the application manager via the advert service}}{5}}
\newlabel{Fig:BigJob_Structure}{{4}{5}}
\citation{LAMMPS}
\citation{Chimera}
\newlabel{eq:Newton}{{3}{6}}
\newlabel{eq:PEnergy}{{4}{6}}
\newlabel{eq:LJ12}{{5}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Implementation of Hybrid Schemes}{6}}
\newlabel{eq:Con_vel}{{6}{6}}
\newlabel{eq:Con_vel}{{7}{6}}
\citation{Time_Mechanism}
\citation{Ko}
\newlabel{eq:Lagrangian}{{8}{7}}
\newlabel{eq:Con2}{{9}{7}}
\newlabel{eq:Con2}{{10}{7}}
\newlabel{eq:Con3}{{11}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Time Evolution Mechanism of Application Codes; CFD and MD codes are scheduled to conduct data exchange in the overlapping region at every $\Delta {t}$ time. CFD and MD solvers do sub-iterations until approaching the next data exchange point, by $I_{CFD}$ and $I_{MD}$. CFD solution at $t-\Delta {t}$ is directly applied as the boundary condition for MD simulation from $t-\Delta {t}$ to $t$, while averaged molecular dynamic velocity on $S_{MD}\times \Delta {t_{MD}}$ durations are imposed as CFD boundary conditions at this time instance.}}{7}}
\newlabel{Hybrid_Timescale}{{5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Construction of a Coupled Multi-physics Simulation Framework}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Load Balancing for Coupled Multi-Physics Simulation}{7}}
\citation{Nie}
\citation{Yen}
\newlabel{eq:SimTime_EachTask}{{12}{8}}
\newlabel{eq:PECondition}{{13}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Implementation of an Execution Framework to Support Multi-physics Applications}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Comparison of dependencies encountered for coupled simulations submitted as conventional jobs, to the scenario when using Pilot-Jobs. Here we use only 1 BigJob (S1). The conventional mode of submission experiences three phases based upon their queue state: (i) All jobs are waiting: ($t_1-t_0$); (ii) Inactive mode (where one job is waiting for another: $t_2-t_1$), and (iii) Active mode (running a coupled simulation: $t_f-t_2$). The Inactive stage, disappears when a coupled simulation runs within a BigJob, as an allocated BigJob distributes its resource to both sub-jobs.}}{9}}
\newlabel{Fig:OneBJ_Flow}{{6}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Multi-scale Flow Simulation and Its Performance}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-A}}Numerical Result of a Nano-scale Couette Flow Simulation}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  L2 Norm of Averaged Velocity with Different Layer Sizes and Sampling Duration, at $\Delta {t_{MD}}=5\times {10^{3}}$ (Left) and $\Delta {t_{MD}}=1\times {10^{3}}$ (Right); Solution tends to produce less noise with bigger layer size and smaller MD timescale in short sampling duration. As sampling interval is increased, all solutions converges to the same fluctuation strength, which is 0.01$\sigma /\tau $.}}{10}}
\newlabel{MD_Regular_Vel0}{{7}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Computational Domain of Couette Flow Simulation; The height of the fluid domain is 52$\sigma $ ($\approx $177$\r A$). CFD mesh size is 201$\times $27 and CFD cells at the pure MD region are treated as holes. MD domain size is about 210$\sigma $ in the X-direction and around 30$\sigma $ in the Y-direction, including the bottom wall. Periodic boundary condition is applied on the principal flow direction.}}{10}}
\newlabel{Couette_Val_Domain}{{8}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-B}}Performance Analysis of a Multi-physics Simulation Framework}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Unsteady Couette Flow Profile; (Left) Pure CFD solution is exactly the same as analytic solution. MD solution shows the same flow pattern as analytic solution, though some fluctuation is observed. This verifies that CFD and MD represents the same flow physics. (Right) The steady result by hybrid approach produces the same numerical result as analytic solution, though the time-lagging in the hybrid boundary is observed during the evolution.}}{11}}
\newlabel{Flat_Plate_Sol}{{9}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Unsteady Flow Profile of Stokes Boundary Layer Problem; Solution by pure CFD simulation is denoted by a solid line and solution by hybrid simulation is presented by symbols. Hybrid flow profile globally matches well with CFD solution, though slight time-lagging in the MD side is observed. This evaluates that current hybrid approach can be applied to purely unsteady problem.}}{11}}
\newlabel{Stokes_Sol}{{10}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Effect of various configurations on waiting time. The tables show the queue waiting times on Ranger and QueenBee with a changing number of processors and different requested resource wall-time limits. Analyzing the actual waiting time as a function of the number of processors at different wall-time limits, it can be said that better more often than not, the waiting time decrease as the requested number of processors increases. The relationship between the waiting time and wall-time limit is harder to quantify. However, obtained numbers provide a good case study for showing the variance of actual queue waiting times.}}{12}}
\newlabel{table:waitingtime}{{I}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Waiting and inactive time for conventional job submissions, and a single BigJob submission. All measured times are in seconds and expressed as 'mean$\pm $SD'. In both cases, conventional job is submitted to use 2$\times $32 px and a BigJob requests 64 px on small and less crowded LONI clusters. Conventional job submission mode showed faster time-to-start (i.e., the sum of waiting time and inactive mode) on small problem size with 6 hr of wall-time limit, while a BigJob gets allocated faster with longer wall-time limit. Conventional job submission showed 3 failures during the test due to the timeover of wall-time limit in the first-started job.}}{12}}
\newlabel{table:BJwaiting}{{II}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Results of runtime for S1, $S1_{LB}$ and conventional submission. All measured times are in seconds and expressed as 'mean$\pm $SD'. 6 distinct experiments were accomplished for each simulation, all with 64 px. In both cases, S1 shows about 1\% overhead due to the communication with advert server. On the other hand, $S1_{LB}$ tests show about 13\% runtime save compared to conventional submission.}}{13}}
\newlabel{table:oneBJ_Test}{{III}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  Change of processor distribution between CFD and MD jobs and resultant computation time in the small simulation. A load balancer starts from the same number of processors assigned to both sub-jobs and detects 20 to 44 px between each sub-job as the optimal solution. The poor scalability of CFD job makes the load balancer to search for the optimal condition once again and the processor assignment finally reaches to a steady solution of 12 to 52 between two sub-jobs. Computation time for every simulation loop reduces from 153 sec to 107 sec after the balancing.}}{13}}
\newlabel{Fig:LBSmall}{{11}{13}}
\citation{ex1}
\citation{ex2}
\bibstyle{IEEEtran}
\bibdata{saga_tg08}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  (Left Column) Change of processor distribution between CFD and MD jobs and resultant computation time in the large simulation. A load balancer directly finds the processor distribution of 24 to 40 between CFD and MD jobs and remains the steady state until it completes after 25 simulation loops. Initial computation time of 1605 sec reduces to 1320 sec after the convergence. (Right Column) Plots showing non-monotonic resource assignment by the LB, and thus demonstrating how the load balancer can be self-correcting and adapt to changing performance; after increasing the number of processors assigned to the MD, the load-balancer unassigns the additional processors.}}{14}}
\newlabel{Fig:LBLarge}{{12}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Next Step: Further Refinement}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Conclusions}{14}}
