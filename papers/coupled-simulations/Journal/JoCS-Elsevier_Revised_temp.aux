\relax 
\emailauthor{sko@nsc.liu.se}{Soon-Heum Ko}
\emailauthor{nykim@cct.lsu.edu}{Nayong Kim}
\emailauthor{meniki@me.lsu.edu}{Dimitris Nikitopoulos}
\emailauthor{moldovan@me.lsu.edu}{Dorel Moldovan}
\emailauthor{sjha@cct.lsu.edu}{Shantenu Jha\corref {Corr}}
\Newlabel{Corr}{1}
\Newlabel{add1}{a}
\Newlabel{add0}{b}
\Newlabel{add2}{c}
\Newlabel{add3}{d}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction and Motivation}{2}}
\newlabel{sec:intro}{{1}{2}}
\citation{Thompson}
\citation{Nie}
\citation{Nie_cavity}
\citation{Cui}
\citation{Wang}
\citation{Yen}
\citation{Liu}
\citation{Hadjicon1}
\citation{Hadjicon2}
\citation{Hadjicon3}
\citation{Werder}
\citation{Kotsalis}
\citation{Flekkoy}
\citation{Wagner}
\citation{Delgado1}
\citation{USHER}
\citation{Time_Mechanism}
\citation{Giupponi}
\citation{Thompson}
\citation{Nie}
\citation{Thompson}
\citation{Nie}
\citation{Nie_cavity}
\citation{Yen}
\citation{Wang}
\citation{Liu}
\citation{Flekkoy}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Previous Hybrid CFD-MD Implementations}{3}}
\citation{Hadjicon1}
\citation{Hadjicon2}
\citation{Hadjicon3}
\citation{Hadjicon2}
\citation{Werder}
\citation{Thompson}
\citation{Nie}
\citation{Flekkoy}
\citation{Delgado1}
\citation{Hadjicon2}
\citation{Hadjicon3}
\citation{Werder}
\citation{Kotsalis}
\citation{Hadjicon2}
\citation{Flekkoy}
\citation{Wagner}
\citation{Delgado1}
\citation{USHER}
\citation{Flekkoy}
\citation{Delgado1}
\citation{Time_Mechanism}
\citation{Flekkoy}
\citation{Hadjicon3}
\citation{Hadjicon3}
\citation{Time_Mechanism}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Numerical and Computational Issues Associated with Hybrid CFD-MD Methodologies}{4}}
\citation{Liu}
\citation{Wang}
\citation{Thompson}
\citation{Cui}
\citation{Wang}
\citation{Yen}
\citation{Hadjicon3}
\citation{Time_Mechanism}
\citation{Liu}
\citation{Wang}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Objectives and Outline of the Paper}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Fundamentals of the Hybrid Coupled CFD-MD Simulation Toolkit}{8}}
\newlabel{sec:numerical}{{2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Overview of the Continuum-Molecular Coupled Approach}{9}}
\citation{PseudoCompressibility}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Governing Equations and Numerical Schemes}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Continuum Incompressible Flow Formulation (CFD)}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Schematic diagram of the overall continuum/atomistic and hybrid computational domains for the chosen test problems (left) including a detailed view of the overlapping zone (right)}}{11}}
\newlabel{Fig:Couette}{{1}{11}}
\citation{LU-SGS}
\citation{Osher}
\citation{MUSCL}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Molecular-Level Formulation (MD)}{12}}
\newlabel{eq:LJ12}{{3}{12}}
\citation{Chimera}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Implementation of hybrid interface on CFD and MD codes. Both codes are equipped with the Þle-based information exchange routine, to update the hybrid boundary condition. CFD code experiences the global change of its data structure to store the information of the entire ßuid system. MD code adopts hybrid equations to impose the macroscopic information on microscopic domain and to ensure numerical stability. }}{13}}
\newlabel{table:interface_implementation}{{1}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Hybrid Interfaces and Schemes}{13}}
\citation{Nie}
\newlabel{eq:External_Force}{{4}{14}}
\newlabel{eq:Mass_Flux}{{5}{14}}
\newlabel{eq:Con_vel}{{6}{14}}
\newlabel{eq:Lagrangian}{{7}{14}}
\citation{Hadjicon3}
\citation{Time_Mechanism}
\citation{Yen}
\newlabel{eq:Con2}{{8}{15}}
\newlabel{eq:Con2}{{9}{15}}
\newlabel{eq:Con3}{{10}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Statistical Error and Coupling Parameters}{15}}
\newlabel{sec:numerical_noise}{{2.4}{15}}
\citation{Hadjicon3}
\citation{Time_Mechanism}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Determining Coupling Conditions}{16}}
\newlabel{sec:numerical_parameters}{{2.4.1}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Sampling Multiple Independent Experiments}{17}}
\newlabel{sec:numerical_lowspeed}{{2.4.2}{17}}
\citation{Wang}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Temporal Coupling Schemes}{18}}
\newlabel{sec:numerical_temporal}{{2.5}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Conventional Time Evolution Mechanisms of a Hybrid CFD-MD Approach; CFD and MD codes are scheduled to conduct data exchange in the overlapping region at every $\Delta {t}$ sampling interval. CFD solution at $n{\Delta }t$ is directly applied as the boundary condition for MD simulation and backward time-averaged molecular dynamic properties over $\Delta {t_{S}}$ sampling durations are imposed as CFD boundary conditions. (1) Synchronized Coupling: Both codes communicate at the same time level and independently iterate to next exchange point. (2) Sequential Coupling: From the same time level, one solver advances to the next communication point and impose implicit boundary condition to its counterpart.}}{19}}
\newlabel{Hybrid_Timescale1}{{2}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip A Prediction-Correction Approach with Extrapolated/Interpolated Hybrid Boundary Conditions; Applying extrapolation/interpolation enables increasing sampling duration as long as sampling interval. (0) Default Formulation; The hybrid BC in MD code is extrapolated from previous solutions. (1) Extrapolated Boundary; CFD code runs the prediction step by $0.5{\Delta }t$. MD code imposes the hybrid BC by the extrapolation from the current value. (2) Interpolated MD Boundary; CFD code runs the prediction step by $1.5{\Delta }t$. MD code imposes interpolated hybrid BC. (3) Interpolated Hybrid Boundary Conditions; Time axis of CFD domain shifts back by 1 sampling interval and the prediction step is set $2.5{\Delta }t$. CFD and MD domains impose interpolated hybrid BC.}}{21}}
\newlabel{Hybrid_Timescale2}{{3}{21}}
\citation{saga_web}
\citation{ogf_web}
\@writefile{toc}{\contentsline {section}{\numberline {3}Coupled Concurrent Multi-Scale (Continuum-Molecular) Simulation Framework}{22}}
\newlabel{sec:computational}{{3}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}SAGA and SAGA-based Frameworks - An Efficient Runtime Environment for Coupled Multi-component Computations}{22}}
\citation{saga_royalsoc}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Layered schematic of the different components of the SAGA landscape. At the topmost level is the simple integrated API which provides the basic functionality for distributed computing. Our BigJob abstraction is built upon this SAGA layer using Python API wrapper}}{23}}
\newlabel{Fig:SAGA1}{{4}{23}}
\citation{Ko}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Architecture of the Controller/Manager and Control Flow: Application manager is responsible for job management including BigJob and sub-job submission, their status monitoring functions. We implement a load-balancing module, and migration service based on job information. Application agent system resides on each HPC resource and conducts job information gathering and also communicates with the application manager via the advert service}}{24}}
\newlabel{Fig:BigJob_Structure}{{5}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Load-Balancing of Coupled Multi-Physics Simulation}{24}}
\newlabel{eq:SimTime_EachTask}{{11}{25}}
\newlabel{eq:PECondition}{{12}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Implementation of an Execution Framework and Application-level Corrections}{26}}
\citation{Nie}
\@writefile{toc}{\contentsline {section}{\numberline {4}Multi-physics Flow Simulations in Various Flow Conditions}{27}}
\newlabel{sec:accuracy}{{4}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Problem Description and Coupling Conditions}{27}}
\newlabel{sec:accuracy_conditions}{{4.1}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Comparison of dependencies encountered for coupled simulations submitted as conventional jobs, to the scenario when using Pilot-Jobs. Here we use only 1 BigJob (S1). The conventional mode of submission experiences three phases based upon their queue state: (i) All jobs are waiting: ($t_1-t_0$); (ii) Inactive mode (where one job is waiting for another: $t_2-t_1$), and (iii) Active mode (running a coupled simulation: $t_f-t_2$). The Inactive stage, disappears when a coupled simulation runs within a BigJob, as an allocated BigJob distributes its resource to both sub-jobs.}}{28}}
\newlabel{Fig:OneBJ_Flow}{{6}{28}}
\citation{Hadjicon3}
\citation{Time_Mechanism}
\newlabel{eq:Noise1}{{14}{29}}
\newlabel{eq:Noise2}{{14}{29}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Statistical Error in Stationary Flow Simulation; Pure MD simulations of 35 $\times $ 52, 70 $\times $ 52 and 140 $\times $ 52 ${\sigma }^2$ are conducted. Liquid is bound in Y-direction by upper and lower walls and able to move in X-direction where periodic boundary condition is imposed. Initial data up to 100 $\tau $ are disregarded to provide enough time for minimization. Velocity of particles around the central layer are accumulated over 512 $\tau $ by using {\it  {fix-ave-spatial}} function in LAMMPS package and post-processed to get the average velocity at different layer size and sampling duration conditions. Statistical noise becomes about half at 4 times bigger domain. The unit is 1/1000 of non-dimensional MD velocity (1$\sigma $ / $\tau $).}}{30}}
\newlabel{table:MD_Vel0}{{2}{30}}
\citation{Yen}
\citation{Nie}
\citation{Yen}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}A Sudden-start Couette Flow}{31}}
\newlabel{sec:accuracy_couette}{{4.2}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Computational Domain of Couette Flow Simulation; The height of the fluid domain is 52$\sigma $ ($\approx $177$\r A$). CFD mesh size is 71$\times $27 and CFD cells at the pure MD region are treated as holes. MD domain size is about 140$\sigma $ in the X-direction and around 26$\sigma $ in the Y-direction, including the bottom wall. Periodic boundary condition is applied on the principal flow direction.}}{32}}
\newlabel{Couette_Val_Domain}{{7}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Unsteady Couette Flow Profile; The evolution of velocity field along the vertical direction is presented. CFD solution is the instantaneous profile at specified time and MD solution is spatially averaged over 2 $\sigma $ in height and temporally averaged for 1 sampling durations (=10$\tau $). (Left) Pure CFD solution is exactly the same as analytic solution. MD solution shows the same flow pattern as analytic solution, though some fluctuation is observed. This verifies that CFD and MD represents the same flow physics. (Right) The steady result by hybrid approach produces the same numerical result as analytic solution, though the slight time-lagging in the hybrid boundary is observed during the evolution.}}{34}}
\newlabel{Flat_Plate_Sol}{{8}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Couette Flow Profile with the Upper Plate Velocity of 0.25 $\sigma /\tau $; The noisy solution when 4 individual simulations are averaged (left) is resolved by sampling 16 independent runs (right). Red lines denote the solution at 20 $\tau $; Green, blue and black lines are solutions at 100, 200 and 1500 $\tau $, respectively.}}{35}}
\newlabel{multiple_couette}{{9}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Variation of Continuum Velocity at the Position of 0.3 Height from the Bottom Wall (Middle of the Overlapping Region); Velocity changes at various sampling runs are compared with the analytic solution. In case 16 simulations are averaged, the noise is about 5 $\%$ compared to the analytic solution.}}{36}}
\newlabel{multiple_couette_temporal}{{10}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}A Physically Unsteady Flow Field: Oscillating Boundary Problem}{36}}
\newlabel{sec:accuracy_oscillation}{{4.3}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip (Left) Couette Flow Profile in Increased System Size; Accurate flow profile can be obtained by increasing the system size 16 times larger when the velocity is reduced to 1/4. Red lines denote the solution at 20 $\tau $; Green, blue and black lines are solutions at 100, 200 and 1500 $\tau $, respectively. (Right) Variation of the Velocity in the Middle of Overlapping Region; Solutions by multiple replica sampling and increasing the system contain the similar strength of the noise in the solution, which is around 5 $\%$ of analytic velocity profile. This implies that multiple replica sampling approach can replace the increase of system size for solving moderate velocity flow.}}{37}}
\newlabel{increase_system}{{11}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Unsteady Flow Profile of Stokes Boundary Layer Problem; (Left) Velocity profiles by pure CFD and hybrid simulations at specified time instances. Difference between hybrid and pure CFD solutions in hybrid region shrinks in the continuum domain. Noise from the MD simulation does not affect the global velocity profile a lot, because the driving force in this simulation is the oscillating velocity from CFD domain. (Right) Temporal variation of the velocity in the middle of hybrid layer. History of the velocity field reveals that the hybrid solution contains much noise in this experiment.}}{39}}
\newlabel{Stokes_Sol}{{12}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Temporal Variation of the Velocity in the Middle of Hybrid Layer; (Left) With conventional model, large overshoot/undershoot phenomena are observed on the peak points: the statistical noise around 0.01 $\sigma /\tau $ is nearly 20 $\%$ of continuum velocity. The time-lagging effect is also captured. (Right) Overshoot/undershoot phenomena in the conventional approach is almost resolved by applying the prediction-correction approach. The interpolated hybrid boundary conditions are imposed on CFD and MD domains.}}{40}}
\newlabel{Temporal1}{{13}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Performance Analysis of a Multi-physics Simulation Framework}{41}}
\newlabel{sec:performance}{{5}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Waiting Time according to the Size and Wall-time Limit of an Individual Job}{41}}
\newlabel{sec:preliminary}{{5.1}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Reduction of Unsteady Error by Prediction-Correction Approach; Figures show the velocity difference between pure CFD and hybrid solutions in the middle of hybrid layer. Plotted data include the conventional extrapolation model and 3 different types of prediction-correction approaches, which are corrected extrapolation model, interpolated boundary model on MD domain and interpolated boundary condition on both domains. The velocity difference in the conventional model ranges from -0.017 to 0.015. This error reduces to be within -0.007 and 0.009 in refined model.}}{42}}
\newlabel{Temporal2}{{14}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Waiting Time for a Coupled Simulation}{43}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Effect of Job Configurations on Waiting Time; The tables show the queue waiting times depending on the number of cores and requested wall-time limits. Measurements are made on Ranger system, which is a TeraGrid resource located at Texas Advanced Computing Center. Analyzing the actual waiting time as a function of the number of cores at different wall-time limits, it can be said that better more often than not, the waiting time decrease as the requested number of cores increases. The relationship between the waiting time and wall-time limit is harder to quantify. However, obtained numbers provide a good case study for showing the variance of actual queue waiting times. 10 independent experiments are sampled and expressed in seconds as "mean$\pm $SD".}}{44}}
\newlabel{table:WaitTime}{{3}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Performance Gain through Load Balancing}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Waiting and inactive time for conventional job submissions, and a single BigJob submission. Average of 10 independent runs are presented in seconds. In the first two cases, conventional job is submitted to use 2$\times $128 cores and a BigJob requests 256 cores: latter two cases use 2$\times $256 and 512 cores, respectively. Conventional job submission mode showed faster time-to-start (i.e., waiting time of the first job + inactive mode) with 24 hr - 256 core request, and a BigJob is allocated faster in all other cases.}}{45}}
\newlabel{table:BJwaiting}{{4}{45}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Results of simulation runtime for S0 (conventional job submissions), S1 (default BigJob) and S1\ensuremath  {_{\textrm  {LB}}} (a BigJob with load-balancing). 256 and 512 cores are used for the coupled simulation. For both cases, S0 and S1 show nearly identical computational cost because the same resource distribution is applied for the coupled simulation. With the load-balancing capability enabled, S1\ensuremath  {_{\textrm  {LB}}} shows about 23.5\% and 6.1\% runtime save compared to S0 when 256 and 512 cores are used. All measured times are in seconds and 5 distinct experiments are averaged.}}{46}}
\newlabel{table:oneBJ_Test}{{5}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Change of Resource Distribution and Computation Time between CFD and MD Sub-jobs using 256 Cores (=16 Nodes); A LB detects the distribution of 3 -- 13 nodes to CFD and MD sub-jobs as the optimal solution. In view of simulation runtime, 1270 seconds for the initial simulation loop reduces to 885 seconds after load-balancing is achieved. Note that each node (which contains 16 cores in the current system) is dedicated to a single application. Triangle and circle symbols denote averaged values from CFD and MD sub-jobs. Dashed lines are solutions from the individual experiment. }}{47}}
\newlabel{Fig:LBSmall}{{15}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Change of Resource Distribution between CFD and MD Sub-jobs and Resultant Computation Time using 512 Cores (=32 Nodes); A LB solution fluctuates between 3 -- 29 and 4 -- 28 nodes assigned to CFD and MD sub-jobs, because of temporary internal overhead of a system. Initial simulation runtime around 815 seconds is reduced to be 725 seconds after the load-balancing is achieved, because of the poor scalability of application codes for this problem set. The same caption is used as in Fig.\nobreakspace  {}15\hbox {}. }}{48}}
\newlabel{Fig:LBLarge}{{16}{48}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Next Step: Further Refinement}{48}}
\newlabel{sec:futureworks}{{6}{48}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{49}}
\newlabel{sec:conclusion}{{7}{49}}
\bibstyle{elsarticle-num}
\bibdata{saga_tg08}
\bibcite{Thompson}{{1}{}{{}}{{}}}
\bibcite{Nie}{{2}{}{{}}{{}}}
\bibcite{Nie_cavity}{{3}{}{{}}{{}}}
\bibcite{Cui}{{4}{}{{}}{{}}}
\bibcite{Wang}{{5}{}{{}}{{}}}
\bibcite{Yen}{{6}{}{{}}{{}}}
\bibcite{Liu}{{7}{}{{}}{{}}}
\bibcite{Hadjicon1}{{8}{}{{}}{{}}}
\bibcite{Hadjicon2}{{9}{}{{}}{{}}}
\bibcite{Hadjicon3}{{10}{}{{}}{{}}}
\bibcite{Werder}{{11}{}{{}}{{}}}
\bibcite{Kotsalis}{{12}{}{{}}{{}}}
\bibcite{Flekkoy}{{13}{}{{}}{{}}}
\bibcite{Wagner}{{14}{}{{}}{{}}}
\bibcite{Delgado1}{{15}{}{{}}{{}}}
\bibcite{USHER}{{16}{}{{}}{{}}}
\bibcite{Time_Mechanism}{{17}{}{{}}{{}}}
\bibcite{Giupponi}{{18}{}{{}}{{}}}
\bibcite{PseudoCompressibility}{{19}{}{{}}{{}}}
\bibcite{LU-SGS}{{20}{}{{}}{{}}}
\bibcite{Osher}{{21}{}{{}}{{}}}
\bibcite{MUSCL}{{22}{}{{}}{{}}}
\bibcite{Chimera}{{23}{}{{}}{{}}}
\bibcite{saga_web}{{24}{}{{}}{{}}}
\bibcite{ogf_web}{{25}{}{{}}{{}}}
\bibcite{saga_royalsoc}{{26}{}{{}}{{}}}
\bibcite{Ko}{{27}{}{{}}{{}}}
\global\NAT@numberstrue
