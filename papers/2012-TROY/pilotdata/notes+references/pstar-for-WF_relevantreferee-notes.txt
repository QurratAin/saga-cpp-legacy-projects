----------------------- REVIEW 1 ---------------------
PAPER: 98
TITLE: Exploring Flexible and Dynamic Enactment of Scientific Workflows using Pilot Abstractions


This is a very interesting but quite theoretical approach, which i
personally would like to see implemented because i have no idea how to
solve the late data binding problem, and then matching the data and
compute resources in such a manner. The theory is appealing but i
would much prefer to see it in action and also to see some performance
numbers. Hence the weak accept.

The paper is about the design of scientific workflows using the
authors recently developed P* model for pilot abstractions.  

While the paper is not supposed to be about the basic abstraction, it
would have helped to do a better job of describing exactly what that
abstraction is.  Saying that it is a way of separating the logical
description of compute and data units from thieir realization as jobs
or files is just not enough.  

The interesting part of the model would be how it can be used to
manage and schedule resources, but again, that is not at all addressed
in the paper, and there are not evern preliminary results.

Finally, there has been a lot of early work on executing data intensive
jobs in the Grid with late bindings to resources that is not referenced.
For example, see "Distributed Processing of Very Large Datasets with
DataCutter", by Beynon et. al, in Parallel Computing, Vol. 27, No. 11,
2001, or "Caravela: A Novel Stream-Based Distributed Computing
Environment", in IEEEE Computer, Vol. 40, No. 5, 2007.


The authors observe that current workflow abstractions in general
lack:

(a) an explicit/adequate approach to handle distributed data on a
workflow and (b) proper separation between logical tasks and data flow
from their mapping into physical location on a distributed computing
infrastructure (DCI).

The authors argue that the management of dynamic data and compute
should become part of the runtime system of workflow engines to enable
applications implemented as workflows to scale as necessary to address
big data challenges and fully exploit DCIs. They explore how the P*
model for pilot-abstractions could serve as a substrate for such a
runtime environment. 

The authors claim that "the main feature of this implementation is
that, by defining the output data decoupled from its physical
location, we allow the system to decide on its most optimal physical
location taking into account dynamic conditions", and that this
feature is lacking from the most popular workflow systems.
These most general assertions are not substantiated by any reference
to the existing literature or software, and indeed other workflow/job
management systems claim to implement such features.  A comparison
with (at least) Condor would have been needed.

* The concept of PC (Pilot Compute) and PD (Pilot Data) is not
  sufficiently explained:

  - a PC "generalizes the concept of using a placeholder job as a
    container for a set of compute tasks".  What differentiates a PC
    from a placeholder job?  What can a PC do that a regular
    placeholder job cannot?

  - "Pilot-Data (PD) abstraction has been introduced to provide a
    placeholder for data as a container for a set of application-level
    logical Data Units (DU), separately from their physical
    allocation."  Again, how is PD different from a set of
    input/output files (which is the only example authors give)?  Can,
    e.g., the P* system model database queries/updates?  If, as it
    seems from what the authors say on lines ...., all the PD does is
    reserve some space on the disk, then how does it compare with
    systems that have been able to do it since years (e.g., SRM)?

* Although it seems that the main advantage of PDs over "plain files"
  is the ability to reserve space in advance for the above, no mention
  of the data size is done in section IV.B, and it's nowhere to be
  found in the code as well.

