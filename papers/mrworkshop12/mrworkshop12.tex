% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\usepackage{xspace}
\usepackage{color}
\newif\ifdraft
\drafttrue

\ifdraft
\newcommand{\terminology}[1]{ {\textcolor{red} {(Terminology used: \textbf{#1}) }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***shantenu: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***andreL: #1 }}}
\newcommand{\pnote}[1]{ {\textcolor{brown} { ***pradeep: #1 }}}
\newcommand{\note}[1]{ {\textcolor{magenta} { ***Note: #1 }}}
\else
\newcommand{\terminology}[1]{}
\newcommand{\alnote}[1]{}
\newcommand{\pnote}[1]{}
\newcommand{\jhanote}[1]{}
\newcommand{\note}[1]{}
\fi



\newcommand{\pilot}{Pilot\xspace}
\newcommand{\pilots}{Pilots\xspace}
\newcommand{\pilotjob}{Pilot-Job\xspace}
\newcommand{\pilotjobs}{Pilot-Jobs\xspace}
\newcommand{\computeunit}{Compute Unit\xspace}
\newcommand{\computeunits}{Compute Units\xspace}
\newcommand{\cu}{CU\xspace}
\newcommand{\cus}{CUs\xspace}
\newcommand{\cs}{Compute Service\xspace}
\newcommand{\css}{Compute Services\xspace}
\newcommand{\pcs}{Pilot Compute Service\xspace}
\newcommand{\dataunit}{Data Unit\xspace}
\newcommand{\dataunits}{Data Unit\xspace}
\newcommand{\du}{DU\xspace}
\newcommand{\dus}{DUs\xspace}
\newcommand{\pilotdata}{Pilot-Data\xspace}
\newcommand{\pd}{PD\xspace}
\newcommand{\pds}{Pilot Data Service\xspace}
\newcommand{\pdss}{Pilot Data Services\xspace}
\newcommand{\su}{SU\xspace}
\newcommand{\sus}{SUs\xspace}
\newcommand{\schedulableunit}{Schedulable Unit\xspace}
\newcommand{\schedulableunits}{Schedulable Units\xspace}
\begin{document}

\title{An extensible Pilot-based SAGA MapReduce}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
\alignauthor Pradeep Kumar Mantha\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}
       \email{pmanth2@cct.lsu.edu}
\alignauthor Andre Luckow\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}
       \email{aluckow@cct.lsu.edu} 
\alignauthor Shantenu Jha\titlenote{Author for correspondence}\\
      \affaddr{Center for Autonomic Computing}\\
     \affaddr{Rutgers University}\\
      \affaddr{94 Brett Road}\\
      \affaddr{Piscataway, NJ}
     \email{shantenu.jha@rutgers.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
In recent years, there has been a large increase in the size of computational
data widely distributed across different geographic locations. Timing and
cost-effective processing of these large distributed datasets on distributed
computational resources requires effective management of data and compute
resources. Pilot-Jobs are successful in up-taking the distributed
infrastructures by compute intensive applications, but still there is a
necessity of an abstraction for data intensive applications, to provide
coupling between data and compute units through a set of relationships.
Pilot-Data provides an abstraction for expressing and managing relationships
between data units and/or compute units. The coupling of Pilot abstractions,
Pilot-Job and Pilot-Data, through SAGA Pilot API implementation, provides
flexibility to specify the relationship between compute and data units and
thus effective management of compute and data units across distributed
infrastructures. MapReduce is an effective programming model for processing
large distributed datasets. However, processing widely distributed data sets
using traditional MapReduce setup configurations limits concurrent usage of
distributed infrastructure and depending on the type of workload aggregation,
overheads like huge data transfers or additional computation to combine
results are involved [weissmanns]. Most MapReduce implementations however,
tied to specific infrastructure. In this paper, we describe the design and
implementation of Pilot API based SAGA MapReduce (PMR), which is 
infrastructure
independent and extensible to multiple clusters. We validate and characterize
PMR by benchmarking it with Hadoop MapReduce using canonical word count
application. We present a PMR workflow to post-process data produced by deep
sequencing machines at widely distributed geographic locations. The workflow
takes the output of the sequencing machines, performs short read mapping using
BWA aligner, and removes duplicate reads. Our experiments show that it
provides a significantly improved throughput as it scaled to multiple
clusters, over traditional distributed MapReduce~\cite{weissman-mr-11}.
\end{abstract}

% A category with the (minimum) three required fields
\category{D.1.2}{Software}{MapReduce}
%A category including the fourth, optional field follows...
\category{J.3}{Computer Applications}{Bioinformatics, Mapping}

% \terms{Design, Experimentation, Performance}

\keywords{Pilot-Jobs, Pilot-Data, Data-Intensive, MapReduce, Genome Sequence
Alignment, BWA, Human Genome, MapReduce, Distributed Computing, Simple API for
Grid Applications (SAGA)}% NOT required for Proceedings

\section{Introduction}

There are various challenges associated with data at extreme scales: which
have become a critical factor in many sciences disciplines, e.\,g.\ in the
areas of fusion energy (ITER), bioinformatics (metagenomics), climate (Earth
System Grid), and astronomy
(LSST)~\cite{Berriman:2011:AAS:2039359.2047483,Jha:2011fk}. The volumes of
data produced by these scientific applications are increasing rapidly driven
by advancing technologies (e.\,g.\ increasing compute capacity and higher
resolution sensors) and decreasing costs for computation, data acquisition and
storage~\cite{hey2009}.

But, processing huge amounts of data is a challenging tasks.
MapReduce~\cite{Dean:2004:MSD:1251254.1251264} as originally developed by
Google aims to address the big data problem by providing an easy-to-use
abstraction for parallel data processing. However, in many cases data is
highly distributed. While existing MapReduce perform very well on single
machines, they show severe limitations in highly distributed scenarios.

So, we need to go distributed to solve problem at the
forefront of science, engineering, medicine \& social networking. There are
numerous scientific applications that either currently utilize, or need to
utilize data and resources distributed over vast heterogeneous infrastructures
and networks with varying speeds and characteristics. The challenges faced by
these applications are interoperability, efficiently managing compute tasks,
and moving data to the scheduled compute location, which is inevitable in case
of programming models like MapReduce.

Pilot-Jobs have been notable in their ability to manage the compute units
across multiple high performance clusters, providing decoupling between
compute units and resource assignment, but there is also a need of an
abstraction to liberate applications/users from challenging requirement of
moving data to the scheduled compute location to execute it successfully.
Pilot-Data provides an abstraction for expressing and managing relationships
between data units and/or compute units. The coupling of abstractions,
Pilot-Jobs and Pilot-Data, provide a complete solution for data intensive
applications to utilize distributed cyber infrastructure effectively. The SAGA
Pilot-API is an implementation of P* model, and provides flexibility to manage
compute, data and relationships between them (affinities)~\cite{pstar-2012}.

MapReduce is an effective programming model for processing large datasets and
can be easily scaled across multiple clusters, involves execution of map
compute tasks, shuffling and moving the intermediate data to the reduce task
for successful execution~\cite{Dean:2004:MSD:1251254.1251264}. Most MapReduce 
implementations however, are tied to
specific infrastructure and are limited to a single cluster. When the source
data and computing platform distributed widely, the most efficient
architecture for processing data over the entire data set becomes non-trivial.
MapReduce setup configurations like Local MapReduce (LMR), Global
MapReduce (GMR) , Distributed MapReduce (DMR) were proposed to process the
distributed data, depending on the workload aggregation
scheme~\cite{weissman-mr-11}. In all these configurations, the huge factor
contributing towards time to solution is, time involved in transferring huge
amounts of data transfer either before the map phase or in shuffle phase.

By carefully analyzing the data movement patterns in MapReduce programming
model, we understood the importance of moving compute to data in the map
phase, and moving only required data to complete the reduce task in the
shuffle phase. These patterns and the limitations of MapReduce programming
model to utilize distributed cyber infrastructure motivated us, to use Pilot
abstractions for managing the map and reduce tasks and intermediate shuffle
data between them.


In section II we talk about background-- Pilot API, Pilot-Job, Pilot-Data,
saga, SAGA previous implementation of MapReduce, Genome sequencing MapReduce
application. In section III, we discuss how Pilot abstractions, Pilot-Job and
Pilot-Data used to implement the architecture of MapReduce.,. In section IV we
discuss about the experiments.

\section{Related Work}

Hadoop

Sphere

Twister/Iterative Map Reduce~\cite{Ekanayake:2010:TRI:1851476.1851593}

Dryad

Twitter Storm

\section{Pilot Compute and Pilot Data}

Pilot-Jobs support effective distributed resource utilization, and are arguably one of the most widely-used distributed computing abstractions. 

\subsection{Pilot Abstractions for Compute and Data}

The abstraction of a {\emph Pilot-Job} generalizes the reoccurring concept of
utilizing a placeholder job as a container for a set of compute tasks;
instances of that placeholder job are commonly referred to as Pilot-Jobs or
pilots. The PJ provides applications (user) level control and management of
the set of allocated resources. Analogous to \pilotjobs, {\emph Pilot-Data}
provides late-binding capabilities for data by separating the allocation of
physical storage and application-level data units~\cite{pstar-2012}.

The Pilot-API exposes the core functionalities of a \pilot framework via a
unified interface providing a common API that can be used across multiple PJ
frameworks. The API provides three core classes: the
\texttt{PilotComputeService} for the management of Pilot-Jobs,
\texttt{PilotDataService} for the management of Pilot-Data and the
\texttt{ComputeDataService} for the management of \texttt{ComputeUnits} (CUs)
and \texttt{DataUnits} (DUs). A CU represents a primary self-containing piece
of work, while a DU represents a logical set for data~\cite{pstar-2012}.

\subsection{BigData: A Pilot-Data Implementation}
\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.47\textwidth]{figures/bigdata.pdf}
	\caption{BigData Architecture and Interactions}
	\label{fig:figures_bigdata}
\end{figure}

BigData (BD) is the SAGA-based prototype of the Pilot-Data abstraction.
BigData is based on BigJob~\cite{bigjob_web} -- a SAGA-based Pilot-Job
implementation. Figure~\ref{fig:figures_bigdata} gives an overview of the
architecture. The system consists similarly to BigJob of two components: the
BD-Manager and the BD-Agents deployed on the physical resources. The
coordination scheme used is again M/W with some intelligence that is located
de-centrally at the BD-Agent. As communication mechanism the SAGA Advert
Service is used, in a similar push/pull mode as for BJ.

The BD-Manager is responsible for (i) meta-data management, i.\,e.\ it
keeps track of the pilot stores that a pilot data object is associated
with, (ii) for scheduling data movements and data replications (taking
into account the application requirements defined via affinities), and
(iii) for managing data movements activities. For this purpose, it can rely
on external service, e.\,g.\ Globus Online for data transfer management.  
Similar to BigJob, an agent on each resource is used to manage the physical 
storage on a resource.  

A particular critical requirement for data-intensive application, is
the management of affinity between CUs and also between DUs and
DUs. The BD scheduler supports preliminary affinity-aware
scheduling: both BigJob and BigData are tightly integrated to
efficiently support compute- and data-related aspects of dynamic
execution.


\section{PilotMapReduce -- A Pilot-based MapReduce Implementation}
\alnote{TODO: Pradeep}


\subsection{Previous SAGA-based MapReduce Implementations}

In~\cite{Sehgal:2011:UAI:1945091.1945329} we investigated the scale-out of an 
MapReduce application on different infrastructures. The utilization of Pilot 
abstractions has several advantages: (i) compute and data pilots allow an 
efficient decoupling of resource allocation and usage, i.\,e.\ the Map Reduce 
master can efficiently schedule compute units containing mapper and reduce 
tasks; (ii) the co-location of data and compute units descriptively defined 
and are automatically handled by Pilot framework; This enables the 
applications to easily trade-off data transfers and available compute 
capacities.

\subsection{Architecture of Pilot-based SAGA MapReduce}

Figure~\ref{fig:figures_mapreduce-pilotdata} shows the architecture of the 
Pilot-based MapReduce framework. PilotMR utilizes Compute Pilots for managing the execution of mapper and reduce tasks, and Data Pilots for managing the flow of data, i.\,e.\ for transferring the input data to the compute tasks and for transferring the intermediate results between mapper and reduce tasks.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.4\textwidth]{figures/mapreduce-pilotdata.pdf}
	\caption{\textbf{Pilot-based MapReduce:} Each pilot (both compute and data 
	pilot) can be associated with an affinity label. The BigData and BigJob 
	Manager will ensure that CUs and DUs are placed with respect to these 
	requirements.}
	\label{fig:figures_mapreduce-pilotdata}
\end{figure}

Flow of an MapReduce application (assumption input data is pre-staged?):
\begin{enumerate}
	\item Start \pilotjobs on all resource.
	\item Execute \texttt{chunk} script via pilot
	\item Execute \texttt{mapper} tasks via pilots
	\item Move output data via Pilot Data
	\item Execute \texttt{reduce} tasks via pilots 
\end{enumerate}

Role of Pilot Data

\subsection{Genome sequencing duplicate read removal}

High-throughput genome sequencing techniques provided by Next Generation
Sequencing (NGS) platforms are changing biological sciences and biomedical
research. In this section we show how PilotMR can be used for a sequence
alignment application scenario.

\subsection{Discussion}

Contrast to Hadoop:
Pilot-MapReduce provides more flexibility and building block, e.g. flexible 
usage of sorting, more fine-grained control of data transfers etc.

\section{Experiments and Results}


\subsection{Pilot MapReduce vs. Non-Pilot MapReduce}
1. Local PMR vs Hadoop MR using canonical word count application.
 its not a performance comparison. because Hadoop uses HDFS which places the tasks close to the data. But, it shows PMR follows the same trend in different configurations, just as hadoop
     - experiment configurations
         - varying workers
         - varying reduces,
         - varying input size (2GB, 4GB, 8GB, 16GB- experiments need to be repeated for 3 times atleast..)
         - varying chunk size.

     - Hadoop configuration tuned to start reduce phase after map phase, since PMR also starts reduce phase after map phase..No combiner is used since in current PMR implementation combiner is not used, number of tasks/node parameter is tuned to set the number of workers. hdfs block size is used as input split size. 
     - infrastructure used - futuregrid india grid cluster.
     - Hadoop tts calculation -involves time to load data into hdfs, map and reduce phase.
     - PMR tts calculation - chunk time, map and reduce phase times. ( local data movement is negligible < 1sec)

\subsection{Distributed MapReduce}

2. distributed PMR vs weissmann's implementation of  DMR using GS duplicate read removal application.

Weissmanns paper doesn't talk about low workload scheme applications where reduce output is still significantly large.. ( it talks about zero, high, balooning ).. so we have a model for these type of applications.

why weissmanns DMR is choosed?? LMR involves huge amount of initial data transfer. GMR need global filesystem across clusters, which is difficult to have, and latencies issues in moving files. So, DMR is a potential choice over LMR and GMR. 

DMR outperforms PMR if pilot-data doesn't transfer intermediate data concurrently. At the same time, if the concurrency  is not tuned properly, it might cause bandwidth problems.

------ So a experiment section needed to fine tune the concurrency between machines.. Again it depends on bandwidth and the machines use. need to perform experiments.

Infrastructure used - India, hotel, sierra futuregrid machines.. hotel is the fastest and sierra is slowest.

DMR implementation - use multiple Local PMR clusters and then combine their result with second round of Local PMR cluster. ( India, sierra, hotel) machines are used.
DMR tts = max ( tts of LMR on india, tts of LMR  on sierrra, tts of LMR  on hotel ) + max (reduce data transfer from india->hotel, reduce data transfer from sierra->hotel) + ( combined LMR on hotel ).. why hotel?? because the runtime on hotel is less.  

PMR tts = max( map  on india, map on  sierrra, map on hotel) + max( exchange of intermediate data between all machines(n) ( n * n-1 concurrent exchanges)) + max ( reduce phase on  india, reduce on  sierrra, reduce on hotel)

--- experiments done for local PMR on each individual cluster.. once time taken to trasnfer intermediate data between machines is obtained.. then tts for PMR can be caluculated and compared to DMR.

\subsection{Pilot MapReduce and Hadoop}


\section{Conclusions and Future Work}


Extending BigData: Access data through BigData abstraction.



%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{pilotjob,saga,saga-related,mrbib}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%

cite p* paper,
cite weissmann
cite moore's law
cite script used to convert fastq to qseq
cite reference paper used to implement duplicate removal application.


\subsection*{Acknowledgments}
grants, funding, jhkim for providing data sets.

\end{document}
