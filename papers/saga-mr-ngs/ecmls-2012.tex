% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009


\documentclass{acm_proc_article-sp}
%\documentclass{sig-alternate} 

\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{float}
\usepackage{times}
\usepackage{color}
\usepackage{multirow}
\usepackage{listings}
\usepackage{times}
\usepackage{paralist}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{ifpdf}
\usepackage{hyperref}
\usepackage[numbers, compress]{natbib}
\usepackage{array}

\begin{document}

\conferenceinfo{ECMLS'12,} {}
\CopyrightYear{2012}
\crdata{978-1-4503-0702-4/11/06}
\clubpenalty=10000
\widowpenalty = 10000

\newif\ifdraft
\drafttrue   
\ifdraft 
\newcommand{\jkimnote}[1]{{\textcolor{green} { ***Joohyun: #1 }}} 
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\newcommand{\pnote}[1]{ {\textcolor{magenta} { ***Pradeep: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***andreL: #1 }}}
\newcommand{\todo}[1]{ {\textcolor{red} { ***TODO: #1 }}}
\newcommand{\fix}[1]{ {\textcolor{red} { ***FIX: #1 }}}
\newcommand{\ny}[1]{ {\textcolor{red} { ***NY: #1 }}}
\newcommand{\reviewer}[1]{} 
\else 
\newcommand{\reviewer}[1]{}
\newcommand{\jkimnote}[1]{} 
\newcommand{\pnote}[1]{}
\newcommand{\alnote}[1]{}
\newcommand{\jhanote}[1]{}
\newcommand{\todo}[1]{}
\fi


\title{Understanding MapReduce-based Next-Generation Sequencing
  Alignment on Distributed Cyberinfrastructure}

%\title{Next-Generation Sequencing Reads Alignment Using Pilot-based SAGA-MapReduce}

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
\alignauthor Pradeep Kumar Mantha\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}
       \email{pmanth2@cct.lsu.edu}
\alignauthor Nayong Kim\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}
       \email{nykim@cct.lsu.edu}
\alignauthor Andre Luckow\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}
       \email{aluckow@cct.lsu.edu}
\and
\alignauthor Joohyun Kim\titlenote{Author for correspondence}\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA} \\
       \email{jhkim@cct.lsu.edu}
\alignauthor Shantenu Jha\titlenote{Author for correspondence}\\
      \affaddr{Center for Autonomic Computing}\\
     \affaddr{Rutgers University}\\
      \affaddr{94 Brett Road}\\
      \affaddr{Piscataway, NJ}
     \email{shantenu.jha@rutgers.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{25 Feb. 2012}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract} 
  Although localization of Next-Generation Sequencing (NGS) data is
  suitable for many analysis and usage scenarios, it is not
  universally desirable, nor possible. However most solutions
  ``impose'' the localization of data as a pre-condition for NGS
  analytics.  We analyze several existing tools and techniques that
  use MapReduce programming model for NGS data analysis to determine
  their effectiveness and extensibility to support distributed data
  scenarios. We find limitations at multiple levels. To overcome these
  limitations, we developed a Pilot-based MapReduce (PMR) -- which is
  a novel implementation of MapReduce using a Pilot task and data
  management implementation.  PMR provides an effective means by which
  a variety of new or existing methods for NGS and downstream analysis
  can be carried out whilst providing efficiency and scalability
  across multiple clusters.  Hierarchical MapReduce can be a viable
  solution proposed for distributed data processing, but developing a
  global reduce is not easy and straight forward in all cases and
  might effect the statistical semantics of final output
  data. Pilot-MapReduce (PMR) circumvents the use of global reduce and
  yet provides an effective, scalable and distributed solution for
  MapReduce programming model.  We compare and contrast the PMR
  approach to similar capabilities of Seqal and Crossbow, two other
  tools which are based on conventional Hadoop-based MapReduce for NGS
  reads alignment and duplicate read removal or SNP finding,
  respectively. We find that PMR is a viable tool to support
  distributed NGS analytics, as well as providing a framework that
  supports parallelism at multiple levels.
\end{abstract}

%   Next-generation DNA sequencing machines are generating enormous
%   amount of sequencing data, placing unprecedented demands on the
%   requirements of computational resources. MapReduce is proven as an
%   effective programming model to exploit the computational resources
%   efficiently for NGS data analysis.  A traditional single MapReduce
%   achieves parallelism at job(map and reduce) level, whilst most of
%   the workflows requires parallelism at MapReduce level i.e executing
%   multiple MapReduce instances concurrently, where a single cluster
%   does not provide enough resources to achieve significant performance
%   improvement. 


% -- fine grained
  % task-level concurrency for a wide range of NGS data analysis and
  % downstream processing.


% We analyze several existing tools and technqiues that use MapReduce
% programming model for Next-Generatiion Sequencing (NGS) data analysis.
% Our approach is based on the Pilot-based SAGA-MapReduce (PMR)
% framework which extends previously introduced SAGA-MapReduce with a
% Pilot task and data management implementation.  PMR provides an
% effective means to develop software tools with which a variety of
% existing or novel methods for NGS data and downstream analysis are
% carried out and more importantly achieve scalability across multiple
% clusters.  As demonstrating examples for the capabilities of our
% PMR-based approach, we implemented similar capabilities of Seqal and
% Crossbow, two other tools which are based on conventional Hadoop-based
% MapReduce for NGS reads alignment and duplicate read removal or SNP
% finding, respectively.  The comparison to these tools allows us to
% characterize our approach with PMR as a viable tool for the
% scale-across requirement as well as a versatile parallelism framework
% for a wide range of NGS data analysis and process tools.  The
% scalability with distributed sequencing data and the potentials for
% other analysis applications are discussed.


\category{D.1.3}{Software}{Concurrent Programming}{ Distributed
  programming/parallel programming} \category{J.3}{Computer
  Applications}{Bioinformatics, Mapping}


% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous} %Acategory including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures,performance measures]

\terms{Design, Experimentation, Performance}

 \keywords{Genome Sequence Alignment, BWA, Human Genome, RNA-Seq Data,
  MapReduce, Distributed Computing, Simple API for Grid
  Applications (SAGA), Pilot Job and Data}

%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings 
%\keywords{RNA conformation energy landscape, Runtime Environment, SAM-I riboswitch,
% S gene of Bovine Corona Viral Genome} % NOT required for Proceedings


% With astronomically explosive volumes of raw data and processed data,
% along with computational tasks that often need scalable
% infrastructure,
% computational implementations exploiting modern parallel processor or
% cluster architectures, and infrastructure developments
% altogether.

\section{INTRODUCTION} 

Recent advances in high-throughput DNA sequencing technologies such as
Next-Generation Sequencing (NGS) platforms have resulted in
unprecedented challenges in the areas of bioinformatics and
computational
biology~\cite{metzker2010,1000genome,wang2009-natrevgen,alex2009,mcpherson2009}.
These challenges are to some extent novel because of the need of the
cross-cutting and integrated solutions leveraging algorithmic
advances, tools and services, and scalable cyberinfrastructure and
middleware.


The use of parallelism -- task-level, thread-level and
data-parallelism, continues to hold promise. In fact, the programming
model of MapReduce tries to exploit task-level and data parallelism
effectively.  Several tools using MapReduce have been already
introduced for NGS data analysis such as read alignment onto a
reference genome~\cite{cloudburst,
  gatk,langmead2009,seal2011,langmead2010, taylor2010}.  MapReduce
implementations on Cloud environments has been widely touted as an
efficient solution for big data
problems\cite{mapreduce-2004-dean,schatz-nature-biotech-2010,
  taylor2010}.

Whereas the combination of MapReduce and Clouds works well for
scenarios where all ``the data can be poured into the Cloud'', often
this is not possible.  It is likely that the proportion/instances
where such stringent models of data localization are not possible will
increase.  Some contributing factors will be: increasing volumes of
data and thus greater challenge in transferring \& localizing it;
increasing number of data-repositories that will need to be accessed
-- each with their own data-transfer and access policies; greater
variety and distribution in the number of producers and consumers of
this data. 

A consequence of the increasing importance of distributed data in NGS
analytics, is that current programming models -- or at least
programming models as currently implemented and executed, will prove
inadequate when faced with the challenge of distributed data; this
will be true of analytical approaches and tools that depend upon these
programming models. We thus posit that there is an emerging downstream
(analysis) problem in the field of NGS. \pnote{we dont have distributed data example, but we can specify
that PMR is an option for distributed data analysis, by showing 1st paper as reference}

The focus of our work is to demonstrate that existing approaches can
be extended to support distributed data scenarios with simple but
effective changes in their execution and runtime environments.

Hierarchical MapReduce can be a viable solution proposed in
\cite{ecmls11-mr-autodock}, but developing a global reduce is not easy
and straight forward in all cases. For example in Crossbow
\cite{langmead2010}, SOAPsnp is used as reduce phase and merging
multiple SOAPsnp output data might effect the statistical semantics of
data. The output of SOAPsnp analysis could be very large as 60 times
than the genome size\cite{soap_snp} and the global reduce is an
additional overhead for these applications as more amount of data need
to processed in the global reduce phase. Pilot-MapReduce
\cite{pmr2012} is a distributed solution and circumvents use of a
global reduce providing an efficient and scalable solution for
MapReduce applications.


Specifically, we demonstrate the use of a Pilot-based SAGA-MapReduce
with which NGS read alignments and subsequent analysis can be
conducted in a distributed, yet scalable fashion. Such Pilot-based
SAGA-MapReduce, herein referred to as PMR, can be thought of as the
capability resulting from writing MapReduce using SAGA to support task
and data distribution, whilst exploiting the capabilities of a
generalized and extensible Pilot-Job (and data) as a runtime
environment\cite{Sehgal2011590,pmr2012,pstar11}.

The implementation of the MapReduce programming model using Pilot
capabilities in turn supports the development of applications which
have significant flexibility in scheduling as well as the ability to
exploit resources that are determined at runtime.  These features
(arising from the Pilot-abstraction) when coupled with
interoperability (arising from SAGA), enable such {\it dynamic
  applications}, to be executed across a range of distributed
resources -- such as clusters, or a mix of HPC grids and Clouds.

Indeed, PMR provides a viable approach for NGS data analytics.  We
shall establish that in addition to the challenges arising from the
required data volume scalability and efficiency, PMR provides support
for a range of software tools as well as the emergence of
revolutionary NGS techniques such as RNA-Seq.  It is instructive to
contrast the flexible and extensible support for such tools to other
approaches using Hadoop-based MapReduce
implementations~\cite{cloudburst,langmead2009,seal2011,langmead2010}.

Indeed, it is still common to find many biologists who have sequencing
data sets and want to pursue biological/biomedical queries in a
timely fashion but are puzzled by the proliferation of tools with
different algorithms and computational requirements including the
installation, maintenance, and update of the software tools.

\jhanote{Say something about tooling, something about integrated the
  concurrency also increased. and extensible environment}

To demonstrate the extensibility of PMR, we analyze and compare two
known MapReduce-based tools, Seqal in the Seal package and
Crossbow~\cite{seal2011,langmead2010}.  The two tools were developed
for carrying out two different analyses; the alignment step was
implemented as the first step with BWA and Bowtie, respectively but
the second analysis is duplicate read removal for Seqal whereas SNP
finding for Crossbow.  The comparison provides a promise for PMR tools
combining the read alignment and successive tasks.  Those features
include, i) scalability across multiple heterogeneous infrastructures
ii) framework supporting extensions iii) effectiveness for
data-intensive problem with distributed data. Points (i) to (iii)
imply that PMR provides a viable compute and data management
environment for high-throughput DNA sequencing data whilst using
distributed cyberinfrastructure effectively.


Before we present the contribution and outline of the paper, we
clarify our usage of the different types of scaling that this paper
will be concerned with: {\it scale-up} -- or the most common type of
scaling behavior, is a reference to the ability (performance) of using
many cores efficiently; {\it scale-out} is a measure of the number of
tasks that can be concurrently executed \& managed; {\it scale-across}
is a measure of the number of distinct compute resources that an
application can utilize.

% \pnote{ Doesn't scale-up and scale-out mean same thing as per above
%   description??  In Fig 3, as the number of nodes or cores increase,
%   the runtime decreases- so its scales-up well.  The concurrency of
%   tasks, increase with number of cores.. Figure -3 again shows that.
%   I mean both are related to the number of workers used.  }

\jhanote{Unfortunately, users cannot directly deploy a MapReduce
  framework such as Hadoop on top of these clusters to form a single
  larger MapReduce cluster. Typically the internal nodes of a cluster
  are not directly reachable from outside. However, MapReduce requires
  the master node to directly communicate with any slave node, which
  is also one of the reasons why MapReduce frameworks are usually
  deployed within a single cluster. Therefore, one challenge is to
  make multiple clusters act collaboratively as one so it can more
  efficiently run MapReduce.''}

%\pnote{Why do we need a distributed mapreduce framework to process
% distributed data on multiple frameworks?  1. To process the data on
%  a single cluster takes more time and getting shared HPC resources to
%  process that huge is also difficult. So distributing the data( Which
%  can be a one time setup) to multiple clusters allows the flexibility
%  to use more computation and process the data quickly.  2. "Although
%  commercial clouds can provide virtually unlimited computation and
%  storage resources on-demand, due to financial, security and possibly
%  other concerns, many researchers still run experiments on a number
%  of small clusters with limited number of nodes that cannot unleash
%  the full power of MapReduce." - {Hierarchical mapreduce} }

The paper is organized as follows. In
Section~\ref{sec:materials_and_methods}, the data sets we used for
this work and experimental methods are presented.
%In Section~\ref{sec:experiments}, we describe the rational and experimental details for this paper. 
After that, in Section~\ref{sec:results}, results for parallel performance
measurements and comparison of our results with Seqal and Crossbow
which represents applications that use the open source Hadoop-based
MapReduce\cite{hadoop-url, taylor2010,seal_2011_mapred,seal2011} are
presented.  Finally, in Section~\ref{sec:discussions}, we 
discuss and conclude this paper with remarks focusing on
implications of our experiments for development of biological
information infrastructure around NGS data for a short term goal of
RNA-Seq data analysis and for a long term goal for an integrative
research infrastructure that provides various holistic services of
downstream analyses streamlined with effective computation of NGS data
and analytics.

\begin{center}
\hfill{}
\begin{figure*}
 \centering
%\includegraphics[scale=0.45]{figures/align-dup.pdf} 
\includegraphics[scale=0.35]{figures/F1_1.pdf} 
\hfill{}
\caption{\small PMR architecture and the workflow for a MapReduce task: The compute and data units are basic blocks of scheduling in Pilot abstractions ~\cite{pstar11}. In our NGS analysis, the compute units in map phase are the alignment tasks, data units in the shuffle phase refer intermediate data units generated and compute units in reduce phase could be either duplicate read removal or SOAPsnp tasks.}
  \label{fig:arch-pj-saga-mr} 
\end{figure*}
\end{center}

\jhanote{Merge: Since the paper about the MapReduce programming model
  from Google in 2004\cite{mapreduce-2004-dean} and the emergence of
  Hadoop, the model has drawn significant attention from various
  application domains as a general strategy for parallelization of
  processing a large data.  In recent years, the computational biology
  community, in particular focusing on data analytics and downstream
  analyses of high-throughput deep sequencing techniques such as NGS
  has found the potential of MapReduce-based approaches for dealing
  with unprecedented data produced from the novel
  platforms\cite{schatz-nature-biotech-2010}.  Not surprisingly, most
  of software tools using MapReduce for NGS data were developed with
  Hadoop environment and consequently with Cloud computing environment
  such as Amazon EC2.}

\section{Materials and Methods}\label{sec:materials_and_methods} 
In this section we explain the materials and methods used for our NGS data analysis used by PMR and other Hadoop based applications.

\subsection{NGS Data Sets}
We used paired-end RNA-Seq data from the Flemington lab, which was originated from a Human Burkitt's lymphoma cell line\cite{erik_2010}. The read length is 100 for both directions of paired-end data, and the total size of each data set is about 20.1 GB containing 70,927,422 reads.  For varying size of sequencing data experiments, we chunked the required amount of reads by partitioning the entire data.  For example, 2 GB paired end data implies 1 GB for one end and 1 GB for the other end.  The file format of sequencing data is fasta and all manipulation of sequencing data was conducted by in-house python scripts and samtools\cite{samtools}.  

\subsection{HPC Resources}
For this work, we used two Futuregrid systems, Sierra and Hotel.  More details on the specification of these machines can be found from the web page of Futuregrid\cite{futuregrid_url}.  In brief, two systems are multi-node clusters and equipped with the PBS job scheduler and Hadoop, implying that the systems are appropriate to test our PMR and to compare our results with Hadoop-based tools. Hadoop version 0.20.2 was used with the two FutureGrid machines. A replication factor of 2 and default chunk size of 128 MB is used. 

\subsection{Pilot-MapReduce(PMR)}
Pilot-MapReduce is a Pilot-based implementation of the MapReduce programming model decoupling the core MapReduce framework from the actual management of the compute, data and network resources. By decoupling job scheduling and monitoring from the resource management, PMR can efficiently reuse the resource management and latebinding capabilities of BigJob and BigData. PMR exposes an easy-to-use interface which provides the complete functionality needed by any MapReduce algorithm, while hiding the more complex functionality, such as chunking of the input, sorting the intermediate results, managing and coordinating the map and reduce tasks, etc., these are generically implemented by the framework \cite{pmr2012}.
Pilot-MapReduce supports different MapReduce topologies (i) \emph{local} (ii) \emph{distributed} (iii) \emph{hierarchical}. Local PMR performs all map and reduce computations on a single cluster.  Distributed and hierarchical PMR are mainly intended to support distributed data processing. Distributed PMR performs map tasks on the multiple resources close to the initial input data and moves the intermediate data files to another resource for executing reduce tasks. The hierarchical PMR the outputs of first phase of MapReduce are combined using final MapReduce step. We showed the trade-off between hierarchical PMR and distributed-PMR in \cite{pmr2012} based on the type of workload or data aggregation involved. In this work, we mainly focus on local and distributed PMR. Fig.~\ref{fig:arch-pj-saga-mr} illustrates the MapReduce flow using PMR.\begin{itemize}
\item{~PMR launches map tasks on N clusters using the Pilot-Job of Pilot framework.}
\item{~After the map stage, PMR shuffles intermediate data between clusters using Pilot-Data of Pilot framework} 
\item{Finally,PMR launches reduce tasks on a cluster using the Pilot-Job of Pilot framework.}
\end{itemize}

\subsection{Target NGS Analysis}
In this section we present the important steps of DNA sequencing considered for our work. 

\subsubsection{Short Read Alignment}
The short reads alignment and the de-novo assembly are the required first steps in every pipeline software tool aiming biological discovery with sequencing data from NGS platforms.  Considering the fact that the de-novo assembly remains still challenging at this moment, because of the complications with the short length of sequencing reads from NGS machines, in practical sense,  read alignment or mapping is considered as the first task of most of the NGS workflows.Generally for RNA-Seq data analysis, in particular with eukaryote genomes, the spliced aligner such as TopHat\cite{pepke2009} is used. In our work, we consider an alternative strategy, to use a non-spliced aligner and later splicing events are detected separately, justifying the use of non-spliced aligners such as BWA and Bowtie for the RNA-Seq data. We consider non-spliced aligner tools, Bowtie and BWA tools to align short reads onto human reference genome hg19.

\subsubsection{Post-Alignment}
Duplicate read removal step might be required  after short read alignment, because sample preparation processes before sequencing might contain artifacts stemming from high-throughtput read amplification; many duplicates introduced are not relevant to true biological conditions. Seqal is a Hadoop MapReduce application which implements the alignment in map phase using BWA aligner and the same criteria as the Picard MarkDuplicates\cite{seal2011,seal_2011_mapred} for duplicate read removal in reduce phase.
On the other hand, information about aligned reads is essential for many downstream analyses such as SNP finding, genome variation study, transcriptome analysis, complex functional genomics and pathway analysis.
Crossbow is Hadoop streaming MapReduce application which performs alignment in the map phase using Bowtie aligner and  performs SNP finding in the reduce phase.\cite{langmead2009}. 

%Interestingly, Crossbow uses Bowtie aligner and thus our PMR-based tool \pnote{ can we change this} is extended to utilize Bowtie, in addition to BWA for the comparison with Crossbow.  Note that Bowtie is well-known for its %efficiency in memory requirement and computation, but the current version do not support gapped alignment whereas BWA does.}

\subsection{Experiments: Motivation and Configuration}

The primary goal of our work is to provide a scalable and extensible
MapReduce runtime framework capable of supporting various steps of
sequencing procedure. Our experiments primarily compare PMR-based approach
with Hadoop API based Seqal (tool for alignment and duplicate removal)
and Hadoop streaming based Crossbow (tool for alignment and SNP
finding) (see Table 1) to:
\begin{itemize}
\item{Validate PMR scale-out, scale-up and scale-across nature on dynamic cyberinfrastructure using various NGS analytic tools}
\item{Validate PMR extensibility to support multiple NGS analytic tools}
\item{Validate PMR support for fine and coarse grained parallellism}.
\end{itemize}

Seqal of Seal package and Crossbow compared to other tools such as Cloudburst and GATK (as summarized in Table ~\ref{table:mr-comparison}) are different from each other in terms of usage and parallel strategies they employ for NGS data processing and management.

Seqal performs short read alignment using BWA aligner in map phase and duplicate read removal in reduce phase.  Crossbow performs short read alignment using Bowtie aligner in map phase and SNP finding in reduce phase using SOAPsnp. For our experiments, Seqal part of Seal package of version 0.2.3 version and Crossbow package of version 1.1.2 is used. Crossbow 1.1.2 uses Bowtie 0.12.7v and SOAPsnp 1.02v tools. Default configuration or execution options are used for both the packages.

Preprocessing is a common step for both Seqal and Crossbow. The input file format for Seqal is prq format.  We use PairReadsQSeq tool of Seal package to convert qseq/fasta format files to prq format. Crossbow also performs preprocessing on input fast files and then performs alignment, snp finding. 

We developed a PMR based NGS applications which are similar to Seqal and Crossbow. PMR based approach for Seqal involves development of a custom map script which performs short read alignment using BWA aligner and reduce script to perform duplicate read removal based on the same criteria provided in \cite{seal_2011_mapred}. But the reduce phase implementation significantly vary from the seqal implementation's. PMR based approach for Crossbow involves development of a custom map script which performs short read alignment using Bowtie aligner. We did not implement the reduce phase of Crossbow, since we are interested in comparing the scalability of PMR at map phase level as shown in  The reduce phase development would be our future work. PMR based applications directly use the fasta format files and no preprocessing required.

In case of executing Hadoop based MapReduce NGS tools on production cyberinfrastructure, we request a set of nodes to start the Hadoop cluster. Once 
required resources are obtained, we set the number of map and reduce tasks per node and number of nodes ($N_{W}/node$), \jhanote{need better notation..} via Hadoop configuration or NGS analytic tool configuration.  Once the Hadoop cluster is ready, NGS tools can be executed. Hadoop MapReduce environment setup involves significant amount of efforts and it is not trivial for a new user. 

PMR provides a simple framework which can be easily deployed on multiple clusters. PMR inherits the advantage of Pilot abstractions which liberates the application/user from the resource management. Some of the parameters involved in PMR setup are specifying the map and reduce scripts and their arguments, number of cores for each map and reduce task, total number of compute nodes for entire MapReduce application and the maximum estimated time(wall time) required to run the MapReduce, nature of the map or reduce task(single or mpi). There is no limitation on the number of cores that can be assigned to a map/reduce task, where as Hadoop enforces number of cores per node as the maximum cores that can be used by a map task. This affects the performance of the MPI based NGS tools which scale-up well. For example NovoalignMPI~\cite{novo-align} is a message passing version of Novoalign, claims to be an accurate aligner till date,  allows a single alignment process to spread across mutliple compute nodes. 

The default input split/chunk size used by Hadoop based applications is 128MB. In the PMR based applications,we make sure the right amount of chunk size is specified so that the number of map tasks are equal in both Hadoop and Pilot MR approaches. The chunk size  for PMR is considered as number of reads. The number of mappers($N_M$) depend on the size of input($S_I$) and chunk size. The number of reduces($N_R$) is set to 8 in our experiments. For the relation between parameters used for the experimental set-up, see examples in Table\ref{table:exp-description}.

The intermediate data management affects the performance of NGS tools in distributed data processing. PMR uses Pilot-Data which transfer the intermediate data in parallel and scale across multiple clusters. Hadoop depends on HDFS and TCP/IP based communication between data nodes. 

%\subsubsection{NGS Data Analytics using PMR}

\pnote{ Merged--The two combined steps of alignment and duplicate read removal, or
alignment and SNP-finding are amenable to task-level parallelism as
provided/used by MapReduce; the read alignment step is conducted as a
map phase, and duplicate read removal or SNP finding as the reduce
phase.  MapReduce-based tool can also be used for other advanced
downstream analyses, such as transcript quantification or peak calling
for other advanced NGS protocols.  The development for such tools will
logically follow this work.}

% The schematic workflow and architecture for running PMR is presented
% in Fig.~\ref{fig:arch-pj-saga-mr}.  

% Also, we consider the simple scenario
% with the Pilot configuration; we only create one PilotJob per each
% resource. For more details on the use of dynamic Pilot-based job
% execution, refer our previous
% publications\cite{dare-ecmls11,dare-tg11,repex_ptrsa}.

\pnote{PMR provide flexibility to configure the required computational resources and NGS analytic tool specific options for the map and reduce tasks. The NGS analytic tool specific options could be the arguments to the tools, for example Bowtie uses -p option to specify the number of threads which can be used to exploit the fine grain parallelism. }
%fine-grained parallelism options supported by the NGS analytic tools
%For the experiments, we need to configure computational tasks required for map and reduce phases which includes the number of workers,
%fine-grained parallelism options of software tools used for each
%phase, and configurations for Pilot Job. Note that we can choose many
%different combinations of these configurations (i.e. associated with
%parallel options with used software tools, and Pilot and MapReduce
%levels) for different strategies that tune the parallelism for a
%target analysis (in which map and reduce phases are decoupled, of
%course) differently by carefully considering fine-grained and
%coarse-grained parallelism levels.  Below, we describe more details on
%the parameters that we vary for a specific experiment.

\begin{center}
\begin{table*}[ht]
{\small
\hfill{}
\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
  &\centering\textbf{PMR}\cite{pmr2012} & \textbf{Seqal}\cite{seal2011} & \textbf{Crossbow}\cite{langmead2009} & \textbf{CloudBurst}\cite{cloudburst} & \textbf{GATK}\cite{gatk} \\ \hline
%\cline{3-9}
 \hline 
 Key Parallelism   & Pilot-based   &  Hadoop-based/  &  Hadoop   & Hadoop-based & MR-based Structured \\ 
Strategy  & Distributed MR & MR  & Streaming  & MR & Programming  \\
& & (Pydoop) &  & & Framework \\ \hline
  
Hadoop & No & Yes & Yes\footnote[1] & Yes & No \\ 
Requirement  & & & &  &\\ \hline  
    
Multiple  Cluster & Yes  & Limited   & Limited  & Limited  & Limited \\
Support &  & by Hadoop &  by Hadoop & by Hadoop  & by JVM   \\ \hline

Multiple Node & Support & Not allowed  & Not allowed  & Not allowed & Not  \\
Task Supprt &  & by Hadoop & by Hadoop & by Hadoop & Easy  \\ \hline
Distributed Job and  & Advert Service  & Hadoop/HDFS & Hadoop/HDFS & Hadoop/HDFS & Java \\ 
Data Coordination &(SAGA) &  & & & Framework\\ \hline


Primary Aligner &  BWA, Bowtie,  &  BWA & Bowtie & RMAP &  BWA \\
& and Others (coming) &  &  &  &  \\ \hline
Multiple Aligner  & Straightforward & Not Straight- & Possible & Not Straight-  & Straight-  \\ 
Support &  & forward &   & forward  & forward \\\hline
Primary Tasks & Alignment/Duplicate  & Alignment/ & Alignment/ & Alignment &Various\\
  &  Removal & Duplicate & SNP Discovery & & NGS Data  \\  
           & (and Extensible &  Removal & &  & \& Downstream  \\
           & for RNA-Seq) & & &  & Analysis \\ \hline  
Extensibility for   &  High  & Medium &  Low & Low & High      \\
Multiple Tools  &      &  &  &  &   \\ \hline

\hline
\end{tabular}}
\hfill{}
\caption{Feature comparison of PMR with other tools for NGS data analysis that primarily provide a parallelism support using the MapReduce framework.  $^{1}${The feature of Crossbow that can run with a desktop environment without Hadoop is not considered in the scope of this work due to the lack of scalability support.} }
 \label{table:mr-comparison}
\end{table*}
\end{center}

%\subsubsection{Comparing  Seqal and Crossbow}
\pnote{Among the software tools compared in Table ~\ref{table:mr-comparison}, we
choose Seqal of the Seal package and Crossbow for the comparison.  Note
that these two tools along with other tools such as Cloudburst and
GATK are different from each other with respect to the usage mode of
MapReduce and the parallel strategies they employ, in particular, for
NGS data processing and management as summarized in
Table~\ref{table:mr-comparison}.  Importantly, Seqal and Crossbow are
Hadoop-based tools but differ in that the former is designed
by implementing map and reduce functions, whereas the
latter uses Hadoop-streaming.  In the case of Crossbow, the overall
analysis is comprised of 4 steps, where each step is an independent
Hadoop-streaming task. The first step is for pre-processing and the
last one is for post-processing. The second one is for the alignment
and the third one is for SNP finding, which uses slightly modified
Bowtie and SOAPsnp.}

\pnote{For the two applications, Seqal and Crossbow, Hadoop version 0.20.2
was used with the two FutureGrid machines.  For Seqal experiment, Seal
version 0.2.3 was used; for Crossbow experiments, version
1.1.2 that contains Bowtie version 0.12.7 and SOAPsnp version 1.02.}

%\jhanote{Please be consistent with upper case: use either upper
%or lower for all tools!}\jkimnote{I think we have to follow what the developer prefers, for example, SOAPsnp or BWA are  more like trade marks for these tools. I do not have any preference for %SEQAL, though}\pnote{addressed}

\pnote{For Seqal and Crossbow experiment, the replication factor for Hadoop
configuration is set to 2.  If not specified, all options for aligners
and external tools as well as Seqal and Crossbow setting are default
as suggested.  For example, Seqal does not support the multi-threading
option of BWA, whereas Crossbow sets up the default option for Bowtie,
"--mm" to support running many concurrent Bowtie processes on a node
while sharing the memory index to avoid memory overhead effectively.}

\pnote{The input file format for Seqal is prq format.  To convert a fastq
format sequence file to this format, we use Seal, a software tool
containing Seqal as a part that has a file conversion capacity between
qseq/fastq and prq format.}

\begin{table}
\small
 \begin{tabular}{|c|c|c|c|c|c|} 
 \hline 

Case & \# of  & \# of &  \# of & \# of & \# of  \\
E($N_{node}$,$N_W$,  & Nodes & Worker   & Mapp- & Reduc- & available  \\
$N_M$, $N_R$,  & &  & ers & ers & cores per\\
$S_I$ (GB)) & ($N_{node})$& ($N_W$) & ($N_M) $ & ($N_R$) & Worker \\
 \hline
E(4, 4, 32, 8, 8) &4 &  4 & 32  & 8 & 8 \\
E(4, 8, 32, 8, 8) & 4 & 8 & 32 & 8 & 4 \\
E (8, 8, 32, 8, 8) & 8 & 8 & 32 & 8 & 8 \\ 
 \hline
 \end{tabular}

 \caption{Description of experimental cases examined in this paper for understanding scalability, impact of parallelism strategy, and performance comparison.  A experiment will be described with the five parameters in E($N_{node}$,$N_W$,$N_M$, $N_R$,$S_I$).  Three examples are illustrated in this table.  Note that all nodes for this study have 8 cores and the number of cores per Worker is determined by the total number of cores divided by the number of Workers in a node.}
    \label{table:exp-description} 
\end{table}

\pnote{In the case of Hadoop-based MapReduce tools, we set the number of
workers per a node ($N_{W}/node$), \jhanote{need better notation..}the
number of mappers ($N_M$) and the number of reducers ($N_R$) via
Hadoop configuration or tool configuration.  These number should be
carefully chosen along with the parameters for the size of resource,
i.e., the number of nodes ($N_{node}$) and the given number of cores
in a node.  For example, the memory size of a node wold limit a
possible choice for $N_{W}/node$ since such multiple workers in a node
might need more memories than the total memory of the node and
consequently fail to run.}  

\pnote{Such parameters dictate the degree of parallelism of a target task
with MapReduce configuration as well as fine-grained parallelism that
might be implemented in tasks for map or reduce phases.  For the input
data partitioning, if not specified differently, the default chunk
size of 128MB was used, which corresponds to nearly 292,763 reads.
For example, the size of 2 GB input is broken into 16 files (8 files
for each end data), and results in 8 map phase tasks.  Note that in
the case of Seqal which uses a different format, the chunking process
generates the same amount of reads per file.  This chunk size is
directly related to the number of Mappers $N_M$ for the alignment task
since $N_M$ is determined as the ratio of the size of input($S_I$)
over this chunk.  For the relation between parameters used for the
experimental set-up, see examples in Table\ref{table:exp-description}.}
%$N_{W_{node}}$
%The configuration , we set the number of nodes as 4, and the maximum map and reduce tasks is set to be 2.  We found that if the number of tasks is set to be more than 2, the issue with memory size occurred.  \pnote{refer to reference paper}  Thus, total 8 workers were used.

\pnote{merged--PMR set-up is as follows: Similar to the Hadoop-based MapReduce tools,
the configuration requires to set $N_{node}$, $N_W$, and the size of
BigJobs that is a task container in our Pilot
framework\cite{pmr2012,saga_bigjob_condor_cloud,pstar11}.  Note that
$N_W$ has no limitation with the size of a node since multiple-node
worker is possible when a tool executed by a work supports multi-node
parallel implementation such as MPI.  Thus, with PMR, we set $N_W$,
instead $N_{W}/node$.  Since BWA and Bowtie have an option for
multi-threading, with PMR, we can provide the configuration for the
option as ON or OFF to understand the impact of such fine-grained
parallelism along with coarse-grained parallelism with the MapReduce
framework.  Note that Seqal and Crossbow turn off this option as
default.  Also, multiple clusters can be used for MapReduce. For
example, half of the workers are assigned to Hotel, and the other half
to Sierra.  For a single system set up, like the two Hadoop-based
MapReduce tools, all workers run on a given system.}

\pnote{Data management is an important component for tools processing
distributed data; the efficiency of data transfer should be considered
when a MapReduce-based tool is assessed.  Whereas Hadoop-based
approaches rely upon HDFS and TCP/IP-based communication between data
nodes, PMR uses SAGA Advert coordination among workers via Pilot
framework (see Fig.~\ref{fig:arch-pj-saga-mr}).  PMR provides the
ability to scale across multiple clusters without the global file
system support.}

 
\pnote{merged--For experiments with PMR using distributed resources, we assume that
all input and required data are stored initially on a single cluster
to process and measure the time for distributing data.}

\section{Evaluation}\label{sec:results}
We evaluate the performance of the developed PMR based applications in the context of scalabilty at different levels and extensibility to multiple tools.
Each experiment is repeated at least three times.




 
%\subsection{PMR for NGS data analysis : Reads Alignment and Duplicate Removal}

\subsection{Scalability}
The scalability characteristics of PMR, at different levels of parallelism and granularity were evaluated using applciations specified in Section ~\ref{sec:materials_and_methods}.
In the first experiment shown in the  Fig ~\ref{fig:read-size}, not surprisingly the total runtime increased linearly as the input read size increased. 
This shows PMR \textit{scale-out}\pnote{n tasks over n cores} well as the number of  compute tasks increase. The PMR runtime involves time taken 
to chunk the input data, map, shuffle and reduce phase times.  The Fig ~\ref{fig:scale-p-saga-mr} shows the evaluation of \textit{scale-up} factor of PMR. 
The PMR scale-up well as the runtime decreases with increase in number of nodes \pnote{we have one unit of task(10GB)..scaling it up across multiple nodes}.
In both the cases the map phase, read alignment is a single dominant step for the overall time-to-solution, and the reduce task performs duplicate read removal 
which is low compute intensive task. The intermediate data is managed by Pilot-Data which parallelizes the intermediate data transfers between 
map and reduce phases in a cluster or between clusters, which improves performance of PMR.
The Fig ~\ref{fig:comp_with_seqal_1} shows the comparision of different configurations of PMR with Seqal application. 
Notably, Seqal takes more time than other configurations of PMR becuase of non-optimal configuration of hadoop. 
The local disks available on FutureGrid are too small for the used input data;  thus, HDFS had to be configured to utilize a shared, distributed file system, 
which leads to a non-optimal performance during the I/O intensive map phase. Hadoop is a most prominent framework for doing MapReduce computations but it is designed for
cluster/local environment, but not for a high degree of distribution. Unfortunately, Hadoop cannot be executed on more than one cluster on Futuregrid, because of firewall issues. 
Hierarchical MapReduce is one of the viable solution to use multiple clusters for  MapReduce programming model and then combine the final output with a global reduce function. Designing global
reduce is not easy and straight forward in all cases. For example in Crossbow application, combining reduce outputs of multiple SoapSNP's using a global reduce might effect the semantics of the final data. The distributed PMR could be a viable solution for running MapReduce programming model on multiple clusters. The distributed PMR is executed on Sierra and Hotel, where the number of workers and the input data is distributed equally between the machines. The distributed PMR performs significantly better than Hadoop Seqal, but it is slower than local PMR since it involves PMR significantly low overhead of distributing tasks between the machines.  But this overhead is significantly less important when \textit{scale-across} factor is considered which allows concurrent usage of multiple cluster resources. Note that the direct comparison of the reduce phase is not straightforward due to the potentially different implementation between Seqal and ours in spite of the fact that we used the same criteria of the duplicate read removal of Seqal\cite{seal_2011_mapred}. The map phase of Seqal and our PMR tool are almost comparable since the same aligner, BWA is used and all parameters (see Table~\ref{table:exp-description}) for configuring a MapReduce task are same in the comparison.   


%The computation of read alignment and duplicate read removal represents an
%interesting and simple category of computational tasks among NGS data
%analysis that could be benefitted by employing the MapReduce
%programming model.  We implement such a MapReduce task with PMR.  The
%main findings with this analysis are as follows.  First, as shown in
%Fig.~\ref{fig:read-size}, the scalability is critically required as
%the input data size increases.  Not surprisingly, the results shown in
%Fig~\ref{fig:scale-p-saga-mr} suggests that the parallel strategy with
%MapReduce could be a viable solution for increasingly demanding large
%data sets.

 \begin{figure}
 \centering
\includegraphics[scale=0.50]{figures/pj-smr-tts.pdf} 
\caption{\small Need of a scalable solution with increment of data volume.  Read size dependent time-to-solution is measured for MapReduce tasks of read alignment and duplicate read removal.  The number of nodes for this experiment,$N_{node}$ is 16, the number of Workers, $N_W$ is 32, and the chunk size is set to contain 625000 reads.  The number of Reducers is set to 8. BWA is used for read alignment.}
  \label{fig:read-size} 
\end{figure}


%The good scalability with PMR shown in Fig~\ref{fig:scale-p-saga-mr}, as a matter of fact, appears to be helped by the characteristic nature of the target computational tasks.  The map phase, the read alignment is a single dominant step for the overall time-to-solution, and the %read alignment task is the most prominent example among a variety of NGS data analysis tasks that is suitable for data parallelization as well as task parallelization.  Note that the reduce phase task, duplicate read removal is also easy for parallelization primarily due to the fact %that to remove the duplicates needs only the local information, i.e. a single key or a small number of keys from the map phase results.  Despite of the simple nature, read alignment and duplicate read removal does provide an understanding of challenges for MapReduce-based %approaches for NGS data analytics.  For example, unlike the well-know word count problem, the map phase produces high volume of data size, indicating the low-aggregation problem, compared to the high-aggregation problem of the word counting\cite{weissman-mr-11}.  I%ndeed, the data management along with this feature should be considered carefully as indicated in Fig.~\ref{fig:read-size} with the 40 GB case that shows a significant portion of the processing time for chunking data.  Note that 40 GB is the entire data set from the RNA-Seq %experiment of interest.  Nonetheless, we note that the data management between the map phase and the reduce phase is reasonably handled by the Pilot framework for a single cluster or even a case for utilizing a couple of clusters as presented later.


\begin{figure}
 \centering
\includegraphics[scale=0.50]{figures/pj-smr-scale.pdf}
\caption{\small PMR scalability of a MapReduce-based computation for read alignment and duplicate read removal.  The number of Workers per node, $N_{W}/node$ is set to 2.   The input read file is 10GB, the number of Reducers is set to 8, The number of reads in a chunk is 625000. BWA is used for read alignment}
\alnote{I thought we don't include PMR only data}
  \label{fig:scale-p-saga-mr} 
\end{figure}

\subsection{Extensibility}
The extensibility of PMR is particularly demonstrated with two aligners, BWA and Bowtie.  One of the important reasons why multiple aligners are needed is because of the difficulty of validation of an aligner used.  It is well studied that each aligner implements different strategies to deal with the requirement of computational loads, memory usage, and sensitivity associated with decision on algorithms and computational implementations of indexing, search, and match tasks\cite{mapping-survey}. Indeed, the decision of an aligner affects not only alignment results but also scientific discovery with downstream analysis aiming genome variation study, transcriptome analysis, and DAN-protein interactions. Therefore, it is not overstated to emphasize the importance of supporting multiple tools as well as providing an effective means for implementing such tools within a reasonably short development period for infrastructure of NGS data. 
Fig.~\ref{fig:tool_comp}, evaluates the performance of read alignment in the map phase of both Hadoop and PMR based applications for Bowtie and BWA aligners. Hadoop implementations - Crossbow uses Bowtie aligner and Seqal uses BWA aligner.  Custom python wrappers to Bowtie and BWA aligner are developed to execute alignment in the map phase of PMR. In the evaluation, both Hadoop based implementations face the problem of non-optimal configuration of Hadoop, i.e usage of shared file system for HDFS, where as both local and distributed PMR performs better than Hadoop map phase for both aligners. The PMR is extensible and can support multiple NGS analytic tools. 

Extending PMR to support new NGS analytic tools involve development of simple map and reduce wrapper scripts to the tools. The wrapper scripts could be developed in any language. 
The Hadoop streaming supports this types of functionality but still involves complexity of managing computational resources to maintain hadoop cluster.
The PMR liberates the user from the complex task of maintaining and acquiring computational resources and executing map and reduce tasks on them.

\subsection{Granularity} \pnote{please correct if i am not correct}
PMR manages multiple levels of parallelisms and optimize them for the efficient execution to lower time-to-solutions. BWA and Bowtie have a fine-grained parallelism option with the multi-threading support option.   In Fig. \ref{fig:mulit-parallel}  \pnote{ figure will be updated with more data}, we showed how such an option could affect on the performance.  
Even though it is feasible for other tools such as Seqal or Crossbow to handle such options, our approach to combine the Pilot and the MapReduce framework provides capability of utilizing the fine-grained parallelism along with the coarse grain parallelism provided by MapReduce. The fine grain parallelism provided by Pilot-Job framework is demonstrated in replica exchange implementation \cite{repex_ptrsa}.  

One of the advantages of PMR is it doesn't impose any restriction on number of compute nodes that can be assigned to a particular map or reduce task. This supports NGS MPI tools to exploit the  computational resources fully using fine grain parallelism. For example NovoalignMPI~\cite{novo-align} is a message passing version of Novoalign, claims to be an accurate aligner till date,  allows a single alignment process to use multiple compute nodes. The MPI versions of Novoalign are more beneficial when large computing infrastructures are available. Hadoop doesn't provide flexibility to assign multiple compute nodes to a single compute task, which allineates Hadoop MR usage for MPI based NGS analytic tools.

\subsection{Comparison to Hadoop-based Seqal and Crossbow}
\subsubsection{Closely Related Works}
Compared to other areas, the computational biology community, specifically focusing on NGS data analysis, joined later for the use of MapReduce\cite{cloudburst}.  Nonetheless, there have been active works resulting in the programs listed in Table~\ref{table:mr-comparison}.  In brief, Cloudburst was one of the first generation tools in this field and  demonstrated the significant potential of the MapReduce model for NGS data analysis.  After Cloudburst, Crossbow was developed by focusing on better scalability using Hadoop streaming.  Compared to the two Hadoop-based tools, GATK was introduced to address the general parallel execution pattern across many NGS data analytics.  Recently, the Seal package was announced in which Seqal is a utility for the alignment and duplicate read removal.   

\pnote{Merged---In spite of the commonality that MapReduce is the core programming model for the tools, they differ in several aspects depending upon the need of Hadoop, the way to utilize Hadoop, the level of flexibility for the extension of tools for alternative tasks or the multiple tool support for different aligners, for example, and whether the multiple cluster support is possible.
Interestingly, the cross-domain support over multiple clusters with the MapRedue framework, specifically via so called hierarchical MapReduce, was recently suggested for the AutodDock tool.  We note that this docking application belongs, in contrast to our applications that are mostly loosely-coupled, to pleasingly parallel and thus faces a different bur simpler type of challenges, compared to ours\cite{ecmls11-mr-autodock}. To demonstrate the distinctive strategies and features of PMR-based tools, we compare the performance with Seqal and Crossbow in the followings. }

\begin{figure}
 \centering
\includegraphics[scale=0.50]{figures/seqalvslocalpmr.pdf}
\includegraphics[scale=0.52]{figures/8GB_phasewisetimes.pdf}

\caption{\small Comparison of the total time-to-solution for a MapReduce-based computation of alignment and duplicate read removal (upper).  Dissected elapsed times for each step of the results with 8 GB (lower). Hadoop-based Seqal is compared to Local-PMR vs. distributed-PMR.  BWA is the aligner for the map phase.  The size of input is 8 GB.  For this experiment, the number of Nodes, $N_{node}$ is 4, the number of Workers, $N_W$ is 8, the number of Reducers, $N_R$ is 8, and the number of reads in each chunk is 292,763. For the distributed-PMR, two machines of FutureGrid, Sierra and Hotel were used, whereas Sierra was used for other cases.}
\alnote{same analysis in in MR paper}
  \label{fig:comp_with_seqal_1} 
\end{figure}

%\subsubsection{Performance Comparison}
%In Fig.~\ref{fig:comp_with_seqal_1}, we compared directly the time-to-solution between Seqal and two scenarios with PMR, one with a single cluster and the other with two clusters.  In %Fig.~\ref{fig:comp_with_seqal_2}, also how each step contributes for the total runtime is presented with the case of 8 GB input.  

\pnote{Merged---Notably, Seqal takes more time on the overall run time, and the most
contributing cause for benchmark difference comes from the map phase.
We contribute this result with the situation of the Hadoop
configuration with the FutureGrid systems.  The tmp directory accessed
by workers of each node is configured by Hadoop, but the size of a tmp
directory is limited with the fast and local attached disk space and
thus a remote file system is configured as the tmp data directory.
This situation is likely to enforce most of users, a non-expert on
Hadoop system, and results in the performance loss with File I/O.  The
similar observation was also indicated with the comparison of
Crossbow.}

\pnote{Merged----Another important aspect which can be inferred from results shown in
Fig.~\ref{fig:comp_with_seqal_1} is that whereas Seqal is limited by a
single cluster with Hadoop, PMR can be easily scale-across over
multiple clusters. The distributed-PMR, which uses the two cluster
systems, Hotel and Sierra, is indeed performing well against the
local-PMR in spite of the low network connectivity between two
clusters compared to a intra-cluster network connection.  The overhead
arising from the connection between two clusters is indicated in
Fig.~\ref{fig:comp_with_seqal_1}(lower) by showing the longer time for
shuffling in the case of "distributed-PMR".}

\pnote{Merged---Note that the direct comparison of the reduce phase is not straightforward due to the potentially different implementation between Seqal and ours in spite of the fact that we used the same criteria of the duplicate read removal of Seqal\cite{seal_2011_mapred}.  In any event, unlike the reduce phase, the map phase of Seqal and our PMR tool are almost comparable since the same aligner, BWA is used and all parameters (see Table~\ref{table:exp-description}) for configuring a MapReduce task are same in the comparison. }



\begin{figure} 
 \centering
\includegraphics[scale=0.40]{figures/map_comp.pdf}
\caption{\small  Comparison of runtimes for the Map phase. The map phase of Seqal, Local-PMR(BWA), distributed-PMR(BWA), Local-PMR(Bowtie), distributed-PMR(Bowtie), and Crossbow(Bowtie) are compared.  The aligner used for each case is indicated in a parenthesis.  For this experiment, the number of nodes, $N_{node}$ is 4, the number of Workers, $N_W$ is 8, and the number of reads in each chunk is 292,763.  For the distributed-PMR, two machines of FutureGrid, Sierra and Hotel were used, whereas Sierra was used for other cases.}
  \label{fig:tool_comp} 
\end{figure}

%number of nodes=4, number of workers/node=2,reduces=8, number of reads/chunk=292763; ********DMR provides scale-across********** as two machines Sierra and Hotel are used.

\pnote{merged--Another Hadoop-based tool, Crossbow, uses Bowtie as a aligner and
utilizes Hadoop-Streaming\cite{hadoop-url}.  We summarize the result
of measuring the times of alignment step calculations and present in
Fig.~\ref{fig:tool_comp}.  The tendency shown with the comparison of
Seqal to PMR is also observed with the comparison with Crossbow; the
longer runtime with the Hadoop-Streaming-based tool regardless of the
use of a different aligner, Bowtie.  It is well-known that Bowtie is
extremely efficient in computation of alignment and the results found,
correspondingly, less time-to-solution with the aligner.}


%As the number of input sequences increased the time to solution also increased linearly for both Seqal and local-PMR. 
%The time to solution decreased by an average of 43.39\% with 95\% confidence interval of 3.03\%. The map phase of Local-PMR is on an average of 64\% of map phase of Seqal application, and is reduced by an average of 35.7\%
%The reduce phase is very less compared to seqal applicaiton because there is not sort involved in reduce phase.
%\pnote{( why is that?? i dont see any reason to have a sort before reduce starts)) we do sort intermediate data before shuffled.}
%The reduce phase is average of 8.85\% of reduce phase of seqal  ( it involves sort )
%
%\pnote{how to explain performance of PMR ??}..One of the performance bottlneck for PMR is coordination system used... we used redis as coordination system... which is proved best ,when compared to other coordination systems{Pstar reference}. I think we can compare the results.. but cant say why Seqal performed better.. since, it needs understanding of how Seqal actually works.

\pnote{merged---\subsubsection{Scale-across, Extensibility, and Parallelism Support}
The scale-across over multiple clusters and the ease of extensibility are two important advantages with the PMR as indicated with the benchmarking results.  The ease of extensibility is particularly demonstrated with two aligners, BWA and Bowtie support.  One of the important reasons why multiple aligners are needed is because of the difficulty of validation of an aligner used.  It is well studied that each aligner implements different strategies to deal with the requirement of computational loads, memory usage, and sensitivity associated with decision on algorithms and computational implementations of indexing, search, and match tasks\cite{mapping-survey}   Indeed, the decision of an aligner affects not only alignment results but also scientific discovery with downstream analysis aiming genome variation study, transcriptome analysis, and DAN-protein interactions. Therefore, it is not overstated to emphasize the importance of supporting multiple tools as well as providing an effective means for implementing such tools within a reasonably short development period for infrastructure of NGS data. }


\begin{figure} 
 \centering
\includegraphics[scale=0.50]{figures/8GB_4nodes_8w_threadcomp.pdf}
\caption{\small  The performance role of multi-threading support option is indicated.  $N_W$=8, $N_{node}$=4, and input data size is 8 GB.}
  \label{fig:mulit-parallel} 
\end{figure}

\pnote{merged--Last but not the least, PMR has an advantage to manage multiple levels of parallelisms and optimize them for the efficient execution to lower time-to-solutions.  BWA and Bowtie have a fine-grained parallelism option with the multi-threading support option.   In Fig. \ref{fig:mulit-parallel}, we showed how such an option could affect on the performance.  
Even though it is feasible for other tools such as Seqal or Crossbow to handle such options, our approach to combine the Pilot framework and the MapReduce framework is easily capable of utilizing the fine-grained parallelism along with the parallelism provided by MapReduce.  One important discussion in this line of direction is that our PMR is also able to optimize the multiple levels of parallelism when a standalone executable run by MapReduce is an MPI application.  Previously, we reported the usefulness of our Pilot-Job framework for such a situation\cite{repex_ptrsa}.  Note that it is not straightforward or impossible for Hadoop-based approaches to deal with multi-node applications.}


\section{Discussion}\label{sec:discussions}

% \textit{Computational Challenges for NGS data analysis, processing,
%   and downstream analysis}

DNA sequencing data generated by NGS
platforms\cite{metzker2010,1000genome,wang2009-natrevgen,alex2009,mcpherson2009}.
has resulted in a fundamental change in the amount of data that needs
to be analyzed. Such high-throughput technologies are able to provide
information about an entire genome or statistically meaningful number
of genomes of same species or related speices.

% In the last several years, every area of biology and biomedical
% fields has witnessed a paradigm change with DNA sequencing data
% generated by NGS
% platforms\cite{metzker2010,1000genome,wang2009-natrevgen,alex2009,mcpherson2009}.
% Most importantly, these high-throughput technologies were able to
% provide the thorough information about an entire genome or
% statistically meaningful number of genomes of same species or
% related ones with enormously affordable cost, resulting in an influx
% of unprecedented amount of data.

While there have been algorithmic advances and a plethora of new
software tools t by user-friendly interface via web-based tools, GUI
tools, or computing environments \cite{galaxy}, the end-to-end
development of scalable infrastructure is lagging.  MapReduce is a
widely employed programming model for parallelization of a large data
process; as reported by others with the tools listed in
Table~\ref{table:mr-comparison}, we observed that an efficient 
runtime environment for MapReduce such as that provided by PMR 
enables efficient methods for dealing with the required
scalability of NGS data analysis which comprises short reads alignment
and other following analysis such as duplicate read removal and SNP
finding.

%First, as shown in  Fig.~\ref{fig:read-size} and Fig.~\ref{fig:scale-p-saga-mr}. the map phase, read alignment, takes significantly more time than the task with the %reduce phase, duplicate read removal or other steps such as chunking input dta.  Secondly, the reduce phase task, like the map phase task, i.e. read alignment, additive.  %Here, the term additive means $f(X + Y) = f(X) + f(Y)$ where $X$ and $Y$ are input variables and $f$ represents the map or the reduce function.  On the contrary, SNP f%inding or other advanced NGS analysis are not in this type of analysis.   

% \textit{PMR, a viable solution for scale-across and extensible
%   framework for NGS data analytics}

\textit{PMR -- A viable solution for scale-across and extensible NGS
  analytics:} In fact, not only is PMR a viable solution for
scale-across and extensible framework for NGS data analytics, it
provides some unique features, viz., support for distributed data
analysis and multiple tools that can each exploit multiple levels of
parallelism.  In other words, PMR provides a runtime environment with
which minimally modified, yet standalone target tools are executed in
multiple network-connected infrastructures and the overall workflow
can be dynamically optimized by exploiting multiple levels of
parallelism. Furthermore, as indicated by results for BWA and Bowtie
for alignment, PMR allows further extensions of existing
implementation with other complementary tools or a flexible pipeline
development.  In fact, our primary goal behind this work is to develop
an integrative pipeline service for RNA-Seq data, and our development
presented in this work is indicative of preliminary progresses toward
such a goal.

\textit{PMR vs. Hadoop-based MapReduce tools: }The open source Hadoop
provides an ideal environment for the tools based on the MapReduce
programming model.  Perhaps, the ease of installation with commodity
hardware and the robust stability with fault-resilency and easy
scalability could propel a wide acceptance of Hadoop.  Nonetheless,
Hadoop-based approaches find many hurdles in distributed contexts and
scenarios: for example, when scaling across multiple
clusters\cite{weissman-mr-11}, the execution of applications across
multi-nodes such as one using MPI, and the solutions involving the
limitations imposed by a current Hadoop implementation need to be
addressed.

For example, due to limited scalability with Hadoop, discussions and
suggestions for the next-generation Hadoop are being circulated in the
community\cite{ng-hadoop-url}.\jhanote{What suggestions are these? Why
  are they relevant here?}\jkimnote{this means that Hadoop is matured but also aging...}  But as evidenced with the comparison of
our PMR-based read alignment and a following analysis with two
Hadoop-based tools -- Crossbow and SEAQL, PMR-based implementations
demonstrate scalability advantages over Hadoop-based approaches.

%It was reported that the overall performance of Hadoop-based MapReduce is determined by the unknown network performance and the connectivity between the data storage and the compute which are determined by characteristics of the cluster Hadoop is installed.  The tight coupling between the MapReduce applications with underlying Hadoop causes unnecessary complexity when the optimization or the design of parallel execution is required.  Secondly, it is not trivial to scale across different clusters.  Many issues on the connectivity between two separate clusters including firewalls, different security policies, and potentially different administration structure are making difficult to extend Hadoop with other cluster systems.  Thirdly, it is a still ongoing concern that Hadoop's current implementation has obstacles from the scalability issue with the design with Namenode and Job tracker.   The open source Hadoop is implemented with a job and task tracker: the job tracker is the central manager that dispatches map and reduce tasks to the nodes(nodes could be from multiple clusters) of the Hadoop cluster. On each node the task tracker is responsible for executing the respective tasks. The main limitation of this architecture is the fact that it intermixes both cluster resource management and application-level task managements. Thus, it is e.g. not easily possible to integrate Hadoop with another resource management tool, e.g. PBS or Torque. Also, the job tracker represents a single point of failure and scalability bottleneck. Another limitation is the latencies between machines should not be so big as HDFS uses Avro - an RPC-style protocol for communications.
%
%Collectively, PMR has edges in terms of flexibility and non-Hadoop-based architecture


\textit{DARE and beyond: }PMR-based NGS tools, implemented and
scrutinized in this work, were developed in conjunction with our
development of the runtime environment for dynamic applications,
Distributed Application Runtime Environment
(DARE)~\cite{dare-tg11,dare-ecmls11}.  DARE is a strategically
important component for the development of Science Gateway for NGS
data analytics and downstream analysis.  Under the design strategy of
DARE-based gateways, PMR-based tools were conceived to be a main
category of supporting execution patterns for parallel and distributed
task and data management.


\section*{Acknowledgement}

The project described was partially supported by Grant Numbers HPCOPS
NSF-OCI 0710874 award, NSF-ExTENCI (OCI-1007115) P20RR016456 from the
NIH National Center For Research Resources.  Important funding for
SAGA has been provided by the UK EPSRC grant number GR/D0766171/1 (via
OMII-UK) and the Cybertools project (PI Jha) NSF/LEQSF
(2007-10)-CyberRII-01.  Computing resources were made possible via NSF
TRAC award TG-MCB090174 and LONI resources.  This document was
developed with support from the National Science Foundation (NSF)
under Grant No.  0910812 to Indiana University for ``FutureGrid: An
Experimental, High-Performance Grid Test-bed.''  We also acknowledge
the Seal developer, Luca Pireddu for useful performance related
discussions, and Erik Flemington for allowing us to use his RNA-seq
data sets.

\bibliographystyle{unsrt}
\bibliography{compbio,saga}


\end{document}

Any opinions, ndings, and conclusions or recommendations expressed in
this material are those of the author(s) and do not necessarily
reflect the views.
