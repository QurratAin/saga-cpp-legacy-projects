 - Motivation for rewriting Montage using SAGA.
   -- Montage, a benchmark application
   -- Limitations of Montage when used with Pegasus and DAGMAN
      --- static scheduling, i.e., need source and desitination. Pegasus does	
      upfront placement.
      --- dependence on DAGMAN(!)
      --- rigid execution model. We want to just define the rules and not the DAG!
   -- Currently not programmed distributedly, but executed as an legacy application
      in distributed mode.
         
 - Three levels at which SAGA can be integrated with Montage:
   (i) Replace Pegasus and DAGMAN, i.e., write/generate a DAG or the rules using SAGA?
       determine what goes where, how, and programmatically control relative placement of
       data w.r.t compute
   (ii) Use SAGMan in lieu of DAGMAN, that is, now that we have a DAG,
        let us execute the DAG using SAGA
   (iii) Use SAGA in lieu of Condor. i.e. SAGA becomes the interface to Condor and other
   systems as opposed to Condor becoming the interface to other systems via Condor-G.
   Advantage of this is that now data/file transfer and other functionality is all
   integrated with job submission.


The general formulation and the degree-of-freedom at each level is as follows:
 
Application: which is comprised of multiple heterogenous work-load components. There is flexibility in
             defining the work-load decomposition.
==========
Scheduling: Different scheduling approaches exist. e.g Gang scheduling vs bulk aggregation

==========
Resources:  Clouds versus Grids. Cloud-typeA vs Cloud-typeB.

MapReduce although a multi-stage application is essentially a
"homogenous" workload, i.e., each worker does the same thing, albeit
on different data sets. A multi-physics applications (a la Chris Hill)
is the other extreme. (SAGA)Montage is in between. Hence the choice.

Other narratives: Does it make sense to saga-fy a parallel application.
How do you create a distributed version of a parallel application?
i.e. use MPI to communicate / coordinate across distributed work loads

1.      MPI    MPI    MPI
     ====== ====== ======
  CN(WN) C3(W3) C2(W2) C1(W1)   where C is a cluster, assigned Workload W

2.
    
 T1(W1)  T2(W2)   T3(W3)
       <-->    <-->
      ===============
            SAGA

In example 2, each T (== task) may still have a parallelised workload,
but now the application is responsible for i) placing and ii)
coordinating the individual tasks.  Contrast this with example 1,
where the static mapping of workload to resources is
performed. Admittedly 2 is more complicated than 1, but 2 is also more
powerful.  Current MPI version of Montage use 1; SAGAMontage, will be
type 2.

