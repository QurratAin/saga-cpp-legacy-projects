\documentclass{sig-alternate}
\usepackage[numbers, sort, compress]{natbib}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{hyperref}
\usepackage{pdfsync}
\usepackage{mdwlist}


\begin{document}

\conferenceinfo{XSEDE '12, July 16-20, 2012, Chicago, USA.} {}
\CopyrightYear{2012}
\crdata{}
%\clubpenalty=10000
%\widowpenalty = 10000

% \title{BigJob: Lessons of Supporting High Throughput High Performance
%   Ensembles on XSEDE}

%\title{The Anatomy of a successful ECSS Project: Leveraging Expertise
%  of Users, Software-providers and XSEDE Personnel}

\title{The Anatomy of a Successful ECSS Project: Lessons of Supporting 
       High Throughput High Performance Ensembles on XSEDE}

\numberofauthors{3}
\author{
\alignauthor Melissa Romanus\\
       \affaddr{The Cloud and Autonomic Computing Center}\\
	\affaddr{Rutgers, The State University of New Jersey} \\
       \affaddr{506 CoRE} \\
	\affaddr{Piscataway, NJ} \\
       \email{melissa.romanus@rutgers.edu}
\alignauthor Pradeep Mantha\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}\\
       \email{pmanth2@cct.lsu.edu}
\alignauthor Yaakoub El Khamra\\
       \affaddr{A Shady Computing Center}\\
       \affaddr{Austin, TX} \\
       \email{}
\and
\alignauthor Andre Merzky\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}\\
       \email{amerzky@...}
\alignauthor Shantenu Jha\\
       \affaddr{Her Majesty's Secret Service}\\
       \affaddr{No Comment} \\
       \email{Can't tell you that love}
}

\maketitle


\begin{abstract}
The Extended Collaborative Support Service (ECSS) of XSEDE is a means
of providing support for advance user requirements that cannot and
should not be supported via a regular ticketing system. Recently two
ECSS projects were awarded by XSEDE management to support the
high-throughput of high-performance (HTHP) molecular dynamics (MD)
simulations; both of these ECSS projects are using SAGA-based
Pilot-Jobs approach as the technology required to support the HTHP
scenarios.  More significantly, these projects were envisioned as
three-way collaborations: between the application stake-holders,
advanced/research software development team and the resource
providers. In this paper, we describe the aims and objective of these
ECSS projects, how the deliverables have been met, and some
preliminary results obtained. We also describe how SAGA has been
deployed on XSEDE as a precursor to the seamless uptake of BigJobs.
\end{abstract}


\newif\ifdraft \drafttrue \ifdraft
\newcommand{\mrnote}[1]{{\textcolor{green} { ***MR: #1 }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\newcommand{\yyenote}[1]{ {\textcolor{cyan} { ***YYE: #1 }}}
\newcommand{\pmnote}[1]{ {\textcolor{blue} { ***PM: #1 }}}
\newcommand{\todo}[1]{ {\textcolor{red} { ***TODO: #1 }}}
\newcommand{\fix}[1]{ {\textcolor{red} { ***FIX: #1 }}}
\newcommand{\reviewer}[1]{} \else \newcommand{\yyenote}[1]{}
\newcommand{\mrmnote}[1]{} \newcommand{\pmnote}[1]{}
\newcommand{\jhanote}[1]{} \newcommand{\todo}[1]{ {\textcolor{red} {
      ***TODO: #1 }}} \newcommand{\fix}[1]{} \fi



\category{D.1.3}{Software}{Concurrent Programming}{ Distributed programming/parallel programming} 
\category{J.3}{Computer Applications}{Bioinformatics}

\section*{General Terms}{Design,Measurement,Theory}

 \keywords{}

\section{Introduction} \todo{SJ}

\jhanote{ECSS: Extended Collaborative Support Service } \yyenote{Do
  you want to just grab the definition from the internal docs about
  the ECSS? Save yourself a couple of paragraphs?}\jhanote{YYE, Sure}


\section{Background and Motivation} \todo{SJ}

\subsection{Scientific Problem that we are trying to help..}

\subsection{Computational Challenges}


%\subsection{Scientifc Applications of SAGA / BigJob}


\section{ECSS Projects}

\subsection{Formal description of What is ECSS - from mgmt perspective}

\subsection{Why ECSS for this requirement?}

\jhanote{(i) from a users perspective? (ii) from a software
  providers/developers perspective? (iii) from a resource providers
  perspective?}

\subsubsection{Users} (i) single point of contact, (ii) integrated capabilities,
(iii) capabilties not bound to a specific resource; scalable across
different resources

\subsubsection{Software Providers} Point (iii) above, but also focussed support and
consultant on individual machines.

\subsubsection{From XSEDE-RP} (i) Gain more than system level support/expertise, (ii)
common/shared knowledge and thus spread across community, (iii)
derived from previous point, if successful, at the end of ECSS
XSEDE/RP have a new capability/ability to provide an advanced service

\subsection{The ECSS Projects}
\yyenote{So this section talks about Ron Levy's ECSS. Charles has no ECSS, we
are doing his pro-bono, same as Dave Wright.}

\subsubsection{Distributed and Loosely Coupled Parallel Molecular Simulations
Using the SAGA API}
The first ECSS project~\cite{RonLevy} is part of an intense effort to understand
important aspects of the physics of protein-ligand recognition by
multidimensional replica exchange (RE) computer simulations. These are
compute intensive calculations which require large numbers (103-104) of
loosely coupled replicas and long simulation times (days to weeks). In
conventional implementations of RE, simulations progress in unison and
exchanges occur in a synchronous manner whereby all replicas must
reach a pre-determined state (typically the completion of a certain
number of MD steps), before exchanges are performed. This synchronous
approach has several severe limitations in terms of scalability and
control. First of all, Sufficient dedicated computational resources must
be secured for all of the replicas before the simulation can begin
execution. Second, the computational resources must be statically
maintained until the simulation is completed. Third, failure of any
replica simulation typically causes the whole calculation to abort.

The reliance on a static pool of computational resources and no
fault tolerance prevents the synchronous RE approach from being a
solution on XSEDE resources. We therefore implemented asynchronous
parallel replica exchange conformational sampling algorithms together
using the SAGA distributed computing framework to enable dynamic
scheduling of resources and adaptive control of replica
parameters. The basic idea is to allow pairs of replicas to contact
each other to initiate and perform exchanges independently from the
other replicas. Because replicas do not rely on centralized
synchronization steps, asynchronous exchange algorithms are scalable to an
arbitrary number of processors. These algorithms also circumvent
the need to maintain a static pool of processors and therefore
can be distributed logically and physically across XSEDE resources.
In that case, the number of concurrent replicas changes dynamically
depending on available resources. This mode of execution is particularly
suitable for implementation within using SAGA and tools-based upon SAGA.

\yyenote{consider breaking this down}
In terms of scope, assistance from XSEDE personnel was needed to: (i) set up and
harden the necessary SAGA, BigJob and Advert Service software infrastructures on
Ranger, Lonestar, Kraken, and Nautilus and deploy the scientific software
IMPACT~\cite{IMPACT} and AMBER~\cite{AMBER} (ii) work with the
users to enable launching NAMD, IMPACT, and AMBER distributed jobs using the
SAGA BigJob framework, (iii) work with users to develop customized RE scripts
and with the SAGA team to test and validate relevant adaptors aimed at
conducting synchronous and asynchronous ﬁle-based replica exchange simulations
in the context of the SAGA-based Pilot-Job framework, and (iv)  document and
validate the usage of the Replica-Exchange framework on XSEDE.

Hardening the software infrastructures requires profiling and benchmarking the
workflow management tools (SAGA, BigJob) to measure overhead and identify any
load spikes on the filesystems. Once the bottlenecks and problem areas were
identified, they were resolved by the SAGA development team with help from
the ECSS consultants. The SAGA development team handled all major feature
implementations and bug fixes required for this effort including custom
adaptors for lonestar and ranger. 

\subsubsection{High Throughput – High Performance MD Studies of the Nucleosome
using SAGA-based Pilot-Jobs}
The second ECSS project focuses on high throughput, high performance molecular
dynamics studies of the nucleosome~\cite{TomBishop}. Genomes in higher organisms
exist for most of the cell's cycle as a protein-DNA complex called chromatin.
Nucleosomes are the building blocks of chromatin, thus, nucleosome stability and
their positioning within chromatin impacts virtually all genetic processes
including: transcription, replication, regulation, repair.

The primary goal of the proposed simulation studies in this project is to
investigate by means of all atom molecular dynamics simulations variations in
nucleosome structure and dynamics arising from DNA chemical modifications and
from receptor binding. This is being accomplished by means of a hierarchical
modeling and simulation strategy that utilizes high throughput, high
performance all atom molecular dynamics simulation techniques. The project is
on track to generate approximately 140,000ns of molecular dynamics simulation
data for at least 150 different realizations of the nucleosome.

This project's work flow requires simulating as many as 85 independent systems
simultaneously. For each system, every nanosecond of simulation is an HPC event
that requires approximately 50 MB of input, generates 4 GB of output and scales
efficiently from as few as 32 to as many as 512 cores. On 64 processors the run
time for a single 1 ns simulation task is approximately 6 hrs. The goal is to
accumulate 50 ns of simulation for the entire set of 85 systems (4,250 1 ns
simulation tasks in total) as quickly as possible, then select a subset of
systems from the ensemble for which 500 ns of additional dynamics will be
accumulated. If we chose only 10 systems for continued studies this is still
5,000 1ns simulation tasks to be completed. To reduce the time-to-completion of
the entire workload and to manage simulation inputs/outputs on scratch file
systems, we are intend to use multiple XSEDE resources simultaneously and
therefore require data staging.

The major differences in usage modes between the two ECSS projects is that this
project requires: (i) a much larger number of ensembles running concurrently and
for longer durations, (ii) the chaining of ensembles, (iii) much larger
data-volumes, (iv)  BigJob infrastructure that currently does not support the
co-movement and coordinated movement of data(files) in conjunction with ensemble
placement. Bigjob file movement capabilities are being enhanced to provide
such support.

Support from ECSS consultants is necessary for this ECSS, specifically for disk
and storage bottlenecks. Data management is a growing issue, especially with
varying transfer rates, less-than-portable transfer mechanisms and so on.

\subsubsection{Common Ground}

Both projects share the same basic infrastructure in terms of software and
hardware. Both projects share consultant teams, software developers and some of
the scientific tools. The workflows are not dissimilar either: both ECSS
projects intend to launch pilot jobs in a dynamic workflow on distributed
resources, stage and collect data and so on.

With a view towards making these capabilties more publicly accessible and
widely used, The ECSS project also included developing a prototype
Replica-Exchange Gateway. This gateway was developed using the DARE~\cite{DARE}
scientific gateway framework. This scientific gateway prototype is hosted
on IU's Data Quarry machine~\cite{DataQuarry}. Furthermore, all tools,
utilities, and components will be placed in CSA space on Ranger,
Lonestar, and Kraken. All code and corresponding documentation will also be
maintained in a local revision control software repository which is publicly
available.


\section{Methodological Solution and Technology}

\subsection{SAGA: Simple API for Grid Applications}
 \label{ssec:saga}
 \todo{AM}

XSEDE is inherently a very complex infrastructure.  Given the wide
range of user groups and application use cases it aims to address,
and the large number and the diversity of participating resource
providers, this is to be expected.  Complex systems are though
usually very difficult to use, as that complexity and the underlying
resource diversity often translates into complicated user tools and
interfaces.  The relatively clean architecture of XSEDE is, to some
extent, addressing this problem, but is, in itself and at this point
in time, a moving target.

The Simple API for Grid Applications (SAGA) aims to address a part of
that problem, by providing a well defined and stable API, which
exposes those operations which are required on application level, but
encapsulates the complexity of translating them into the respective
operations on the XSEDE infrastructure.  In other words: SAGA tries
to move the complexity of dealing with distributed
cyberinfrastructures like XSEDE out of the application, and into the
SAGA implementation layer, while providing the semantics necessary to
efficiently implement distributed applications which can utilize
XSEDE.

SAGA is standardized in the Open Grid Forum.  It is comprised of a
core API, which defines the overall API structure, and a set of API
packages, covering topics like job submission and management, file
access and movement, and the coordination of distributed application
components.  Multiple implementations of SAGA exist, most notably a
C++/Python implementation from LSU, which is deployed on all major
XSEDE clusters (see sec.~\ref{ssec:csa}).  A newly emerging pure
python implementation (Bliss) is expected to replace it later in
2012 -- Bliss' development and testing is specifically targeting
XSEDE and FutureGrid as target platforms.



\subsection{BigJob: SAGA Pilot-Job Implementation}
\todo{PM}
The abstraction of a Pilot-Job (PJ) generalizes the reoccurring concept
of utilizing a placeholder job as a container for a set of compute tasks;
instances of that placeholder job are commonly referred to as Pilot-Jobs or
pilots. The PJ provides application- (user-) level control and management of the
set of allocated resources. BigJob (BJ) is a SAGA-based PJ framework that
implements the Pilot-API. BJ has been designed to be general purpose and
extensible. While BJ has been originally built for HPC infrastructures, such as
XSEDE and FutureGrid, it is generally also usable in other environments, such as
OSG. This extensibility mainly arises from the usage of SAGA as a common API for
accessing distributed resources. ~\cite{saga_bigjob_condor_cloud, pstar11}


\section{SAGA and BigJob on XSEDE}
\todo{MR}

This section describes the deployment, testing, and documentation of SAGA and BigJob on XSEDE resources. 

\subsection{CSA Deployment on XSEDE Machines}
 \label{ssec:csa}
 \todo{AM}

 SAGA and BigJob are deployed on all major XSEDE machines (ranger,
 kraken, lonestar, trestles, blacklight, steele) and FutureGrid hosts
 (india, sierra, hotel, alamo).  While it is possible to install and
 use SAGA and BigJob in the users home directory, it is a significant
 effort to do so, and even more effort to configure and maintain that
 installation over time. On the other hand, as SAGA is a community
 level software (as opposed to a XSEDE project deliverable or system
 level software), it is not part of the default XSEDE software stack.

 XSEDE is continuing the TeraGrid model of providing Community
 Software Areas (CSA) to communities, which basically is a system
 level area to install software packages which are shared within a
 larger community.  For the reasons listed above, we are using the CSA
 approach to deploy, configure and maintain SAGA on XSEDE and
 FutureGrid resources.

 The installation and update is semi-automatic: a set of deployment
 scripts can be used to manually trigger the update on all (or
 individual) XSEDE resources, for all or a subset of the supported
 components.  The set of supported components includes the SAGA core
 libraries, different API packages, the respectively supported
 middleware adaptors for the machines, the python bindings, and the
 bigjob package and its dependencies.  Each installation is
 automatically creating a README which describes the installed
 components, and documents the settings required to use the
 installation (environment settings like \texttt{LD\_LIBRARY\_PATH} etc).  A
 'module' file is also generated, and on some hosts symlinked into the
 systems default module path -- on all other hosts, our deployment
 wiki documents how to use the CSA module file.

 After each installation, a set of unit tests is run \textit{on the
 target machines} to ensure that not only the deployed version is
 viable, but more important, that it is correctly installed and
 configured for that target host.  The unit tests range from basic
 (low-level) environment testing, to application level test runs of a
 BigJob application.  The test README, module file and test results
 are all committed to the central SAGA code repository, and
 (partially) used to document the current state of deployment on the
 SAGA deployment wiki.

\mrnote{Andre, can you fill out this table? Or do you think it's not important?}
%\begin{table}
%\begin{tabular}{| c | c | c |}
%\hline
%Machine \& Adaptors Supported \& \\ \hline
%Lonestar \& \& \\ \hline
%Ranger \& \& \\ \hline
%Kraken \& \& \\ \hline
%Trestles \& \& \\ \hline
%\end{tabular}
%\caption{CSA Deployments on XSEDE Resources}
%\label{table:CSA-Deployments}
%\end{table}

\subsection{Developments and Setbacks}

The obvious challenges reappear with every distributed ECSS project that spans
multiple resources: the different user environments, the different shells, the
different invocation order of startup profiles on both login and compute nodes
of the different machines (lonestar, ranger, kraken, trestles). A major
complication is the differences in system versions of Python. Most machines
have an older version of Python supplemented by a Python module. The Python
module did not typically include the tools required to easilly install
user-side python modules. Therefore, a fresh installation of Python is always
present in the shared CSA space. A similar issue is encountered with GCC
compiler versions as well.

A slightly more intricate issue is the use of custom ``mpirun'' wrappers. On
ranger/lonestar it is ``ibrun'', on kraken it is ``aprun''. These wrappers
massage the nodelist files, aggregate important environment variables to launch
with the application and so on. Modifications have to be made the launch
mechanism in BigJob to account for the use of these scripts.

\yyenote{The following contains a jab at gram}
Job submission is another interesting issue. Lonestar and ranger user SGE,
Kraken and Trestles use PBS. While SAGA retains the ability to submit jobs
through the GRAM~\cite{gram} job adaptor, it is an un-necessary burden on
users. Furthermore, when GRAM submitted jobs fial, they generate a very lenghty
error report without much useful information. Both projects needed an
immediate, clear, and fail safe mechanism to submit jobs and this lead to the
development of the \textit{pbs-ssh} and \textit{sge-ssh} plugins to support
both PBS and SGE. The plugins enable local/remote launch of BigJob agents using
traditional PBS/SGE script over SAGA ssh job adaptors.

The last issue is a user-side issue. The more diverse the machines and their
environments are, the more diverse the documentation and the higher the entry
barrier. For example Kraken requires the initialization of a ``myproxy'' for
successful job submission, whereas ranger and lonestar do not. These small  but
critical differences can mean the difference between a successfull several
thousand core jobs or a week of waiting in the queue to exit on an error.


\subsection{Testing and Documentation Process}
\todo{mr, pm}
\mrnote{Show how it is a team effort, BigJob deployment pipeline,
  Interaction with chemists to address their issues and formulate
  scripts to accomodate their needs}

The source code for SAGA and BigJob is stored in a git repository~\yyenote{Can
we mention it?}. A github wiki is used as a means of storing
user guides for each of the individual XSEDE machines. The BigJob wiki stores
all information about the installation and configuration of BigJob. Only users
of the BigJob development group can edit this wiki. This wiki is public and can
be shared amongst all collaborators. Public wikis also serve as a way to promote
other people to use and try BigJob for their scientific needs.

A BigJob CSA release is the result of a production pipeline. Any newly developed
codes and bug fixes are created in branches. After review, this code is merged
onto the master branch of the git repository. The main developer is responsible
for determining when a new version of BigJob will be released. 

Members of the BigJob development team then checkout the candidate release
version from git and test on XSEDE resources. Each machine has at least two
machine specific examples in the git repository. These are example BigJob
scripts that run a job in both single and multiple communication (i.e. MPI)
mode. The first script simply runs a shell ``/bin/date`` command. Since the ECSS
project supports molecular dynamics simulations, the second script runs a real
AMBER MD simulation. These scripts test the basic functionality of BigJob using
the CSA installations and serve as the basis for testing process.

\yyenote{We do not say much about how/what we test for. It is worth mentioning
since MR put a lot of effort into it. Also mention that any changes do not
impact users...}
After testing is complete, the python code is pushed to the Python Package Index
(\textit{pypi}). This code is then installed into the CSA space using the pypi
package. Another round of testing is then completed to verify that the CSA
installations are working and no changes to the users' environment are required.
Any corresponding documentation on the github wiki is updated to reflect the
changes.

In addition to this release process, the BigJob team is engaged with members
of the ECSS team in order to resolve any system-level issues that may arise.
These issues may include but are not limited to differences in schedulers or MPI
protocols~\yyenote{Please clarify: there are no MPI protocols, do you mean
implementations?}. Additionally, the scientific collaborators use the wiki as a
starting point to run their applications on XSEDE machines. The scripts and
associated documentation explain how to use BigJob with their own applications
simply by specifiying the job decsription.~\yyenote{Should mention that in the
future a place for them to share scripts and workflows}

If the end users encounter any issues, the BigJob team works with the ECSS
consultants to resolve the problems. In order to provide the fastest possible
response time, a mailing list is used as a primary means of communication
between the ECSS consultants, the BigJob development team, and any users of
BigJob. The researchers also may have custom needs for their workflows, such as
task restarts after a given number of time steps~\yyenote{sentence needs
better tie-in}. If any questions arise during their implementation, the BigJob
team is available to fully assist the researchers. This assistance may include
the creation of custom scripts by the BigJob team for the researchers to use.


\section{Initial Results} 
\jhanote{should we get Chemists involved? }
\mrnote{Show work from chemists (if
  available), Applicability of BigJob to other applications beyond RE
  / chemists}

\yyenote{Charles Laughton Sent this. Not sure if we can include it though (not
ECSS)}
On Lonestar I have been running the following:

2 BigJobs each of 1200 cores total, running 100 sub jobs each of 12 cores, for
24 hrs
5 BigJobs each of 2400 cores total, running 50 sub jobs each of 48 cores, for 24
hrs

On Ranger:

1 BigJob of 2400 cores total, running 50 sub jobs each of 48 cores, for 24 hrs

On Kraken:

1 BigJob of 1800 cores total, running 50 sub jobs each of 36 cores, for 24 hrs

The purpose of these simulations has been to provide benchmark data in support
of a joint UK/US call for proposals due in the summer of this year (funded by
NSF and EPSRC). The aim has been to generate a range of datasets of increasing
complexity and covering a range of areas of biomolecular science that
investigate the question as to how effective a large ensemble of relatively
short MD trajectories can be in sampling conformational space, when compared to
the sampling obtained with a small number of much longer trajectories. The
datasets have also provided the raw material to test the performance of a range
of advanced data-mining tools being developed as part of this US-UK
collaboration.



%\section{Conclusions and Future Work}
\section{Discussions and Lessons Learnt}
\todo{YYE, AM} 

\mrnote{Further deployments, Replica exchange? File
  staging?, Continued support for chemists, Do we want to get into
  pilot job stuff?}
\yyenote{Obviously i will contribute here.}

The most effective aspect of this project is undoubtedly the overlap of the two
ECSS projects in terms of tools and infrastructure (software and hardware).
Added to that the familiarity of the ECSS staff with the scientific problem.
The ECSS consultants were however intimately familiar with SAGA and BigJob. The
strong collaboration between system administrators and tool developers ensures
rapid functionality development and bottlenecks/show-stopper-bugs are identified
very early in the development cycle and squashed immediately. In fact, only one
thing comes to mind in terms of improving the ECSS projects. That would be
having an ECSS consultant whose research overlaps with the scientific aims of
both projects.



\section{Acknowledgments}
We are grateful to Andre Luckow for his original development of
BigJob.  Computing resources used for this work were made possible via
NSF TRAC award TG-MCB090174 and LONI resources.  This document was
developed with support from the National Science Foundation (NSF)
under Grant No.  0910812 to Indiana University for ``FutureGrid: An
Experimental, High-Performance Grid Test-bed.''.

\bibliographystyle{abbrv}
\bibliography{tg11}
\end{document}

