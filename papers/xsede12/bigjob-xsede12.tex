\documentclass{sig-alternate}
\usepackage[numbers, sort, compress]{natbib}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{hyperref}
\usepackage{pdfsync}
\usepackage{mdwlist}


\begin{document}

\conferenceinfo{XSEDE '12, July 16-20, 2012, Chicago, USA.} {}
\CopyrightYear{2012}
\crdata{}
%\clubpenalty=10000
%\widowpenalty = 10000

% \title{BigJob: Lessons of Supporting High Throughput High Performance
%   Ensembles on XSEDE}

\title{The Anatomy of a successful ECSS Project: Leveraging Expertise
  of Users, Software-providers and XSEDE Personnel}

\numberofauthors{3}
\author{
\alignauthor Melissa Romanus\\
       \affaddr{The Cloud and Autonomic Computing Center}\\
	\affaddr{Rutgers, The State University of New Jersey} \\
       \affaddr{506 CoRE} \\
	\affaddr{Piscataway, NJ} \\
       \email{melissa.romanus@rutgers.edu}
\alignauthor Pradeep Mantha\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}\\
       \email{pmanth2@cct.lsu.edu}
\alignauthor Yaakoub El Khamra\\
       \affaddr{A Shady Computing Center}\\
       \affaddr{Austin, TX} \\
       \email{}
\and
\alignauthor Andre Merzky\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}\\
       \email{amerzky@...}
\alignauthor Shantenu Jha\\
       \affaddr{Her Majesty's Secret Service}\\
       \affaddr{No Comment} \\
       \email{Can't tell you that love}
}

\maketitle


\begin{abstract}
\end{abstract}


\newif\ifdraft 
\drafttrue 
\ifdraft
\newcommand{\mrnote}[1]{{\textcolor{green} { ***MR: #1 }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\newcommand{\yyenote}[1]{ {\textcolor{cyan} { ***YYE: #1 }}}
\newcommand{\pmnote}[1]{ {\textcolor{blue} { ***PM: #1 }}}
\newcommand{\todo}[1]{ {\textcolor{red} { ***TODO: #1 }}}
\newcommand{\fix}[1]{ {\textcolor{red} { ***FIX: #1 }}}
\newcommand{\reviewer}[1]{} 
\else \newcommand{\yyenote}[1]{}
\newcommand{\mrmnote}[1]{} \newcommand{\pmnote}[1]{}
\newcommand{\jhanote}[1]{} \newcommand{\todo}[1]{ {\textcolor{red} {
      ***TODO: #1 }}} \newcommand{\fix}[1]{} \fi



\category{D.1.3}{Software}{Concurrent Programming}{ Distributed programming/parallel programming} 
\category{J.3}{Computer Applications}{Bioinformatics}

\section*{General Terms}{Design,Measurement,Theory}

 \keywords{}

\section{Introduction} \todo{SJ}

\jhanote{ECSS: Extended Collaborative Support Service } \yyenote{Do
  you want to just grab the definition from the internal docs about
  the ECSS? Save yourself a couple of paragraphs?}\jhanote{YYE, Sure}


\section{Background and Motivation} \todo{SJ}

\subsection{Scientific Problem that we are trying to help..}

\subsection{Computational Challenges}


%\subsection{Scientifc Applications of SAGA / BigJob}


\section{ECSS Projects}

\subsection{Formal description of What is ECSS - from mgmt perspective}

\subsection{Why ECSS for this requirement?}

\jhanote{(i) from a users perspective? (ii) from a software
  providers/developers perspective? (iii) from a resource providers
  perspective?}

\subsubsection{Users} (i) single point of contact, (ii) integrated capabilities,
(iii) capabilties not bound to a specific resource; scalable across
different resources

\subsubsection{Software Providers} Point (iii) above, but also focussed support and
consultant on individual machines.

\subsubsection{From XSEDE-RP} (i) Gain more than system level support/expertise, (ii)
common/shared knowledge and thus spread across community, (iii)
derived from previous point, if successful, at the end of ECSS
XSEDE/RP have a new capability/ability to provide an advanced service

\subsection{ECSS Project Description}
\todo{YYE}\yyenote{The tense needs to be revised. WIP} This project is
part of an intense eﬀort to understand important aspects of the
physics of protein-ligand recognition by multidimensional replica
exchange (RE) computer simulations. These are compute intensive
calculations which are currently not well supported on the XSEDE
because they require both large numbers (103-104) of loosely coupled
replicas and long simulation times (days to weeks). In conventional
implementations of RE, simulations progress in unison and exchanges
occur in a synchronous manner whereby all replicas must reach a
pre-determined state (typically the completion of a certain number of
MD steps), before exchanges are performed. This synchronous approach
has several severe limitations in terms of scalability and
control. First of all, suﬃcient dedicated computational resources must
be secured for all of the replicas before the simulation can begin
execution. Secondly, the computational resources must be statically
maintained until the simulation is completed. Thirdly, failure of any
replica simulation typically causes the whole calculation to abort.
The reliance on a static pool of computational resources and zero
fault tolerance prevents the synchronous RE approach from being a
solution on XSEDE resources. We plan to implement asynchronous
parallel replica exchange conformational sampling algorithms together
with the SAGA distributed computing framework to enable dynamic
scheduling of resources and adaptive control of replica
parameters. The basic idea is to allow pairs of replicas to contact
each other to initiate and perform exchanges independently from the
other replicas.  Because they do not rely on centralized
synchronization steps, these algorithms are scalable to an arbitrary
number of processors and avoid the requirement of maintaining a static
pool of processors. Thus the method is suitable for deployment in both
logically and physically distributed environments, in which the number
of concurrently running replicas changes dynamically depending on the
available resources. This mode of execution is particularly suitable
for implementation within using SAGA and tools-based upon SAGA.

Prepare the software infrastructure to conduct large scale distributed and
coupled replica exchange molecular dynamics simulations using SAGA with the
IMPACT and AMBER molecular simulation programs.


In terms of scope, assistance from XSEDE personnel was needed to: (i) set up and
harden the necessary SAGA, BigJob and Advert Service software infrastructures on
Ranger, Lonestar, Kraken, and Nautilus, (ii) work with the users to enable
launching NAMD, IMPACT, and AMBER distributed jobs using the SAGA BigJob
framework, (iii) work with users to develop customized RE scripts and with the
SAGA team to test and validate relevant adaptors aimed at conducting synchronous
and asynchronous ﬁle-based replica exchange simulations in the context of the
SAGA-based Pilot-Job framework, and (iv)  document and validate the usage of the
Replica-Exchange framework on XSEDE.

Hardening the software infrastructures requires profiling and benchmarking
workflow management tools (SAGA, BigJob) to measure overhead and identify any
load spikes on the filesystems. Once the bottlenecks and problem areas are
identified, they will be resolved by the SAGA development team with help from
the AUSS consultants. The SAGA development team will handle all major feature
implementations and bug fixes required for this effort. 

The IMPACT program is a biomolecular simulation program developed at Rutgers
University since the 1980’s, currently primarily focused on implicit solvent and
parallel replica exchange conformational sampling modeling. IMPACT is also the
molecular mechanics engine for the widely used Glide docking program and other
commercial drug design software products. A freely distributable academic
version of IMPACT provided by Levy \& Gallicchio will be used for this project.
AMBER is a widely used molecular simulation program. York and Lee are AMBER
developers and will receive support from David Case, also now at Rutgers
University, who is the lead developer of the AMBER package. Levy \& Gallicchio
will provide sample input files for IMPACT and York \& Lee will provide those for
AMBER. These will be used by the AUSS team as a platform to develop file-based
replica exchange protocols. Amber and IMPACT both have built-in quantum
methodologies that could take advantage of larger shared memory platform,
Nautilus. This will add another resource for researchers of quantum mechanical
simulations that depends on a higher memory overhead. However, Amber has been
selected to be ported and tested on Nautilus for immediate research purposes.
SAGA (http://saga.cct.lsu.edu) is a software infrastructure that supports the
development, deployment, and execution of a range of distributed applications
across multiple architectures. SAGA implements an Open Grid Forum Standard API –
endorsed by the global distributed computing community, in the same technical
sense as MPI is for the parallel computing community; SAGA is an important
access-layer to XSEDE.  NAMD is one of the most widely used molecular simulation
packages on XSEDE and other HPC resources.

With a view towards making these capabilties more accessible  and more widely
used we also suggest developing a  prototype of a Replica-Exchange Gateway. We
propose to build upon existing DARE capabilities but will need input and
contribution from XSEDE  Gateway specialists.  

All tools, utilities, and components will be placed in CSA space on Ranger,
Lonestar, and Kraken. All code and corresponding documentation will also be
maintained in a local revision control software repository which is publicly
available.


\jhanote{cut and paste from the proposals!}

\mrnote{brief outline and science objectives of two projects,
  including Charles Laughton, Tom Bishop, Rutgers chemists}

I have this somewhat ready, will flesh out tomorrow
afternoon. 

\section{Methodological Solution and Technology}
\subsection{SAGA: Simple API for Grid Applications}
\todo{AM}

\subsection{BigJob: SAGA Pilot-Job Implementation}
\todo{PM}

\section{SAGA and BigJob on XSEDE}
\todo{MR}
\mrnote{SW description of SAGA, SW description of BigJob}

\subsection{CSA Deployment on XSEDE Machines}
\todo{AM}

%\begin{table}
%\begin{tabular}{| c | c | c |}
%\hline
%Machine \& Adaptors Supported \& \\ \hline
%Lonestar \& \& \\ \hline
%Ranger \& \& \\ \hline
%Kraken \& \& \\ \hline
%Trestles \& \& \\ \hline
%\end{tabular}
%\caption{CSA Deployments on XSEDE Resources}
%\label{table:CSA-Deployments}
%\end{table}

\subsection{Developments and Setbacks}


\yyenote{I will contribute to the set backs. SJ How full do you want
  the description of the setbacks? } \jhanote{If not in gory detail,
  at least bullet point everything that we encountered. Later onwards
  if we can come up with some classification of the problems
  encountered that will be good/helpful}


Must include the following:
\begin{itemize}
 \item Setting up user environments differs from one machine to another. This
includes the type of shell, the startup profile file names, the startup
procedure in which the startup profile files are loaded on front and back ends
of the machine
 \item System installations of Python are almost always old, and Python does not
lend itself to modular user-side installations. Therefore an entire Python
installation is required in the CSA as opposed to using the system or
python-in-a-module (a-la-can)
 \item GCC compilers can be out of date on aging machines, this means we have
to rely on the gcc software stack and risk having to compile everything from MPI
to Amber
 \item Custom mpirun wrappers cause issues: ibrun on ranger/lonestar and aprun
on kraken. Interfaces to these wrappers have to be hardcoded in the BigJob
manager. It is important to note that the most problematic issue is the hostfile
management and mpd boot (if any). For this reason, we find ourselves using
OpenMPI as opposed to mvapich at the cost of lower performance for
per-simulation large core counts (over 256)
 \item Supercomputers are not uniform. File paths are not uniform. Deployment
and usage instructions therefore cannot be uniform. This makes the entire
process confusing and frustrating to novice users. We understand not much can
be done about this.
 \item more to follow once I have thought of it

% pmantha- kind of motivation for plugin development.
\item SAGA job adaptors provide an uniform access to heterogenous resources and BigJob uses the job adaptors 
to launch the BigJob agents on both local and remote resources. While SAGA job adaptors are designed to support 
standard job submission, scheduling systems and their configurations, some of the resources have their custom 
configurations and lack necessary job submission capabilities, which limits the distributed and extensibility capabilities 
of SAGA BigJob on XSEDE. We overcome, the problem by developing plugins combining SAGA adaptors with traditional 
job submission capabilities, which are supported by all the infrastructures without affecting BigJob usage and introducing 
new adaptors or job submission methods. 

We developed plugins \textit{pbs-ssh} and \textit{sge-ssh} to support both PBS and SGE scheduling systems hiding internal details
from the users. The plugins enable local/remote launch of BigJob agents using traditional PBS/SGE script over SAGA ssh job adaptors. 
\end{itemize}



\todo{mr}
\mrnote{List initial setbacks, Show how the setbacks created new
  developments. (Pradeep's new adaptors).}

\mrnote{Can Pradeep write the blurb regarding his adaptors?}

\subsection{Testing and Documentation Process}
\todo{mr, pm}
\mrnote{Show how it is a team effort, BigJob deployment pipeline,
  Interaction with chemists to address their issues and formulate
  scripts to accomodate their needs}

The code for SAGA and BigJob is stored in a git repository. As a result, a github wiki was utilized as a means of storing user guides for each of the individual XSEDE machines. The BigJob wiki stores all information about the installation and configuration about BigJob. Only users of the BigJob development group can edit this wiki. This wiki is public and can be shared amongst all groups that we are working with. Public wikis also serve as a way to promote other people to use and try BigJob for their scientific needs.

A BigJob CSA release is the result of a production pipeline. Any newly developed codes and bug fixes are created in branches. After review, this code is merged onto the master branch of the git repository. The main developer is responsible for determining when a new version of BigJob will be released. 

Members of the BigJob development team then checkout the candidate release version from git and test on XSEDE resources. Each machine has at least two machine specific examples in the git repository. These are example BigJob scripts that run a job in both single and multiple communication (i.e. MPI) mode. The first script simply runs a shell /bin/date. Since the ECSS project supports molecular dynamics simulations, the second script runs a real Assisted Model Building with Energy Refinement (AMBER) MD simulation. 

After testing is complete, the python code is pushed to the Python Package Index (pypi). This code is then installed into the CSA spaces allocated on all machines using the pypi package. Another round of testing is then completed to verify that the CSA installations are working and no changes to the users' environment are required.


\section{Initial Results} 
\jhanote{should we get Chemists involved? }
\mrnote{Show work from chemists (if
  available), Applicability of BigJob to other applications beyond RE
  / chemists}
\yyenote{I will get the list}

%\section{Conclusions and Future Work}
\section{Discussions and Lessons Learnt}
\todo{YYE, AM} 

\mrnote{Further deployments, Replica exchange? File
  staging?, Continued support for chemists, Do we want to get into
  pilot job stuff?}
\yyenote{Obviously i will contribute here.}

\section{Acknowledgments}
We are grateful to Andre Luckow for his original development of
BigJob.  Computing resources used for this work were made possible via
NSF TRAC award TG-MCB090174 and LONI resources.  This document was
developed with support from the National Science Foundation (NSF)
under Grant No.  0910812 to Indiana University for ``FutureGrid: An
Experimental, High-Performance Grid Test-bed.''.

\bibliographystyle{abbrv}
\bibliography{tg11}
\end{document}

