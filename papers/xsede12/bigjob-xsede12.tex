\documentclass{sig-alternate}
\usepackage[numbers, sort, compress]{natbib}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{hyperref}
\usepackage{pdfsync}
\usepackage{mdwlist}


\begin{document}

\conferenceinfo{XSEDE '12, July 16-20, 2012, Chicago, USA.} {}
\CopyrightYear{2012}
\crdata{}
%\clubpenalty=10000
%\widowpenalty = 10000

% \title{BigJob: Lessons of Supporting High Throughput High Performance
%   Ensembles on XSEDE}

%\title{The Anatomy of a successful ECSS Project: Leveraging Expertise
%  of Users, Software-providers and XSEDE Personnel}

\title{The Anatomy of Successful ECSS Projects: Lessons of Supporting
  High-Throughput High-Performance Ensembles on XSEDE}

\numberofauthors{6}
\author{
\alignauthor Melissa Romanus\\
       \affaddr{The Cloud and Autonomic Computing Center}\\
	\affaddr{Rutgers University} \\
       \affaddr{94 Brett Road} \\
	\affaddr{Piscataway, NJ} \\
       \email{melissa@cac.rutgers.edu}
\alignauthor Pradeep Kumar Mantha\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}
       \email{pmanth2@cct.lsu.edu}
\alignauthor Yaakoub El Khamra\\
       \affaddr{A Shady Computing Center}\\
       \affaddr{Austin, TX} \\
       \email{}
\and
\alignauthor Matt McKenzie\\
       \affaddr{NICS .. }\\
       \affaddr{xxx} \\
       \email{}
\alignauthor Tom Bishop\\
       \affaddr{LaTECH .. }\\
       \affaddr{xxx} \\
       \email{}
\alignauthor Emilio Gallichio\\
       \affaddr{Rutgers .. }\\
       \affaddr{xxx} \\
       \email{}
\and
\alignauthor Andre Merzky\\
       \affaddr{Center for Computation and Technology}\\
       \affaddr{Louisiana State University}\\
       \affaddr{216 Johnston}\\
       \affaddr{Baton Rouge, LA}\\
       \email{amerzky@...}
\alignauthor Shantenu Jha\\
     \affaddr{The Cloud and Autonomic Computing Center}\\
     \affaddr{Rutgers University}\\
      \affaddr{94 Brett Road}\\
      \affaddr{Piscataway, NJ}
     \email{shantenu.jha@rutgers.edu}
}

\maketitle


\begin{abstract}
The Extended Collaborative Support Service (ECSS) of XSEDE is a means
of providing support for advance user requirements that cannot and
should not be supported via a regular ticketing system. Recently two
ECSS projects were awarded by XSEDE management to support the
high-throughput of high-performance (HTHP) molecular dynamics (MD)
simulations; both of these ECSS projects are using SAGA-based
Pilot-Jobs approach as the technology required to support the HTHP
scenarios.  More significantly, these projects were envisioned as
three-way collaborations: between the application stake-holders,
advanced/research software development team and the resource
providers. In this paper, we describe the aims and objective of these
ECSS projects, how the deliverables have been met, and some
preliminary results obtained. We also describe how SAGA has been
deployed on XSEDE as a precursor to the seamless uptake of BigJobs.
\end{abstract}


\newif\ifdraft \drafttrue \ifdraft
\newcommand{\mrnote}[1]{{\textcolor{green} { ***MR: #1 }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\newcommand{\yyenote}[1]{ {\textcolor{cyan} { ***YYE: #1 }}}
\newcommand{\pmnote}[1]{ {\textcolor{blue} { ***PM: #1 }}}
\newcommand{\todo}[1]{ {\textcolor{red} { ***TODO: #1 }}}
\newcommand{\fix}[1]{ {\textcolor{red} { ***FIX: #1 }}}
\newcommand{\reviewer}[1]{} \else \newcommand{\yyenote}[1]{}
\newcommand{\mrmnote}[1]{} \newcommand{\pmnote}[1]{}
\newcommand{\jhanote}[1]{} \newcommand{\todo}[1]{ {\textcolor{red} {
      ***TODO: #1 }}} \newcommand{\fix}[1]{} \fi



\category{D.1.3}{Software}{Concurrent Programming}{ Distributed programming/parallel programming} 
\category{J.3}{Computer Applications}{Bioinformatics}

\section*{General Terms}{Design,Measurement,Theory}

 \keywords{}

\section{Introduction} \todo{SJ}

\jhanote{ECSS: Extended Collaborative Support Service } \yyenote{Do
  you want to just grab the definition from the internal docs about
  the ECSS? Save yourself a couple of paragraphs?}\jhanote{YYE, Sure}

\section{Background and Motivation} \todo{SJ}

%\subsection{Scientific Problem that we are trying to help..}

Most run-time environments and tools for high-performance and
distributed computing assume either that there is a single instance of
a task/simulation that must be executed, or there is a complex
dependency (and thus workflow) associated with distinct ``tasks''.
However, in an increasingly large number of scientific problems, there
is a need for a large number of similar tasks to be run concurrently.

There is a fundamental need for supporting ensemble-based simulations
across a range of disciplines, including but not limited to
biological, chemical, environmental and material sciences; existing
capabilities are insufficient to support ES at scale.

Depending upon the specific problem, the identical tasks may vary in
the number of cores required, or in the degree of coupling between the
tasks.  Either way there are missing capabalities to support the
multiple similar tasks that need to run concurrently


% In other words, an ideal framework would be interoperable with a range
% of modeling software and extensible to emerging computational
% platforms such as clouds and GPU-computers, and thus usable on a range
% of underlying distributed resources and independent of the
% machine/resource-specific middleware and services.

\subsection{Computational Challenges}

The challenge for ES is that of managing multiple tasks concurrently;
typically for a given science problem this number does not vary during
the lifetime of the execution. Although, similar to workflows, the
objective is the reduction of the makespan, but in contrast to
traditional workflows there isn't a multi-level dependency to solve
and thus the execution of ES is not fundamentally a challenging
scheduling problem.

The concurrent support of multiple similar simulations is requires the
Efficient and transparent management of a large number of ensembles,
possibly on many heterogeneous resources. The same run-time solution
should be applicable to a range of physical model sizes -- from
thousands of atoms to hundreds of thousands of atoms.%  Effectively
% manage large-scale data transfers and analysis, and all of
The above should be addressed without being tied to a specific
underlying application kernel or physical infrastructure. 
 
Although multi-site resources have the potential for substantial
improvements in time-to-completion of ES, the capability to
effectively utilize NSF's National Production Cyberinfrastructure
XSEDE is missing, as developing, deploying and executing these
capabilities to use the collective power of XSEDE is a challenging
undertaking, which cannot be carried forth by a single group or
sustained without the involvement of all stakeholders: scientists,
resource providers and software-developers.

\section{ECSS Projects}

\subsection{Formal description of What is ECSS - from mgmt perspective}

\subsection{Why ECSS for this requirement?}

\jhanote{(i) from a users perspective? (ii) from a software
  providers/developers perspective? (iii) from a resource providers
  perspective?}

\subsubsection{Users} (i) single point of contact, (ii) integrated capabilities,
(iii) capabilties not bound to a specific resource; scalable across
different resources

\subsubsection{Software Providers} Point (iii) above, but also focussed support and
consultant on individual machines.

\subsubsection{From XSEDE-RP} (i) Gain more than system level support/expertise, (ii)
common/shared knowledge and thus spread across community, (iii)
derived from previous point, if successful, at the end of ECSS
XSEDE/RP have a new capability/ability to provide an advanced service

\subsection{The ECSS Projects}
\yyenote{So this section talks about Ron Levy's ECSS. Charles has no ECSS, we
are doing his pro-bono, same as Dave Wright.}

\subsubsection{Distributed and Loosely Coupled Parallel Molecular Simulations
Using the SAGA API}
The first ECSS project~\cite{RonLevy} is part of an intense effort to understand
important aspects of the physics of protein-ligand recognition by
multidimensional replica exchange (RE) computer simulations. These are
compute intensive calculations which require large numbers (103-104) of
loosely coupled replicas and long simulation times (days to weeks). In
conventional implementations of RE, simulations progress in unison and
exchanges occur in a synchronous manner whereby all replicas must
reach a pre-determined state (typically the completion of a certain
number of MD steps), before exchanges are performed. This synchronous
approach has several severe limitations in terms of scalability and
control. First of all, Sufficient dedicated computational resources must
be secured for all of the replicas before the simulation can begin
execution. Second, the computational resources must be statically
maintained until the simulation is completed. Third, failure of any
replica simulation typically causes the whole calculation to abort.

The reliance on a static pool of computational resources and no
fault tolerance prevents the synchronous RE approach from being a
solution on XSEDE resources. We therefore implemented asynchronous
parallel replica exchange conformational sampling algorithms together
using the SAGA distributed computing framework to enable dynamic
scheduling of resources and adaptive control of replica
parameters. The basic idea is to allow pairs of replicas to contact
each other to initiate and perform exchanges independently from the
other replicas. Because replicas do not rely on centralized
synchronization steps, asynchronous exchange algorithms are scalable to an
arbitrary number of processors. These algorithms also circumvent
the need to maintain a static pool of processors and therefore
can be distributed logically and physically across XSEDE resources.
In that case, the number of concurrent replicas changes dynamically
depending on available resources. This mode of execution is particularly
suitable for implementation within using SAGA and tools-based upon SAGA.

\yyenote{consider breaking this down}
In terms of scope, assistance from XSEDE personnel was needed to: (i) set up and
harden the necessary SAGA, BigJob and Advert Service software infrastructures on
Ranger, Lonestar, Kraken, and Nautilus and deploy the scientific software
IMPACT~\cite{IMPACT} and AMBER~\cite{AMBER} (ii) work with the
users to enable launching NAMD, IMPACT, and AMBER distributed jobs using the
SAGA BigJob framework, (iii) work with users to develop customized RE scripts
and with the SAGA team to test and validate relevant adaptors aimed at
conducting synchronous and asynchronous ﬁle-based replica exchange simulations
in the context of the SAGA-based Pilot-Job framework, and (iv)  document and
validate the usage of the Replica-Exchange framework on XSEDE.

Hardening the software infrastructures requires profiling and benchmarking the
workflow management tools (SAGA, BigJob) to measure overhead and identify any
load spikes on the filesystems. Once the bottlenecks and problem areas were
identified, they were resolved by the SAGA development team with help from
the ECSS consultants. The SAGA development team handled all major feature
implementations and bug fixes required for this effort including custom
adaptors for lonestar and ranger. 

\subsubsection{High Throughput – High Performance MD Studies of the Nucleosome
using SAGA-based Pilot-Jobs}
The second ECSS project focuses on high throughput, high performance molecular
dynamics studies of the nucleosome~\cite{TomBishop}. Genomes in higher organisms
exist for most of the cell's cycle as a protein-DNA complex called chromatin.
Nucleosomes are the building blocks of chromatin, thus, nucleosome stability and
their positioning within chromatin impacts virtually all genetic processes
including: transcription, replication, regulation, repair.

The primary goal of the proposed simulation studies in this project is to
investigate by means of all atom molecular dynamics simulations variations in
nucleosome structure and dynamics arising from DNA chemical modifications and
from receptor binding. This is being accomplished by means of a hierarchical
modeling and simulation strategy that utilizes high throughput, high
performance all atom molecular dynamics simulation techniques. The project is
on track to generate approximately 140,000ns of molecular dynamics simulation
data for at least 150 different realizations of the nucleosome.

This project's work flow requires simulating as many as 85 independent systems
simultaneously. For each system, every nanosecond of simulation is an HPC event
that requires approximately 50 MB of input, generates 4 GB of output and scales
efficiently from as few as 32 to as many as 512 cores. On 64 processors the run
time for a single 1 ns simulation task is approximately 6 hrs. The goal is to
accumulate 50 ns of simulation for the entire set of 85 systems (4,250 1 ns
simulation tasks in total) as quickly as possible, then select a subset of
systems from the ensemble for which 500 ns of additional dynamics will be
accumulated. If we chose only 10 systems for continued studies this is still
5,000 1ns simulation tasks to be completed. To reduce the time-to-completion of
the entire workload and to manage simulation inputs/outputs on scratch file
systems, we are intend to use multiple XSEDE resources simultaneously and
therefore require data staging.

The major differences in usage modes between the two ECSS projects is that this
project requires: (i) a much larger number of ensembles running concurrently and
for longer durations, (ii) the chaining of ensembles, (iii) much larger
data-volumes, (iv)  BigJob infrastructure that currently does not support the
co-movement and coordinated movement of data(files) in conjunction with ensemble
placement. Bigjob file movement capabilities are being enhanced to provide
such support.

Support from ECSS consultants is necessary for this ECSS, specifically for disk
and storage bottlenecks. Data management is a growing issue, especially with
varying transfer rates, less-than-portable transfer mechanisms and so on.

\subsubsection{Common Ground}

Both projects share the same basic infrastructure in terms of software and
hardware. Both projects share consultant teams, software developers and some of
the scientific tools. The workflows are not dissimilar either: both ECSS
projects intend to launch pilot jobs in a dynamic workflow on distributed
resources, stage and collect data and so on.

With a view towards making these capabilties more publicly accessible and
widely used, The ECSS project also included developing a prototype
Replica-Exchange Gateway. This gateway was developed using the DARE~\cite{DARE}
scientific gateway framework. This scientific gateway prototype is hosted
on IU's Data Quarry machine~\cite{DataQuarry}. Furthermore, all tools,
utilities, and components will be placed in CSA space on Ranger,
Lonestar, and Kraken. All code and corresponding documentation will also be
maintained in a local revision control software repository which is publicly
available.


\section{Methodological Solution and Technology}


\subsection{SAGA: Standards-based Access to the Resources Layer}

The Simple API for Grid Applications (SAGA)~\cite{saga_url} is an
implementation of an Open Grid Forum (OGF) Technical Specification
that provides a common and consistent high-level API for the most
commonly required functionality to construct distributed applications.
It also provides a high-level API to construct tools and frameworks to
support distributed applications. The functional areas that are
supported by SAGA include job-submission, file transfer and access, as
well as support for data streaming and distributed coordination. SAGA
provides both a syntactic and semantic unification via a single
interface to access multiple, semantically distinct middleware
distributions.

%SAGA is an API that provides the basic functionality for developing
%distributed applications, tools and frameworks. 

The key advantages of the development using SAGA include, but are not
limited to: i) to provide a general-purpose, commonly used yet
standardized functionality, while hiding complexity of heterogeneity
of back-end resources, ii) to provide building blocks for constructing
higher-level functionality and abstractions, iii) to provide the means
for developing broad range of distributed applications such as
gateways, workflows, application management systems, and runtime
environments. Interestingly, SAGA provides an integrated, light-weight
approach to support scripting for building distributed applications.

Different aspects of SAGA appeal to different groups. The
standardization of SAGA as an OGF Standard is important because it
makes it more likely that production infrastructures, like NSF XSEDE,
EU PRACE and Open Science Grid, will support SAGA. Having SAGA
deployed and tested on these systems makes it readily available for
users and developers of national Cyberinfrastructure projects. The
fact that SAGA is an OGF technical specification also makes SAGA
highly appealing to application frameworks, services and tool
developers, which is quite understandable, as it not only simplifies
their development but also makes for scalable, extensible and
sustainable development. Users find the simple and extensible
interface providing the basic abstractions required for distributed
computing is very appealing to add their own ``functionality'' to a
core base of functionality. Furthermore, SAGA is now part of the
``official'' access layer for the \$121M NSF TG/XSEDE
project~\cite{XSEDE}, as well as for the world largest distributed
infrastructure EGI~\cite{EGI}

The SAGA API provides the base abstractions upon which tools and
frameworks that provide higher-level functionality can be
implemented. Ref.~\cite{saga_url} discusses distributed application
frameworks and run-time systems that SAGA has been used to develop
successfully. 

\begin{figure}[t]
\centering
\includegraphics[width=0.44\textwidth]{./figs/saga-architecture-1}
\caption{\textbf{SAGA Overview: } SAGA is an OGF technical
  specification that provides a common interface to heterogeneous DCI
  -- hitherto typically Grid systems.  The implementation of the
  SAGA\cite{saga_url} specification consists of a high-level {\it
    API}, the SAGA {\it Engine} providing that API, and backend,
  system-specific {\it Adaptors}.  The engine is a lightweight,
  highly-configurable dynamic library that manages call dispatching
  and the dynamic runtime loading of the middleware adaptors.  Each of
  these adaptors implements the functionality of a specific functional
  package (e.g., job adaptors, file adaptors) for a specific
  middleware system. Adaptors are also realized as dynamic libraries.}
 \label{fig:saga-overview}
\end{figure}

A SAGA implementation consists of a high-level API, the SAGA
Engine providing that API, and backend, middleware/systems specific
adaptors. Each of these adaptors implements the functionality of
a functional package (e.g., job adaptors, file adaptors) for a
specific middleware system. The engine is a dynamic library that
manages call dispatching and the runtime loading of the middleware
adaptors. Adaptors are also realized as dynamic libraries. The SAGA
API has been used (in C++, Python and Java versions) to provide almost
complete coverage over nearly all grid and distributed computing
middleware/systems, including but not limited to Condor, Genesis,
Globus, UNICORE, LSF/PBS/Torque and Amazon EC2.

SAGA is currently used on production Cyberinfrastructure in several
ways.  Admittedly the number of first-principle distributed
applications developed is low, but SAGA has been used to develop
``glue-code'' and tools that are used to submit and marshal jobs \&
data across and between heterogeneous resources. Specifically, it has
been used to support multiple computational biology applications that
use high-performance and high-throughput molecular dynamics (MD)
simulations.

SAGA has been used to develop a standards-based library for Science
Gateways to easily utilize different distributed resources; some
science domains that are using SAGA-based Science Gateways include
gateways to support next-generation sequencing, docking and
high-throughput of ensembles.

HPDC infrastructure is by definition comprised of a set of resources
that is fluctuating -- growing, shrinking, changing in load and
capability. This is in contrast to a static resource utilization model
traditionally a characteristic of parallel and cluster computing. The
ability to utilize a dynamic resource pool is thus an important
attribute of any application that needs to utilize HPDC infrastructure
efficiently. Applications that support dynamic execution have the
ability to respond to a fluctuating resource pool, i.\,e., the set of
resources utilized at time (T), $T=0$ is not the same as $T>0$.  Thus,
the need to support dynamic execution is widespread for computational
science applications; here we accomplish this by using the SAGA-based
Pilot-Job.

\subsection{SAGA-based Pilot-Job: BigJob}

Workload management and resource scheduling can lead to significant dynamic 
fluctuations in workloads and resources, reducing the overall efficiency
and speed of the desired calculations. A common approach for decoupling 
these competing allocation problems is the use of \emph{pilot-jobs (PJ)}. The PJ
abstraction is also a promising route to address additional
requirements of distributed scientific
applications~\cite{ko-efficient,bigjob_cloudcom10}, such as
application-level scheduling.

% A prominent example of such a \textit{run-time system} is the SAGA-based
% pilot-job -- which provides an interoperable pilot-job (see next
% subsection), with consistent semantics and common usage across
% different resources. The SAGA-based pilot-job extends and generalizes
% the commonly used Pilot-Job concept, and as a solid validation of the
% base standards and abstractions that can be built upon them, has
% enabled applications to use the same pilot-job API over hitherto
% disparate resources for the first time; advantages include
% scalability, simplicity and extensibility (of basic
% functionality). Similar advantages of scalability, simplicity and
% extensibility are now being experienced by Gateway developers as a
% consequence of DARE -- a SAGA-based library for developing Science
% Gateways.
 

A SAGA-based PilotJob, BigJob
(BJ)~\cite{bigjob_web,saga_bigjob_condor_cloud}, is a general-purpose
pilot-job framework. BigJob has been used to support various execution
patterns and execution workflows~\cite{async_repex11,saga-royalsoc}.
For example, SAGA-BigJob was used to execute scientific applications
categorized as embarassingly parallel applications and loosely coupled
applications on scalable distributed resources~\cite{ecmls_ccpe10,
  dare-ecmls11}

Figure~\ref{fig:figures_re_bigjob_interactions} illustrates the
architecture of BJ. BJ utilizes a Master-Worker coordination
model. The BigJob-Manager is responsible for the orchestration of
pilots, for the binding of sub-tasks. For submission of the pilots,
SAGA relies on the SAGA Job API, and thus can be used in conjunction
with different SAGA adaptors, e.\,g.\ the Globus, the PBS, the Condor
and the Amazon Web Service adaptor. Each pilot initializes a so called
BJ-agent. The agent is responsible for gathering local information and
for executing tasks on its local resource. The SAGA Advert Service API
is used for communication between manager and agent. The Advert
Service (AS) exposes a shared data space that can be accessed by
manager and agent, which use the AS to realize a push/pull
communication pattern.  The manager pushes a sub-job to the AS while
the agents periodically pull for new sub-jobs. Results and state
updates are similarly pushed back from the agent to the
manager. Furthermore, BJ provides a pluggable communication \&
coordination layer and also supports alternative c\&c systems,
e.\,g.\ Redis~\cite{redis} and ZeroMQ~\cite{zmq}.

In many scenarios it is beneficial to utilize multiple resources,
e.\,g.\ to accelerate the time-to-completion or to provide resilience
to resource failures and/or unexpected delays.  BJ supports a wide
range of application types, and is usable over a broad range of
infrastructures, i.\,e.\ it is general-purpose and extensible
(Figure~\ref{fig:figures_re_bigjob_interactions}). In addition there
are specific BJ flavors for cloud resources such as Amazon EC2 and
Microsoft Azure that are capable of managing set of VMs, as well as a
BJ with a Condor-G based backend.

BJ supports dynamic resource additions/removals as well as late
binding. The support of this feature depends on the backend used. To
support this feature on top of various BigJob implementations that are
by default restricted to single resource use (e.\,g.\ BJ), the concept
of a BigJob pool is introduced. A BigJob pool consists of multiple BJs
(each BigJob managing one particular resource). An extensible
scheduler is used for dispatching WUs to one of the BJs of the pool
(late binding). By default a FIFO scheduler is provided.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.40\textwidth]{./figs/re_bigjob_interactions}
   \caption{\textbf{BigJob Architecture:} The core of the framework,
     the BigJob-Manager orchestrates a set of pilots. Pilots are
     started using the SAGA Job API. The application submits WUs, the
     so-called sub-jobs via the BigJob-Manager. Communication between
     the BJ-Manager and BJ-Agent is done via a shared data space, the
     Advert Service. The BJ-Agent is responsible for managing and
     monitoring sub-jobs. From Ref.~\cite{saga_bigjob_condor_cloud}}
        \label{fig:figures_re_bigjob_interactions}
\end{figure}

% \subsection{BigJob: SAGA Pilot-Job Implementation}
% \todo{PM}
% The abstraction of a Pilot-Job (PJ) generalizes the reoccurring concept
% of utilizing a placeholder job as a container for a set of compute tasks;
% instances of that placeholder job are commonly referred to as Pilot-Jobs or
% pilots. The PJ provides application- (user-) level control and management of the
% set of allocated resources. BigJob (BJ) is a SAGA-based PJ framework that
% implements the Pilot-API. BJ has been designed to be general purpose and
% extensible. While BJ has been originally built for HPC infrastructures, such as
% XSEDE and FutureGrid, it is generally also usable in other environments, such as
% OSG. This extensibility mainly arises from the usage of SAGA as a common API for
% accessing distributed resources. ~\cite{saga_bigjob_condor_cloud, pstar11}


\section{SAGA and BigJob on XSEDE}
\todo{MR}

This section describes the deployment, testing, and documentation of
SAGA and BigJob on XSEDE resources.


\subsection{SAGA: Simple API for Grid Applications}
 \label{ssec:saga}
 \todo{AM}

XSEDE is inherently a very complex infrastructure.  Given the wide
range of user groups and application use cases it aims to address,
and the large number and the diversity of participating resource
providers, this is to be expected.  Complex systems are though
usually very difficult to use, as that complexity and the underlying
resource diversity often translates into complicated user tools and
interfaces.  The relatively clean architecture of XSEDE is, to some
extent, addressing this problem, but is, in itself and at this point
in time, a moving target.

The Simple API for Grid Applications (SAGA) aims to address a part of
that problem, by providing a well defined and stable API, which
exposes those operations which are required on application level, but
encapsulates the complexity of translating them into the respective
operations on the XSEDE infrastructure.  In other words: SAGA tries
to move the complexity of dealing with distributed
cyberinfrastructures like XSEDE out of the application, and into the
SAGA implementation layer, while providing the semantics necessary to
efficiently implement distributed applications which can utilize
XSEDE.

A SAGA implementation ss comprised of a
core API, which defines the overall API structure, and a set of API
packages, covering topics like job submission and management, file
access and movement, and the coordination of distributed application
components.  Multiple implementations of SAGA exist, most notably a
C++/Python implementation, which is deployed on all major XSEDE
clusters (see sec.~\ref{ssec:csa}).  A newly emerging pure python
implementation (Bliss) is expected to replace it later in 2012 --
Bliss' development and testing is specifically targeting XSEDE and
FutureGrid as target platforms.

\subsection{CSA Deployment on XSEDE Machines}
 \label{ssec:csa}
 \todo{AM}

 SAGA and BigJob are deployed on all major XSEDE machines (ranger,
 kraken, lonestar, trestles, blacklight, steele) and FutureGrid hosts
 (india, sierra, hotel, alamo).  While it is possible to install and
 use SAGA and BigJob in the users home directory, it is a significant
 effort to do so, and even more effort to configure and maintain that
 installation over time. On the other hand, as SAGA is a community
 level software (as opposed to a XSEDE project deliverable or system
 level software), it is not part of the default XSEDE software stack.

 XSEDE is continuing the TeraGrid model of providing Community
 Software Areas (CSA) to communities, which basically is a system
 level area to install software packages which are shared within a
 larger community.  For the reasons listed above, we are using the CSA
 approach to deploy, configure and maintain SAGA on XSEDE and
 FutureGrid resources.

 The installation and update is semi-automatic: a set of deployment
 scripts can be used to manually trigger the update on all (or
 individual) XSEDE resources, for all or a subset of the supported
 components.  The set of supported components includes the SAGA core
 libraries, different API packages, the respectively supported
 middleware adaptors for the machines, the python bindings, and the
 bigjob package and its dependencies.  Each installation is
 automatically creating a README which describes the installed
 components, and documents the settings required to use the
 installation (environment settings like \texttt{LD\_LIBRARY\_PATH} etc).  A
 'module' file is also generated, and on some hosts symlinked into the
 systems default module path -- on all other hosts, our deployment
 wiki documents how to use the CSA module file.

 After each installation, a set of unit tests is run \textit{on the
 target machines} to ensure that not only the deployed version is
 viable, but more important, that it is correctly installed and
 configured for that target host.  The unit tests range from basic
 (low-level) environment testing, to application level test runs of a
 BigJob application.  The test README, module file and test results
 are all committed to the central SAGA code repository, and
 (partially) used to document the current state of deployment on the
 SAGA deployment wiki.

\mrnote{Andre, can you fill out this table? Or do you think it's not important?}
%\begin{table}
%\begin{tabular}{| c | c | c |}
%\hline
%Machine \& Adaptors Supported \& \\ \hline
%Lonestar \& \& \\ \hline
%Ranger \& \& \\ \hline
%Kraken \& \& \\ \hline
%Trestles \& \& \\ \hline
%\end{tabular}
%\caption{CSA Deployments on XSEDE Resources}
%\label{table:CSA-Deployments}
%\end{table}

\subsection{Developments and Setbacks}

The obvious challenges reappear with every distributed ECSS project that spans
multiple resources: the different user environments, the different shells, the
different invocation order of startup profiles on both login and compute nodes
of the different machines (lonestar, ranger, kraken, trestles). A major
complication is the differences in system versions of Python. Most machines
have an older version of Python supplemented by a Python module. The Python
module did not typically include the tools required to easilly install
user-side python modules. Therefore, a fresh installation of Python is always
present in the shared CSA space. A similar issue is encountered with GCC
compiler versions as well.

A slightly more intricate issue is the use of custom ``mpirun'' wrappers. On
ranger/lonestar it is ``ibrun'', on kraken it is ``aprun''. These wrappers
massage the nodelist files, aggregate important environment variables to launch
with the application and so on. Modifications have to be made the launch
mechanism in BigJob to account for the use of these scripts.

\yyenote{The following contains a jab at gram}
Job submission is another interesting issue. Lonestar and ranger user SGE,
Kraken and Trestles use PBS. While SAGA retains the ability to submit jobs
through the GRAM~\cite{gram} job adaptor, it is an un-necessary burden on
users. Furthermore, when GRAM submitted jobs fial, they generate a very lenghty
error report without much useful information. Both projects needed an
immediate, clear, and fail safe mechanism to submit jobs and this lead to the
development of the \textit{pbs-ssh} and \textit{sge-ssh} plugins to support
both PBS and SGE. \pmnote{  I think, the another main motive to develop these plugins is ; to submit jobs using globus on ranger is 
not possible via ranger... so we have do it from other xsede machine like lonestar... I am not sure but I think, the problem is
becoz SAGA globus adaptors has some problem with verion of globus on XSEDE}
The plugins enable local/remote launch of BigJob agents using
traditional PBS/SGE script over SAGA ssh job adaptors.


The last issue is a user-side issue. The more diverse the machines and their
environments are, the more diverse the documentation and the higher the entry
barrier. For example Kraken requires the initialization of a ``myproxy'' for
successful job submission, whereas ranger and lonestar do not. These small  but
critical differences can mean the difference between a successfull several
thousand core jobs or a week of waiting in the queue to exit on an error.


\subsection{Testing and Documentation Process}
\todo{mr, pm}
\mrnote{Show how it is a team effort, BigJob deployment pipeline,
  Interaction with chemists to address their issues and formulate
  scripts to accomodate their needs}

The source code for SAGA and BigJob is stored in a git repository~\yyenote{Can
we mention it?}. A github wiki is used as a means of storing
user guides for each of the individual XSEDE machines. The BigJob wiki stores
all information about the installation and configuration of BigJob. Only users
of the BigJob development group can edit this wiki. This wiki is public and can
be shared amongst all collaborators. Public wikis also serve as a way to promote
other people to use and try BigJob for their scientific needs.

A BigJob CSA release is the result of a production pipeline. Any newly developed
codes and bug fixes are created in branches. After review, this code is merged
onto the master branch of the git repository. The main developer is responsible
for determining when a new version of BigJob will be released. 

Members of the BigJob development team then checkout the candidate release
version from git and test on XSEDE resources. Each machine has at least two
machine specific examples in the git repository. These are example BigJob
scripts that run a job in both single and multiple communication (i.e. MPI)
mode. The first script simply runs a shell ``/bin/date`` command. Since the ECSS
project supports molecular dynamics simulations, the second script runs a real
AMBER MD simulation. These scripts test the basic functionality of BigJob using
the CSA installations and serve as the basis for testing process.

\yyenote{We do not say much about how/what we test for. It is worth mentioning
since MR put a lot of effort into it. Also mention that any changes do not
impact users...}
After testing is complete, the python code is pushed to the Python Package Index
(\textit{pypi}). This code is then installed into the CSA space using the pypi
package. Another round of testing is then completed to verify that the CSA
installations are working and no changes to the users' environment are required.
Any corresponding documentation on the github wiki is updated to reflect the
changes.

In addition to this release process, the BigJob team is engaged with members
of the ECSS team in order to resolve any system-level issues that may arise.
These issues may include but are not limited to differences in schedulers or MPI
protocols~\yyenote{Please clarify: there are no MPI protocols, do you mean
implementations?}. Additionally, the scientific collaborators use the wiki as a
starting point to run their applications on XSEDE machines. The scripts and
associated documentation explain how to use BigJob with their own applications
simply by specifiying the job decsription.~\yyenote{Should mention that in the
future a place for them to share scripts and workflows}

If the end users encounter any issues, the BigJob team works with the ECSS
consultants to resolve the problems. In order to provide the fastest possible
response time, a mailing list is used as a primary means of communication
between the ECSS consultants, the BigJob development team, and any users of
BigJob. The researchers also may have custom needs for their workflows, such as
task restarts after a given number of time steps~\yyenote{sentence needs
better tie-in}. If any questions arise during their implementation, the BigJob
team is available to fully assist the researchers. This assistance may include
the creation of custom scripts by the BigJob team for the researchers to use.


\section{Initial Results} 
\jhanote{should we get Chemists involved? }
\mrnote{Show work from chemists (if
  available), Applicability of BigJob to other applications beyond RE
  / chemists}

\yyenote{Charles Laughton Sent this. Not sure if we can include it though (not
ECSS)}
On Lonestar I have been running the following:

2 BigJobs each of 1200 cores total, running 100 sub jobs each of 12 cores, for
24 hrs
5 BigJobs each of 2400 cores total, running 50 sub jobs each of 48 cores, for 24
hrs

On Ranger:

1 BigJob of 2400 cores total, running 50 sub jobs each of 48 cores, for 24 hrs

On Kraken:

1 BigJob of 1800 cores total, running 50 sub jobs each of 36 cores, for 24 hrs

The purpose of these simulations has been to provide benchmark data in support
of a joint UK/US call for proposals due in the summer of this year (funded by
NSF and EPSRC). The aim has been to generate a range of datasets of increasing
complexity and covering a range of areas of biomolecular science that
investigate the question as to how effective a large ensemble of relatively
short MD trajectories can be in sampling conformational space, when compared to
the sampling obtained with a small number of much longer trajectories. The
datasets have also provided the raw material to test the performance of a range
of advanced data-mining tools being developed as part of this US-UK
collaboration.



%\section{Conclusions and Future Work}
\section{Discussions and Lessons Learnt}
\todo{YYE, AM} 

\mrnote{Further deployments, Replica exchange? File
  staging?, Continued support for chemists, Do we want to get into
  pilot job stuff?}
\yyenote{Obviously i will contribute here.}

The most effective aspect of this project is undoubtedly the overlap of the two
ECSS projects in terms of tools and infrastructure (software and hardware).
Added to that the familiarity of the ECSS staff with the scientific problem.
The ECSS consultants were however intimately familiar with SAGA and BigJob. The
strong collaboration between system administrators and tool developers ensures
rapid functionality development and bottlenecks/show-stopper-bugs are identified
very early in the development cycle and squashed immediately. In fact, only one
thing comes to mind in terms of improving the ECSS projects. That would be
having an ECSS consultant whose research overlaps with the scientific aims of
both projects.

The technical difficulties encountered in porting from the first machine to the 
seocnd machine prepared the ECSS team to the possible pitfalls in porting to the
third and fourth machines. Consequently, the testing procedure incrementally
became more rigorous as the number of machines increased. Perhaps the most
important technical lesson learnt is the need to keep track of common problems
and expect them to repeat on new systems.

The importance of a rapid hot-patching deployment scheme cannot be underestimated. Whenever a bug is discovered it is immediately reported by the end users or testing team to the consultants. Issues relating to the system are quickly identified and workarounds suggested to the development team. The development team creates a hot-fix, the deployment team pushes it to the machines and it undergoes testing to ensure the user issue is resolved. Having a tightly knit group of developers and ECSS consultants makes the entire difference between resolving bugs in deployment or having end users abandon the infrastructure due to unresolved bugs. This is where involving the ECSS consultants in the infrastructure development cycle pays off.




\section{Acknowledgments}
We are grateful to Andre Luckow for his original development of
BigJob.  Computing resources used for this work were made possible via
NSF TRAC award TG-MCB090174 and LONI resources.  This document was
developed with support from the National Science Foundation (NSF)
under Grant No.  0910812 to Indiana University for ``FutureGrid: An
Experimental, High-Performance Grid Test-bed.''.

\bibliographystyle{abbrv}
\bibliography{tg11}
\end{document}

