\documentclass[a4paper,10pt]{article}

%\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}
\usepackage{multirow}
\usepackage{listings}
\usepackage{times}
\usepackage{paralist}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[hypertex]{hyperref}
\usepackage{subfigure}
\usepackage{color}
\usepackage{ifpdf}

\usepackage{texdraw}
\usepackage{epsf}
\usepackage{array}
\usepackage{cite}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{setspace}
\sloppy
\usepackage{geometry}



\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\BI}[1]{\textbf{\textit{#1}}}
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\dctf}{dC$_{25}$ }
\newcommand{\dctfnsp}{dC$_{25}$}
\newcommand{\atf}{A$_{25}$ }
\newcommand{\dco}{dC$_{1}$ }
\newcommand{\atfnsp}{A$_{25}$}
\newcommand{\dconsp}{dC$_{1}$}
\newcommand{\aonsp}{A$_{1}$}
\newcommand{\ao}{A$_{1}$ }
\newcommand{\ato}{A$_{1}$ }
\newcommand{\ahl}{$\alpha$HL }
\newcommand{\ahlnsp}{$\alpha$HL}
\newcommand{\prim}{$^{\prime}$ }
\newcommand{\primnsp}{$^{\prime}$}


\pdfpagewidth 8.5in
\pdfpageheight 11in 

\setlength\topmargin{0in}
\setlength\headheight{0in}
\setlength\headsep{0in}
\setlength\textheight{9in}
\setlength\textwidth{6.5in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\setlength\parindent{0.1in}
\setlength\parskip{0.25em}

\ifpdf
 \DeclareGraphicsExtensions{.pdf, .jpg, .png}
\else
 \DeclareGraphicsExtensions{.eps, .ps}
\fi

\newcommand{\jha}[1]{ {\textcolor{red} { ***Jha: #1 }}}

\begin{document}
\title{\LARGE % Investigating Scale-Out Performance of 
% Loosely-Coupled Simulations Using Multiple Distributed Resources on the TeraGrid.
  Scale-Up and Scale-Out of Ensemble-based Simulations}

% \author{Principal Investigator: Shantenu Jha$^{1,2}$ \\
% Co-Principal Investigator: Joohyun Kim$^{1}$ \\ 
% Co-Principal Investigator: Yaakoub El Khamra$^{3}$\\\
%    \small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State University, Baton Rouge, 
% USA}}
% \\
%   \small{\emph{$^{2}$Department of Computer Science, Louisiana State
%       University, Baton Rouge, USA}}
% \\
%   \small{\emph{$^{3}$Texas Advanced Computing Center TACC, University of Texas, Austin, USA}}}

\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\amnote}[1]{ {\textcolor{magenta} { ***AM: #1c }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\newcommand{\michaelnote}[1]{ {\textcolor{blue} { ***MM: #1 }}}
\newcommand{\yyenote}[1]{ {\textcolor{green} { ***YYE: #1 }}}
\else
\newcommand{\amnote}[1]{}
\newcommand{\jhanote}[1]{}
\newcommand{\michaelnote}[1]{ {\textcolor{blue} { ***MM: #1 }}}
\newcommand{\yyenote}[1]{ {}}
\fi

%\date{15 July 2010}

\date{}

\maketitle

\subsection*{Summary:} 

This work is built on the extensive efforts over the past several years that have carried out with a wide range of computational science and computer science projects. In this TRAC request, we propose use TeraGrid resources to investigate a broad-range of science and computer science problems using loosely-coupled atomistic simulations.  We will use multiple TeraGrid resources both in concurrent usage mode, as well as individual resources to study the several scientific problems. We are developing the cyberinfrastructure that will enable the collective utilization of TeraGrid resources.  Specifically, in this proposal we request XXXXM SUs for five distinct projects: (1) understanding translocation of nucleic acid in $\alpha$-Hemolysin protein pores; (2) elucidating the underlying mechanism of metabolite binding assisted folding of SAM-I riboswitches and other riboswitches, (3) Atomistic simulations of physiological systems, (4) Atomistic simulations of bi-layered composites, and, (5) developing and enhancing the understanding of distributed applications and testing the {\it scale-out } performance of a range of applications.  Project 3 will be carried out as part of a collaboration with Prof. Peter Coveney (UCL, Yale); Project 4 is part of an NSF funded international collaboration between TeraGrid and DEISA that is being led the PI and SI Prof. Coveney.  The projects for which resources are being requested are all funded projects -- mostly at the National/International level, and some by local resources.  Additionally, the request for XXXXM SUs in this proposal is based upon the projected science problems as outlined below as well as a proven track record of {\it successfully} utilising more than YYYYM SUs in the recent past.

\section{Results From Prior Awards}


\jhanote{This should list out (i) prior awards and what they were for, (ii) scientific progress and understanding that arose and (ii) publications as a result of prior awards.}

\jhanote{Yaakoub/Joohyun: Our award/project number for last years TRAC is MCB090174.
  Please mention resource usage statistics, be sure to mention that we've used 100\% well before
  the expiry. Mention allocation swapping between machines}

\jhanote{Publications arising from clast TRAC: (i) Project 1: JCTC and a publication on ABF are in the works; (ii) Riboswtich -- in addition to NAR, we have ECMLS and others (Joohyun to fix), (iii) UCL is listed in prior.tex so ok, (iv) Project 4, TeraGrid-talk, DEISA Interop talk, and publication in process, but also reference Cybertools publications -- ccgrid and others}
\input{prior.tex}

Using the previous year TG allocation (MCB090174), we have performed all-atom MD simulations for the SAM-I riboswtich study.   We reported our simulation results in recent conferences and the paper of obtained results were recently published in the journal, Nucleic Acids Research\cite{SAM-I-NAR2009}.  In particular, we have broaden our scope for understanding the mechanism of the riboswitch by expanding our interests to other riboswitches such as TPP riboswitches and recurrent RNA structural motifs such as K-turn, resulting interesting results that will be published soon.  Also, we added the complementary computational approaches such as RNA secondary structure prediction in addition to all-atom MD simulations, and our initial effort was presented in the workshop, "Emerging Computational Methods for the Life Sciences" in ACM HPDC 2010\cite{ecmls10}.  Currently, we are pursuing the funding opportunities based on those results.

We have also put our previous allocation to use in CO$_2$ sequestration studies
and reservoir characterization studies. These studies were composed of running an ensemble
of independent simulations followed by an anaylsis step, and iterating until a termination criteria is met.
Since the simulations are indpendent, they can be distributed across TeraGrid resources, with
a sufficiently abstract and sophisticated workflow manager. Using a custom built workflow manager,
Lazarus ~\cite{gmac}, we were able to make use of various TeraGrid machines simulatneously, and
gain deep insights. Beside the obvious science results ~\cite{TG10yye00}, we learned a great
deal about scale-out behavior of ensembles of large scale simulations, the need for fault
tolerance and autonomic behavior, as well as the promising possibilities of combining
Grid and Cloud environments. This work has resulted in several publications \cite{Cloud1,Cloud2,MSEScience,TG10yye00}  and presentations
as well as a Masters degree~\cite{Elkhamra2009}.


\section{Scientific Projects}

\jhanote{An overview...}


\section*{Project 1: Nucleic Acid Translocation through Alpha-Hemolysin Protein Nanopore}

The translocation of polynucleotides across membranes is a fundamental biological process, with important technological and medical relevance.  The translocation process is complex and is influenced by a range of factors including the diameter and inner surface of the pore, the secondary structure of the polymer, and interactions between polymer and protein. We have performed non-equilibrium constant velocity-steered molecular dynamics (cv-SMD) simulations of nucleic acid molecule translocation through the protein nanopore $\alpha$-hemolysin (\ref{fig:edge}) and used Jarzynski's identity% ~\cite{jarz} 
to determine the associated free energy profiles. Constant velocity-steered molecular dynamics~\cite{namd} (cv-SMD) is a type of non-equilibrium simulation that connects an atom or center of mass of a group of atoms via a harmonic spring moved at constant velocity. Cv-SMD has the advantage of a well defined wall-clock and simulated time-frame for a given translocation distance, allowing the induction of high-speed translocation in a consistent manner.

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=5.0in]{ahl_labelled13}
  \end{center}
  \caption{Figure representing the starting configuration of a 3\prim led \atf translocation simulation~\cite{martin-determination}. The heptameric protein pore \ahl (green) is inserted into a lipid bilayer (black). Features of the translocating molecule include the backbone of \atf (dark yellow) and the nucleic acid bases (blue). The $trans$-entrance is at the bottom of the pore; taking the $trans$-entrance of \ahl as a reference point at 0~{\AA}, other notable features include protein residue Leu-135 at 13~{\AA} (red), Met-113 at 43~{\AA} (pink), Lys-147 at 45~{\AA} (light yellow), and the $cis$-entrance at the top of the protein at 95~{\AA}. The $cis$-entrance is 28~{\AA} in diameter, the wide section of the pore running from the $cis$-entrance to residue Lys-147 is termed the inner chamber and is up to 46~{\AA} wide. The constriction marked by residues Lys-147 and Met-113 is 14~{\AA} wide, while the transmembrane barrel runs from the constriction to the $trans$-entrance and is around 20~{\AA} wide; the $trans$-entrance is 24~{\AA} wide. The C3\prim carbon atom of the 3\prim end residue of \atf is aligned with the center of mass of the C$_{\alpha}$ atoms of protein residue 111, which lies at the mouth of the constriction, just above residue Lys-147. For the sake of clarity, water molecules, sodium and chloride ions are not displayed (they are found along the entire length of the pore).}
  \label{fig:edge}
\end{figure} 


With this approach we have been able to explain the observed differences in experimental translocation time through the nanopore between polyadenosine and polydeoxycytidine. Poly(A) and poly(dC) molecules of 100-200 bases in length exhibit a 20-fold difference in translocation time through \ahl in SCCR experiments~\cite{akeson}. The translocation of both 25 base polynucleotides and single nucleotides through $\alpha$-hemolysin has been investigated. An example of our results that qualitatively agree with experimental findings can be seen in Fig.~\ref{full_trans_local}. These simulations are computationally intensive as they employ models with atomistic level resolution; in addition to their size, these systems are challenging to study due to the time-scales of translocation of large asymmetric molecules. Our simulations have provided insight into the role of the interactions between the nucleic acid molecules and the protein-pore. Mutated protein-pores have provided confirmation of residue-specific interactions between nucleotides and the protein-pore. By harnessing such molecular dynamics simulations, we have gained new physical insight into the translocation process.

This work has been published in the {\em Journal of Chemical Theory and Computation}~\cite{martin-determination}.  In the JCTC paper we pushed cv-SMD to new limits, testing the validity of the method for a larger translocating molecule and higher atom count system than previously attempted at such relatively low pulling speeds. We showed that an interaction between a positively charged lysine residue of the pore interior and the negatively charged nucleotide phosphate groups give rise to peaks in the free energy profiles. We also highlighted key ion interactions that play a role in these phosphate-lysine interactions, pointing to important considerations for future computational scientists to consider. The work that has been published so far covers relatively low sampled instances of nucleotide and single nucleotide translocation through wild type and mutated protein pores, yielding good results and has established the groundwork for further publications. 

 \begin{figure}[!h]
  \begin{center}
\subfigure{\label{fig:fulltrans-a}\includegraphics[scale=0.33]{full_trans_16sample_cv-SMD_A25_vs_dC25_nice_adjusted}}
\subfigure{\label{fig:fulltrans-b}\includegraphics[scale=0.33]{full_trans_16sample_cv-SMD_A25_vs_dC25_local_nice_adjusted}}  
    \end{center}
   \caption[Local and global free energy profiles of \atf and \dctf translocation from the top of the constriction to the bottom of the $trans$-entrance of wild type \ahl]{Local and global free energy profiles of \atf and \dctf translocation from the top of the constriction to the bottom of the $trans$-entrance of wild type \ahlnsp. A) Global free energy profile of \atf and \dctf
translocation; each profile was derived from 16 samples, calculated using a bin width of 0.75~{\AA}. Labelled along the $x$-axis are protein residues Met-147, Lys-113, and Leu-135. The residue labels span 5~{\AA} from when the pulled atom to first
phosphate atom passes the labelled residue. The free energy estimate for \atf is $\sim$30\% higher than that of \dctf at the end of the 48~{\AA} reaction coordinate. The plots show discrimination of \atf and \dctf beyond the error bars after 11~{\AA} of translocation. The gradients of both profiles gradually increase, which is in line with expectations that pulling additional nucleotides into the confining dimensions of the transmembrane barrel raises the energetic barriers to further translocation. B) Local free energy profiles of \atf and \dctf translocation; each profile was derived from 16 samples, calculated using a bin width of 2~{\AA}. For these systems, the local environments lead to consistently higher energetic barriers to translocation for \atfnsp. The \atf profiles also exhibits larger peaks than \dctfnsp, most notably at 9~{\AA} and 37~{\AA}.}
  \label{full_trans_local}
\end{figure}

Using the TRAC grant that was awarded to us last year, we have significantly improved the sampling of these results, and amassed further data. For the latest set of data, each profile is calculated from 16 samples (instead of 2-4 samples previously), which has greatly improved the quality and reliability of the data. Systems \dctfnsp-WT,  \dconsp-WT, \atfnsp-WT,  \aonsp-WT,  and \dctfnsp-Mut (see Table~\ref{table:systems1}), were included in the JCTC publication with a low number of samples; these now stand at 16 samples each. Systems \dconsp-Mut, \atfnsp-Mut, and \aonsp-Mut, were not previously explored and now have full 16 sample data-sets. Using these data-sets we have been able to make conclusions such as the negligible impact of the nucleotide base sterics, and are able to point to nucleotide base stacking as  one of the causes for the experimentally observed difference between poly(A) and poly(dC) translocation times.

Additionally, over the last year we have used the molecular models established in the cv-SMD simulations to test the system using an alternative translocation method. The method we examined was Adaptive Biasing Force (ABF). Here the translocating atom (and thus the attached molecule) is moved with a biasing force, which acts to overcome energy barriers in order to translocate along a reaction coordinate. The biasing force adapts to the free energy landscape on-the-fly, calculating the free energy and biasing force based on the forces acting on the atom in question, and applying the force directly to that atom. In this way, ABF surpasses the need for certain approximations to be made related to the use of Jarzynski's Equality and the requirement of a stiff cv-SMD harmonic spring, allowing us to assess the impact that these approximations had on the cv-SMD data. Furthermore, ABF does not constrain the biased atom in axes orthogonal to the reaction coordinate, allowing enhanced sampling of the reaction coordinate. So far, we have examined key ABF simulation parameters to allow for the exploration of the \ahl-nucleotide systems, and have produced single sample free energy profiles for \atfnsp-ABF and \dctfnsp-ABF (see Table~\ref{table:systems1}). This work is now being prepared for submission to the Journal of Computational Science (Expected September 2010).

\begin{table}[!h]
\begin{center}
  \caption{The translocation molecules and pore types to be simulated. `Wild Type' indicates \ahl with no mutated residues; `Mutant' indicates \ahl mutant L147M.\newline }
\label{table:systems1}
\begin{tabular}{| c | c | c | c | c | c |}
\hline
Pulling & System & \ahl Type & Nucleotide & Nucleotides & Samples \\
Method & Name &  & Base &  & Performed \\
\hline
cv-SMD & \aonsp-WT & Wild Type & Adenine & 1 & 16/16  \\
cv-SMD & \atfnsp-WT & Wild Type & Adenine & 25 & 16/16  \\
cv-SMD & \dconsp-WT & Wild Type & Deoxycytosine & 1 & 16/16  \\
cv-SMD & \dctfnsp-WT & Wild Type & Deoxycytosine & 25 & 16/16 \\
cv-SMD & \aonsp-Mut & Mutant & Adenine & 1 & 16/16  \\
cv-SMD & \atfnsp-Mut & Mutant & Adenine & 25 & 16/16  \\
cv-SMD & \dconsp-Mut & Mutant & Deoxycytosine & 1 & 16/16  \\
cv-SMD & \dctfnsp-Mut & Mutant & Deoxycytosine & 25 & 16/16  \\
ABF & \atfnsp-ABF & Wild Type & Adenine & 25 & 1  \\
ABF & \dctfnsp-ABF & Wild Type & Deoxycytosine & 25 & 1  \\
\hline
\end{tabular}
\end{center}
\end{table}

We have ascertained that to allow for the production of quality data that may be compared to our cv-SMD results, we require multi-sample (at least 4) data-sets of systems  \atfnsp-ABF-$\zeta$5k, \dctfnsp-ABF-$\zeta$5k,  \aonsp-ABF-$\zeta$5k,  and \dconsp-ABF-$\zeta$5k (see Table~\ref{table:systems2}). For these simulations, there is a key ABF parameter, $\zeta$, that dictates how many timesteps the biased atom is required to exist within the boundaries of a particular bin along the reaction coordinate before the biasing force in applied. This parameter influences the impact of non-equilibrium effects, the average speed of translocation, plus the computational cost, and its optimum value is highly sensitive to the size and flexibility of the translocating molecule. Therefore, we will also be performing simulations where the $\zeta$ value is pushed to more computationally intensive limits to determine its full impact on the data quality. These systems are \atfnsp-ABF-$\zeta$20k, \dctfnsp-ABF-$\zeta$20k,  \atfnsp-ABF-$\zeta$80k, \dctfnsp-ABF-$\zeta$80k, as listed in Table~\ref{table:systems2}.


\begin{table}[!h]
\begin{center}
  \caption{The translocation molecules and pore types to be simulated. `Wild Type' indicates \ahl with no mutated residues; `Mutant' indicates \ahl mutant L147M.\newline }
\label{table:systems2}
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
Pulling & System & \ahl Type & Nucleotide & Nucleotides & Samples & CPU Hours \\
Method & Name &  & Base &  & Performed & Required\\
\hline
ABF & \atfnsp-ABF-$\zeta$5k & Wild Type & Adenine & 25 & 4 & 39,732 \\
ABF & \atfnsp-ABF-$\zeta$20k & Wild Type & Adenine & 25 & 4 & 158,928 \\
ABF & \atfnsp-ABF-$\zeta$80k & Wild Type & Adenine & 25 & 2  & 317,856 \\
ABF & \dctfnsp-ABF-$\zeta$5k & Wild Type & Deoxycytosine & 25 & 4 & 39,732 \\
ABF & \dctfnsp-ABF-$\zeta$20k & Wild Type & Deoxycytosine & 25 & 4 & 158,928  \\
ABF & \dctfnsp-ABF-$\zeta$80k & Wild Type & Deoxycytosine & 25 & 2 & 317,856 \\
ABF & \aonsp-ABF-$\zeta$5k & Wild Type & Adenine & 1 & 4 &  39,732 \\
ABF & \dconsp-ABF-$\zeta$5k & Wild Type & Deoxycytosine & 1 & 4 & 39,732 \\
\hline
\end{tabular}
\end{center}
\end{table}

For systems of this size (approximately 325,000 atoms) and for the timescales of interest, it is necessary to use a HPC parallel MD Engine.  The project uses the parallel MD code NAMD and as also led to a grid-enabled version of NAMD developed by us to perform steered MD simulations including the capability to connect to distributed haptic devices (c.f. SC05 HPC Analytics Award). This work is currently funded by the UK's EPSRC (equivalent to the US NSF).  We use NAMD on 512 processors typically. We have used Ranger, Kraken and QueenBee extensively for earlier simulations for this project. The CPU hour requirements for these simulations are listed in Table~\ref{table:systems2}. For ABF simulations using a 325,000 atom model, 2~ns of simulation typically takes about 9.7 hours on Kraken. To complete an ABF sample, we estimate that the following number of nanoseconds are required - 4ns, 16ns, and 64ns for $\zeta$=5k, $\zeta$=20k, and $\zeta$=80k respectively. Therefore a single sample $\zeta$=5k simulations will require 9,933 CPU hours, and a 4 sample free energy profile will hence require 39,732 CPU hours. A single sample $\zeta$=20k simulations will require 39,732 CPU hours, and a 4 sample free energy profile will hence require 158,928 CPU hours. A single sample $\zeta$=80k simulations will require 158,928 CPU hours, and a 2 sample free energy profile will hence require 317,856 CPU hours. The total requirement for the remainder of this project is therefore 1,112,496 CPU hours. About 1.2M SUs more are required to finish sampling these remaining systems.  Therefore we ask for 1.2M to be allocated to us for use on this project. To be used primarily on Kraken.

\section*{Project 2: Computational Study of non-coding Functional RNAs: Folding Dynamics and Binding mechanism of Riboswitches}

Continuing our efforts toward a complete understanding of the molecular mechanisms of the SAM-I riboswitch that displays one of remarkable examples of non-coding RNA gene regulatory systems, we plan to carry out and advance our research of the riboswitch RNAs in the cycle of this allocation period.  Our research goals during coming years can be understood readily with the pipeline illustrated in Fig.~\ref{fig:ribo-pipeline}. 

As illustrated by the pipeline, we aim the holistic approach for understanding of folding dynamics of ribsowtich RNAs and closely related RNA-ligand binding affinity calculations, which is contrast to our strategy of the previous years mostly employing straightforward atomistic Molecular Dynamics simulations.  The key idea is to combine multiple computational approaches that differ in its physical principles but taken together we have much better understanding of the complex biological process carried out by riboswitch RNAs.  As a matter of fact, the primary physics principle behind this pipeline-based approach is the energy landscape perspective that interprets the folding of a RNA as statistical treatment of ensemble of structures evolving the energy landscape~\cite{onuchic1997}. The entire pipeline comprises three levels; the first layer represents the Boltzmann Ensemble sampling step of RNA secondary structures, the second layer is about 3D modeling from 2D structure information, and the third layer carries out the conformational sampling with atomistic MD simulations, respectively.  In our pipeline, the entire folding configuration space is efficiently explored by RNA secondary structure sampling and successively atomistic MD simulations explore the relevant basins of attraction starting from the configurations sampled.  Our work that demonstrated the sampling of RNA secondary structures satisfying the Boltzmann ensemble with scalable HPC resources was presented in "Emerging Computational Methodologies in Life Science"~\cite{ecmls10}. 

\begin{figure}
\begin{center}
  \subfigure[]{\includegraphics[scale=0.33]{flowchart-pipeline}}
  \subfigure[]{\includegraphics[scale=0.55]{ss-schema}}
\end{center}
\caption{(a) The pipeline for riboswitch structure prediction and binding affinity estimation (b) Schematic of the secondary structure change displayed by the SAM-I riboswitch (The left figure represents the OFF state resulted by the SAM-binding and the right figure represents the ON state}
\label{fig:ribo-pipeline}
\end{figure}

Therefore, the coming year allocation will be used for carrying out these different simulation strategies separately and later obtained outcomes are combined and used for another round of pipeline-based calculations.  Considering the use of various computational approaches, our strategy is only feasible via Teragrid like federated, distributed and massively parallel resources.  

A brief introduction for the SAM-I riboswitch is as follows.  Here, also we present further details we want to examine with the coming TRAC award.  Riboswitches are regulatory RNAs that control the expression of downstream genes. Small metabolite molecules, such as amino acids, nucleotides, coenzymes etc., can bind to riboswitches as effectors in vivo~\cite{mandal}.  In our recent research efforts, the S-box riboswitch (also called SAM-I riboswitch), one member of the riboswitch family that regulates genes related to the metabolism of sulfur and methionine, has been extensively investigated with atomistic simulations.  This riboswitch choose alternative conformation depending on binding of a SAM .  When S-adenosylmethionine (SAM) is bound, the aptamer domain forms anti-anti-terminator (AAT) conformation, which turns off the downstream genes by forming the terminator (T). Otherwise, the anti-terminator (AT) is formed prohibiting the T element formation for continuing transcription process (see Fig.~\ref{fig:ribo-pipeline}(b)~\cite{brooke}.  Although the structure of the SAM-I riboswitch in the anti-anti-terminator (AAT) conformation has been solved via X-ray crystallography, it is just a static view of how SAM binds to the s-box.  Using extensive all-atom simulations, we became to propose a novel binding mechanism of the SAM-I riboswitch with a SAM in which the role of entropic barrier for the AAT formation as well as the role of $Mg^2+$ specifically bound in the tertiary core structure.   Also, like the previous years, we collaborate with Prof. Fareed Aboul-ela, and we expect the results from our computational calculations provide details that are typically inaccessible to experiments; while at the same time providing the opportunity for our simulation results will be validated using biochemical and biophysical experiments (see Fig.~\ref{fig:ribo-pipeline}(a)).


\begin{figure}
\begin{center}
  \subfigure[]{\includegraphics[scale=0.33]{el}}
  \subfigure[]{\includegraphics[scale=0.4]{pipeline}}
\end{center}
\caption{(a) Illustration of structure sampling in configuration space.  (b) Schematic of a workflow for sampling and analysis of RNA secondary structures obtained with the Boltzmann-weighted sampling}
\label{fig:folding energy landscape}
\end{figure}


As for simulation costs, major proportion of our allocation will be used for atomistic MD simulations.  Continuing our efforts, the goal of this study is to probe the dynamic interactions between the SAM-I riboswitch and SAM at the nanoscale and to explore determinants for the specificity. In particular, we aim to extend our main strategy, combining MD and statistical analysis, for i) other constructs of SAM-I that differ from each other in potentially different secondary structures and tertiary interactions, ii) different sequences in SAM-I family, and iii) other SAM riboswiches (SAM-II and SAM-III) for which X-ray structures were recently reported and TPP ribosiwtches that we recently started to investigate.  To estimate binding affinity, we employed the Molecular Mechanics - Poisson Boltzmann Surface Area (MM-PBSA) approach and expect to develop novel theoretical developments because of the challenges arising from strong electrostatic interactions involved.  Sampling is a also very important task and for that purpose, replica exchange molecular dynamics (REMD) protocol is attempted.  Our recent development for the distributed adaptive REMD, that is described below, will help us to carry out simulations for these sizable systems.

Basically the similar protocol used for our recent studies~\cite{SAM-I-NAR2009, presentations} will be used.  We could start with a structure derived from the X-ray crystal structures of the AAT conformation of SAM-I riboswitch (PDB: 2GIS, 3GX2, 3GX3, 3GX5, 3GX6, 3GX7)~\cite{montange} or configurations generated by a process via two layers of the pipeline. In the simulation of the SAM free riboswitch, SAM is directly removed from the x-ray crystal structure and replaced with solvent water. The amber99bsc0 correction force field is used here~\cite{alberto}. Parameters for SAM are from the Generalized Amber Force Field (GAFF) and missing parameters are calculated using ANTECHAMBER~\cite{wang}. Positions of added hydrogens are guessed using PSFGEN within NAMD 2.6. Then the RNA molecules are solvated in a cubic solvent box of TIP3P waters with a 1.6 nm padding in all directions. Sodium and magnesium ions are distributed around the RNA molecules and neutralize charge of the system. The total number of atoms in the system is 56,000. Energy minimizations are carried out on all of the systems to remove bad contacts. Starting from 0 K, the temperature is raised 10 K for every 10,000 steps and is held constant after reaching the desired temperature (310 K) using temperature reassignment. MD simulations are performed in the NPT ensemble with the pressure maintained using the Langevin piston method with a period of 100 fs and decay times of 50 fs. The time step is 2fs for both equilibration and production phase. Bond lengths between hydrogens and heavy atoms are constrained using SHAKE. The long-range electrostatics is treated with the Particle Mesh Ewald (PME) method with a cutoff distance 12A.  All MD simulations are carried out by using a parallel version of NAMD 2.6.  VMD, wordom~\cite{moe} and homemade scripts are employed to analyze the trajectories. All snapshots of structural images are made using VMD.

\begin{figure}
\begin{center}
  \includegraphics[scale=0.4]{mm-pbsa-mg}
   \caption{MM-PBSA estimation on ${Mg^{2+}}$ binding with a SAM-I riboswitch RNA}
\end{center}
\label{fig:mm-pbsa-mg-table}
\end{figure}


The binding affinity calculation is carried out to employ the MM-PBSA protocol.  This protocol will be used for ligand binding of the metabolites, SAM, or TPP or other related ligands and also for cation binding of ${Mg^{2+}}$ that has been evidenced as crucial for function of catalytic RNAs but remains as elusive for detailed roles in folding dynamics.  Our preliminary results for cation binding are presented in Fig~\ref{fig:mm-pbsa-mg-table}.  


In our pipeline, the Bolzmann ensemble of RNA secondary structures are considered as the primary strategy to explore the energy landscape efficiently.  Our calculation for that purpose employs SFold pacakge\cite{ding2006}. The sampling of structures with an input sequence of riboswitches is not computationally intensive, but requiring a number of tasks depending on the biological questions.  For example, if we want to extract information by comparing riboswitches identified in RFam, at this moment we have to deal with 2092 SAM-I riboswitches together.  Also, after sampling, the anayses are intrinsically many task computing (MTC) tasks.   As we have shown in ECMLS10 paper\cite{ecmls10}, for this end, we built an efficient distributed computing environment, Adapative Distributed Application Management System (ADAMS).  ADAMS allows us to submit and manage many tasks with HPC systems with Teragrid that constitute the calculation we aim.  Therefore, based on our development, we continue to simulate the interesting systems and the results will be used for the further MD simulations or 3D modeling.
One notable challenge we want to try is to implement GPU accelerated RNA secondary structure prediction and sampling.  We will test our implementation with Lincorn cluster and our request includes computational time with the cluster machine. 




%\begin{figure}
%\begin{center}
%  \subfigure[]{\includegraphics[scale=0.60]{ss-schema}} \hspace{0.05in}
 % \subfigure[]{\includegraphics[scale=0.40]{ligand-atom2}}%
%\end{center}
%\caption{(a) Schematic of the secondary structures of s-box riboswitch with SAM bound (left; in the AAT state) and without SAM 
%(right; in the AT state), (b) predicted ligand-SAM interactions with s-box}
%\end{figure}







%All simulations are performed using NAMD 2.6 on LSU (Tezpur) and LONI (Queenbee) Linux clusters. 

%\begin{figure}
%\begin{center}
%   \subfigure[]{\includegraphics[scale=0.42]{rmsd}} \newline
 %  \subfigure[]{\includegraphics[scale=0.49]{RMSD_residue}}
%\end{center}
%\caption{RMSD of overall s-box and binding pocket only in SAM bound and WOSAM (short for the 
%trajectory of SAM free s-box riboswitch) trajectories  with reference to the crystal structure; (b) Root mean square fluctuation (RMSF) and B-factor of each residue of s-box riboswitch from MD trajectories}
%\end{figure}

%\begin{figure}
%\begin{center}
%   \subfigure[]{\includegraphics[scale=0.45]{cluster_2D}}
%   \subfigure[]{\includegraphics[scale=0.35]{cluster_1D}} \end{center} \caption{Clustering and Principal Component Analysis (PCA) point towards a chopstick-like motion involving P1 and P3 helices in the absence of SAM.  (a) Projections of snapshots of the SAM free trajectory are plotted against the first two principal components and color coded according to a k=3 k-means clustering: cluster 0: magenta, cluster 1, green, cluster 2, magenta. Representative snapshots from each cluster are also shown. This plot indicates that snapshots can be broadly clustered into two groups (cluster 1 and cluster 3) with cluster 2 representing a group with characteristics similar to those of cluster 3. The projection along PC1 broadly separates the clusters, while projection along PC2 completes the separation between clusters 1 and 3. Structures of representative snapshots indicate that clusters are distinguished by a dramatic change in relative position of P1 and P3. (b) From top to bottom: The time evolution of the first principle component of the SAM free trajectory. RMSD for each snapshot in the SAM free relative to the representative snapshots for cluster 1 (cyan curve) and for cluster 3 (magenta curve). The distance between the Center of Mass (COM) of P1 and P3 for the SAM free trajectory (blue) and for the SAM bound trajectory (red).  During the first half of the SAM free trajectory, P1 and P3 helices move apart (clusters 0 and 2), then they move back together during the second half of the trajectory (cluster 1).}

%\end{figure}

%Our initial achievements are illustrated in Figure 5 and Figure 6.  Our results suggest that the presence of SAM in the binding pocket is critical to form P1 helix overcoming the entropic cost for bringing two distal strands in proximity.  The essential dynamics found with the SAM-free trajectory are shown in Figure 6 with clustering results, indicating the characteristic long time dynamics in the SAM-free system. 




%\begin{figure}
%\begin{center}
%  \includegraphics[scale=0.660]{56k_scaling-2} \caption{Wall-clock times taken (in second) for each step at different processor counts. The measurement was done with Queen Bee }
%\end{center}
%\end{figure}


\begin{table}[h]
\begin{center}
  \caption{Riboswitch simulations and expected computing resources. \jhanote{Please update} }
\label{table:systems}
\begin{tabular}{| c | c | c | c |}
\hline
Type of Calculation &   Method or Package  &    HPC resources to be used & SUs required \\
\hline \hline
RNA secondary structure &
Sfold/RNAfold& Ranger/QB/Abe & 100K\\ 
sampling and analysis  & /our developed program  & & \\  \hline
Atomistic MD simulation & NAMD  & Ranger/QB/Abe/Kraken & 850K \\  
MM-PBSA        & AMBER    &  &  \\ \hline
RNA secondary structure & In -house CUDA programs & Lincorn & 50K \\
 sampling and analysis &   
&  &  \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection*{Requested Computational Resources}

As for atomistic MD simulations, according to our benchmark with Queen Bee, when using 32 cores, the time taken per step is approximately 0.06s; thus the wall clock time required to complete 1ns is .34 day; in other words for a 56K system, 1 ns simulations require $\approx$ 300 CPU hours.  Thus each 100 ns simulation requires approximately 30,000 CPU hrs.  Our simulations also suggest more than 300 ns trajectory generate meaningful conformational dynamics. Therefore, without additional MM-PBSA calculations, we expect to simulate about 90 trajectories of that time scale with 850,000 SUs (See {\url{http://staging.teragrid.org/userinfo/aus/namd_benchmark.php}} ).  As for RNA secondary structure calculations, our test for 2092 sequences obtained from Rfam takes 6 CPU hours with 32 cores for Boltzmann Enesmble sampling.  And, analyses require 10-100 times of sampling time.  Therefore, we expect to consume 100 K SU for 5 - 50 different tasks. Finally, we request 50 K with Lincorn cluster only for testing purpose of our GPU implementation.  



\section*{Project 3: Atomistic Simulations of Physiological Systems}

\subsection*{Project 3a: Towards patient specific HIV therapy}
\input{subproject2-hiv/hiv.tex}

\subsection*{Project 3b: Predicting the affinity of the EGFR kinase domain for drug inhibitors of lung cancer}
\input{subproject3-egfr/egfr.tex}


\section*{Project 4: Large-scale molecular dynamics simulations of layered bio-mineral composites}
 \input{subproject1-bioclays/clay.tex}


\section*{Project 5: Expeditions in Distributed Computing using SAGA}

\jhanote{Yaakoub-- Please take a first shot. Include resource request for LINCOLN. Please also update
with some details of the TG10 paper submission.}

Advances in Grid applications have simply not kept pace with advances in other aspects of 
distributed CyberInfrastructure, such as Grid middleware -- whether measured by the number of 
existing applications that can easily utilize the many advanced features offered by distributed 
infrastructure or measured by the number of novel applications capable of using the 
infrastructure. A key impediment in the accelerated development and deployment of Grid 
applications is the scarcity of high-level application programming abstractions that bridge the 
divide between the needs of Grid applications and the capabilities offered by middleware.  Much 
Grid development has focused on the support for legacy parallel and cluster application codes 
as a way of ensuring scientific relevance.  The benefit of the Grid paradigm, however, will 
come from new application development that does not depend on the homogeneous and relatively 
static model of resource performance inherited from parallel or cluster legacies.  % The lack 
%of such application-level programming abstractions is compounded by the fact that there exist 
%incompatible and often changing Grid middleware systems in both research and production 
%environments.

To address these challenges and in particular to find a solution to the universal, apparently 
intractable problem of successfully Grid-enabling applications, several applications groups 
expressed the desire for a simple programmatic interface that is widely-adopted, widely-available 
and usable. The goal of such an interface would be to provide a ``grid counterpart to MPI'' (at 
least in impact if not in details) and that would supply developers with a simple, uniform, and 
standard programmatic interface with which to develop distributed applications.  Thanks to the 
efforts of many contributors, but in particular the PI's group, an initial specification of 
such a ``grid counterpart to MPI'' now exists -- the Simple API for Grid Applications 
(SAGA)~\cite{saga_url}. As of fall 2010, SAGA will be an Open Grid Forum (OGF) technical 
specification.


\subsection*{Project 5a: Developing and Deploying Applications using SAGA}

A wide range of applications have been developed using SAGA -- ranging from regular compute 
intensive applications but involving multiple resources ~\cite{saga_escience07, gmac, REMD-
PhilTranA2009}, applications with multiple components and possibly irregular runtime 
requirements~\cite{saga_loosely_coupled, teragrid08} as well data-intensive applications 
~\cite{saga_data_intensive, saga_grid_cloud} using programming abstractions such as 
MapReduce~\footnote{Implementation of MapReduce using SAGA is funded by Google} An initial 
prototype of a ``general pilot-job'' framework using SAGA that can be utilized for Replica-Exchange 
that enables the {\it trivial} utilisation of multiple distributed resources across TeraGrid has 
been developed. This is currently work in progress, but we anticipate sufficient progress to begin 
testing the framework using our 56K riboswitch model.~\cite{REMD-PhilTranA2009}. %A Brief schematic 
%of Distributed Adaptive Replica Exchange Molecular Dynamics is shown in Figure 8.
Our current framework implements the Generalized Pilot-Job feature with the BigJob abstraction 
built upon SAGA and utilization of the framework is the key component for successful massive REMD 
simulations for our project on riboswitch studies.

% \begin{figure} \begin{center} \includegraphics[scale=0.55]{DARE-MD} \end{center} \caption{Schematic of Distributed Adaptive Replica Exchange framework using the BigJob abstraction that is built upon SAGA. It is proposed that DARE-MD framework will ultimately become part of the GridChem Science Gateway (in which the PI and co-PI Kim are involved).} \label{fig:results} \end{figure}

\subsection*{Project 5b: Developing Autonomic Computing Frameworks for CO$_2$ Sequestration Studies using SAGA}

Global energy needs today present serious challenges: the increasing demand for energy must be met, 
however at the same time the emissions of greenhouse gases into the atmosphere must be reduced. 
Even as alternative energy sources continue to develop and gain popularity, the fact remains that 
fossil carbon resources will continue to be in heavy use (in both developing and industrialized 
countries) and consequently generate large volumes of carbon dioxide ~\cite{GeoRPT,Pawar}. The 
atmospheric impact of this greenhouse gas can be abated through capturing and sequestering 
significant fractions of the produced CO$_2$.

For long-term storage of large volumes of CO$_2$, porous subsurface geologic formations are ideal 
candidates: these are the same formations responsible for the existence of oil and gas reservoirs. 
Indeed much the technology behind carbon dioxide sequestration (including drilling, gas injection, 
reservoir management and of course reservoir simulation) stems from drilling, petroleum and 
reservoir engineering. Injecting CO$_2$ into an oil-gas reservoir can also lead to improved oil 
recovery by ``pushing out'' the oil and gas for production, this allows for reduced net cost 
through increased revenues from oil and gas production ~\cite{EORBook}.

% \begin{figure}
% \begin{center}
% \includegraphics[scale=0.33]{gmaps_bqp.jpg}
% \end{center}
% \caption{A snapshot of an application using batch-queue-prediction system to dynamically determine the best resource to spawn a sub-task to; the noteworthy point is that the entire decision process is at the application level -- the fact that the application has to spawn a job of requirements X is mapped to a resource requirements, BQP is used to determine the resource based upon optimal availability and then the application uses SAGA to spawn and launch the sub-task onto the chose resource. A paper demonstrating this feature working across the TeraGrid won the Performance Challenge Award at TeraGrid 2008 (Ref.~\cite{teragrid08})}
% \label{}
% \end{figure}

One of the major challenges in CO$_2$ sequestration is the characterization of reservoirs that are 
safe and secure, environmentally and geologically and are therefore promising candidates for CO$_2$ 
sequestration ~\cite{GeoRPT,Luigi,Pruess2004,Pawar}. To that end, some of our efforts are directed towards 
developing CyberInfrastructure tools, technologies and abstractions that facilitate large scale 
reservoir characterization and forecasting studies~\cite{gmac,Elkhamra2009,MSEScience,TG10yye00}.

Since the amount of information obtained directly from reservoirs is very small compared to the 
actual size of the reservoir, history matching techniques have been developed to match actual 
reservoir production with simulated reservoir production, therefore obtaining a more 
``satisfactory'' set of reservoir models. A promising approache to history matching is 
the use of Ensemble Kalman filters (EnKF) ~\cite{KalmanPaper, DO2007, LiEnKF07, DO2006,Burger98}
and their various extensions.

Of particular interest are the EnKF extensions that can handle multiple physical phenomena,
such as the geochemical interaction between CO$_2$, geologic formation and formation fluids.
These phenomena are important because they allow a deeper understanding
of the long term environmental effects of CO$_2$ sequestration. To perform the analysis
step of highly nonlinear, multi-physics simulations, more than one ensemble is used 
\cite{Zhang,White1987,Durlofsky1992,Ballin1993,Gilks96,vanLeeuwen2003}.

Models with different physics can be combined, either hierarchically or by pooling 
groups of models in computation of relevant gains. That is, different  columns (corresponding to  
vectors of sensitivities of a model parameter to all observation misfits) can be based on different 
ensembles. Consistency is preserved if the gain columns are applied to models or sub-ensembles,
if and only if the models and sub-ensembles were used in the gain calculation. Alternatively, one 
can ``cross''  the sub-ensembles gains \cite{Mitchell02,Anderson2007,Michalak2003}

Having multiple ensembles (co-ensembles or hierarchical ensembles) will naturally increase the
amount of SUs required. A single simulation spanning a 1 million grid cell domain and simulating
15 years of production, 15 years of production forecast and 15 years of CO$_2$ sequestration will
require approximately $445$ SUs on Ranger and $395$ on Kraken. A two ensemble study, each with 100 ensemble members, 
will therefore require $89,000$ SUs on Ranger and $79,000$ SUs on Kraken. We estimate we will run at least $6$ full production 
simulations: $3$ on Ranger ($267,000$ SUs)  and $3$ on Kraken ($237,000$). Additionally, we will need to run several dozen smaller simulations for testing, calibration and development on Ranger as it is a machine we are very familiar with. Therefore we would like to request $40000$ SUs on Ranger and $250,000$ SUs on Kraken.

We would also like to request $50,000$ additional SUs to continue to pursue our autonomic,
on-demand scientific computing research with hybrid Grid-Cloud systems. Ranger has been
used effectively for this purpose \cite{Cloud1,Cloud2}. Furthermore, we would like to
continue to develop our distributed computing infrastructure across TeraGrid resources,
and therefore request a $50,000$ SUs allocation across Abe, QueenBee and Lonestar.


% Ensemble Kalman filters are recursive filters that can be used to handle large, noisy data; the 
% data in this case would be the results and parameters from ensembles of reservoir models that are 
% sent through the filter to obtain the ``true state'' of the data. Since the reservoir model varies 
% from one ensemble to another, the run-time characteristics of the ensemble simulation are irregular 
% and hard to predict. Furthermore, at simulation times when real historical data is available, all 
% the data from the different ensembles at that simulation time must be compared to the actual 
% production data, before the simulations are allowed to proceed. This translates into a global 
% synchronization point for all ensemble members; hence performing large scale studies for complex 
% reservoirs in a reasonable amount of time would benefit greatly from the use of distributed, high 
% performance, high throughput and on-demand computing resources.




\begin{table}[!h]
\begin{center}
 \caption{Summary of allocation usage for a full CO$_2$ sequestration study of a depleted resevoir}
\begin{tabular}{| c | c | c|}
\hline
& On Ranger & On Kraken \\
\hline
SU cost per simulation for history matching& 445 SUs & 395 SUs\\
\hline
Typical number of cores per simulation & 128 & 128 \\
\hline
Typical duration with the above number of cores & 3:30 hours & 3:00 hours \\
\hline
Number of members per ensemble & 100 members & 100 members \\ 
\hline
Number of sub/co ensembles & 2 ensembles & 2 ensembles \\ 
\hline
Total number of simulations & 200 simulations & 200 simulations\\
\hline
Total SUs consumed for full studies & 89,000 SUs & 79,000 SUs\\
\hline
Total SUs consumed for $3$ full studies & 267,000 SUs & 237,000 SUs\\
\hline
Total SUs requested & 400,000 SUs & 250,000 SUs\\
\hline
\end{tabular}
\end{center}
\end{table}



% \begin{figure}
% \begin{center}
% \includegraphics*[scale=0.4,angle=0]{3StageKalmanFilter}
% \end{center}
% \caption{Schematic illustrating the variability between stages of a typical
%   ensemble Kalman filter based simulation. The end-to-end
%   application consists of several stages; in general at each stage the
%   number of models generated varies in size and duration. From References~\cite{teragrid08, gmac}}
% \label{fig:irregular_execution}
% \end{figure}



\begin{figure}
\begin{center}
\subfigure{\includegraphics[scale=0.4]{Figure7.png}}
\subfigure{\includegraphics[scale=0.25]{8replica_scenario_grid_condor_cloud.pdf}}
\end{center}
\caption{Demonstrating the effectiveness of Distributed Applications Developed  Using SAGA to Scale-Out. From Left to Right, Performance as measured by the time-to-completeness of a well-defined workload when using: (i) Ranger only (ii) Ranger (with BQP service) only, (iii) QueenBee only, (iii) Ranger and QueenBee, (iv) Ranger, QueenBee and Abe concurrently (v) Ranger, QueenBee 
  and Abe when using BQP service. From Ref.~\cite{gmac} }
\label{fig:results}
\end{figure}

\subsection*{Project 5c: Utilizing Application-level Interoperability across TeraGrid and DEISA}
As part of NSF funded HPCOPS Award (till June 2010), we are leading a project to utilize the aggregated computational power of the Federated Grids of DEISA and TeraGrid. The aim of the project is to (i) work towards an integrated infrastructure that supports application level-interoperability and, (ii) having created the infrastructure, apply it to the dual challenge of understanding the conformational changes and determining the free energy of biological systems. Specifically, the high-level aim of this project is to enable scientific applications to utilise the federated capabilities of the TeraGrid, DEISA and LONI systems, to enhance the understanding of HIV-1 enzymes and epidermal growth factor receptors (EGFR) implicated in lung cancer -- important science drivers in general. The aim of this project is to use several Replica-based and Replica-Exchange simulations for HIV-1 \& EGFR research, on multiple TeraGrid, LONI and DEISA resources, working concurrently towards the solution of a single problem instance -- the rapid computation of free-energies of binding with high-levels precision.

{\it Resource Requirements:} For Project 5, we require XXX,000 SUs. % This in turn is divided into a component for the Interoperability Project (500K) and the EnKF-based Sequestration Project (250K).
\jhanote{Provde break-up}.  The latter is, a somewhat more experimental component of this proposal, where we do not have clear estimates and benchmarking data. However, based upon our experience/publications alluded to in the previous paragraph, each "experiment" that we conduct, (i.e. application that we develop), requires a minimum of 50000 SUs. For example, running a reservoir simulation for a simulation time of two weeks (typical period of historical production data gathering) consumes roughly ten minutes on four cores.  With two hundred ensembles (typical number of ensembles), running for a simulation time of fifteen years will consume 48k SUs. This obviously is the medium range of simulations; a more detailed reservoir model will naturally consume more SUs per simulation, multiplied by a large number of ensembles and the SUs required to perform the history matching increases dramatically. To complete the forecast stage, we will need to run the same ensembles for an extended period of time (reservoir depletion/workover, enhanced oil recovery and CO$_2$ sequestration) which roughly would range between twenty to thirty years in simulation time, that is an additional 64k-96k SUs, placing the total in the range of 112k-144k SUs for a single complete reservoir study.

The TeraGrid-DEISA Interoperability project -- built upon validated and ready to run models, has already acquired significant resources (several million CPU hours) on DEISA machines; our request for 500K is to support initial production runs of the VPH (Virtual Physiological Human) models and test the infrastructure for Scale-Out tests -- intra-TeraGrid as well as TeraGrid-DEISA Grids, performed on the VPH Models.

% \subsection{Associated Technical Developments, Collaborations \& Other High
% Performance Computing Resources}

% We plan to release the next version of our grid middleware Application Hosting Environment (AHE) in the second half of 2010 \cite{zasada2009,coveney2007}, during the proposed TRAC allocation.  The AHE provides simple graphical and command line interfaces to run applications on resources provided by grid computing infrastructures in addition to local campus-based clusters, while hiding from the user details of the underlying middleware in use by the distinct resource providers.  We are engaged in integrating AHE with a metascheduling framework, based on a computational mechanism design, which is mediated by software agents, to efficiently allocate work between a set of resources based on cost minimization and run time optimization.  The patient-specific clinical simulation work proposed in this TRAC will be supported by our plans to integrate support for SPRUCE (a system developed at Argonne National Labs for urgent computing by allowing users to run emergency jobs) within the AHE client interface, so that jobs submitted with SPRUCE tokens use the mechanisms provided by SPRUCE to preempt the current workload on a machine.


% We also plan to introduce a mechanism to host workflows in the AHE as virtualized applications, composed by orchestrating the execution of other AHE hosted applications. The purpose of workflow management systems is to automate common time consuming tasks that the scientist carries out when performing \emph{in silico} studies. With the planned developments, we can interact with workflows involving simulation pre-processing, launch and post-processing via the AHE in the same way as with a single application. This work is being done in collaboration with developers of the GSEngine workflow management tool within the European Union's (EU) Virtual Physiological Human (VPH) project.


% Use of the AHE and software from the SAGA project has allowed our research team to inter-operate between high performance computing resources in the US, UK and the EU.  Additionally, within our grid middleware development activities, we are working on a new mechanism, termed as Audited Credential Delegation, to allow controlled access to a single grid certificate for a group of users, authenticated by their local institutional security credentials, facilitated by AHE. This approach is being developed to address issues related to the scalability of the current X.509 certificate-based authentication model, which relies on each individual user obtaining and managing his/her own grid certificates. This project is being funded by the UK EPSRC grant (EP/D051754/1) entitled ``User-Friendly Authentication and Authorisation for Grid Environments''.


\section*{Supporting Grants}
%\jhanote{Shantenu to update. (i) ExTECNI, (ii) BIPAS and others.} 






\subsection{Peer Reviewed Research:} The PI's research is supported by current NIH and NSF awarded.  PI-Jha is the co-PI of LSU's HPCOPS NSF-OCI 0710874 award.  


The PI leads Work Package 4 of the NSF Funded Cybertools Project (http://www.cybertools.org) (NSF Award NSF/LEQSF(2007-10)-CyberRII-01; Total Value \$12M) and the \$15M NIH award supporting the Louisiana Biomedical Research Network (LBRN). Project 1 is partially supported by the Biosensors work activity of WP-1 of Cybertools.

Integration of SAGA with applications is part of Cybertools and the PI also holds multiple peer-reviewed awards for the development and integration of SAGA.  The Interoperability Project~\cite{interop_url} is currently funded by an NSF HPCOPS award, and is being executed by the PI (Jha).  See PI's vitae for full grant listing.

Project 2 is funded by multiple Louisiana Board of Regents award and an LSU Faculty Award (PI Jha). Project 2 has been supported by a LSU Faculty Research Award to PI Jha, in conjunction with multiple awards to Fareed Aboul-ela (Experimental Collaborator). Project 2 has also benefitted from a LONI Distinguished Graduate Assistantship to Wei Huang (PhD Student co-supervised by PI Jha and experimental collaborator Aboul-ela) Project 2 forms the basis for a large NSF proposal under review, {\it IIS 1029810 Macromolecular Choreography: Computational methods to detect conserved dynamic properties in non-coding RNAs}.

In addition, the PI is the LSU-lead in the \$2M ExTENCI project that aims to further interoperability between TeraGrid and Open Science Grid.  Project 5b is supported by NSF OCI award - ExTENCI: Extending Science Through Enhanced National Cyberinfrastructure (total \$2M, LSU share \$0.2M; Project Officer Barry Schneider).  Project 5b is also supported by UCoMS project, Department of Energy and Louisiana Board of Regents award No. DE-FG02- 04ER46136 (PI Chris White, LSU -- collaborator of PI Jha and co-PI el-Khamra); it also forms the basis of OIA 1028948 Cyber Enabled Geomodel Inversion (under review at NSF).



\subsection{Completmentary Computing Allocations:} In addition to the requested TeraGrid allocation, we currently have an allocation of 4.04 million Allocation Units (AUs) on the UK's Cray XT4 supercomputing resource associated with the EPSRC grant (EP/F00521/1) entitled ``Large scale lattice-Boltzmann simulation of liquid crystals'' for materials science research. Within the EU VPH Virtual Community project, we annually receive an allocation of 2.0M CPU hours on the EU's DEISA grid for our work in the bio-medical sciences domain.  (This computational allocation has been awarded to Coveney (PI); award number EU FP7-ICT-2007-5.3 223920 funded by the European Union with a start date of 06/01/2008 and expiration date of 11/30/2012).  We have been successful in the DEISA Extreme Computing Initiative (DECI) calls successively for the past several years, most recently gaining 700,000 CPU hours for our computational science activities.  (The most recent DECI computational allocation was awarded to Coveney (PI); award number EU RI-222919 funded by the European Union with a start date of 01/01/2009 and expiration date of 01/31/2010).

% \section{Senior Personnel}
% \input{personnel.tex}

% Part of EnKF work is funded by the UCoMS project, Department of Energy and Louisiana
% Board of Regents award No. DE-FG02- 04ER46136


\section*{Summary}
% In summary, our request is for 1.25M SUs bound (non-roaming) to Ranger, 0.75M SUs bound to Kraken and a 750K roaming allocation over QB-ABE/Ranger/Kraken.  To state the obvious, if a primary aim of our work is to investigate the ability of distributed applications to {\it Scale-Out}, then it is imperative that there be an underlying resource allocation to support the work. We aim to develop frameworks such as Lazarus and Faust to support the ability of applications to {\it Scale-Out} in a manner that is independent of the specifics of the applications.  By extension, in order to scale-out to the TeraGrid and DEISA combined resources (one of the motivations for interoperability), we will also have to scale-out on the TeraGrid.  In general, there are instances where we have chosen to request non-roaming allocations but request more than one machine for the same project; this is to hedge against lengthy, but more challengingly, variable queue length (load) factors. However, the bulk of our resource request is non-roaming ($\approx$ 75\%).

\begin{table}[!h]
\begin{center}
  \caption{Resource distribution requests for different projects \jhanote{This will need revision and updating} \newline}
\label{table:systems}
\begin{tabular}{|c| c | c | }
\hline 
Project & Resource & Total Request \\ 
\hline
1 & Ranger  & TBD \\
1 & Kraken &  TBD  \\
\hline
2 & Ranger & TBD \\
2 & Kraken & TBD \\
\hline
3a & Abe-QB/Kraken/Ranger & TBD \\
3b & Abe-QB/Kraken/Ranger & TBD \\
\hline
4a & Abe-QB/Kraken/Ranger & TBDK \\
\hline
5a & Abe-QB/Kraken/Ranger & TBD \\
5b & Abe-QB/Kraken/Ranger & TBD \\
\hline
\end{tabular}
\end{center}
\end{table}


\bibliographystyle{unsrt}


%\include{combref}
%\include{yye00_refs2}

\bibliography{jha_loni_alloc_jul01,ucl_trac,yye00}
\end{document}

