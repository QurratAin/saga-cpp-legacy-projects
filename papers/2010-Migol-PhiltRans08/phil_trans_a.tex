%\documentclass[times, 10pt, twocolumn]{article} 
%\documentclass[conference,final]{IEEEtran}
     
\documentclass{rspublic}   

%\usepackage{latex8}
%\usepackage{times}

%\documentstyle[times,art10,twocolumn,latex8]{article}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
%\pagestyle{empty}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}    
\usepackage{multirow}    
\usepackage{listings}   
\usepackage{times}     
\usepackage{paralist}    
\usepackage{wrapfig}    
\usepackage[small,it]{caption}
\usepackage{multirow}
\usepackage{ifpdf}
                    
%Bibliography                     
\usepackage{natbib}   



\usepackage{listings}
\usepackage{keyval}  
\usepackage{color}
\definecolor{listinggray}{gray}{0.95}
\definecolor{darkgray}{gray}{0.7}
\definecolor{commentgreen}{rgb}{0, 0.4, 0}
\definecolor{darkblue}{rgb}{0, 0, 0.4}
\definecolor{middleblue}{rgb}{0, 0, 0.7}
\definecolor{darkred}{rgb}{0.4, 0, 0}
\definecolor{brown}{rgb}{0.5, 0.5, 0}

\lstdefinestyle{myListing}{
  frame=single,   
  backgroundcolor=\color{listinggray},  
  %float=t,
  language=C,       
  basicstyle=\ttfamily \footnotesize,
  breakautoindent=true,
  breaklines=true
  tabsize=2,
  captionpos=b,  
  aboveskip=0em,
  %numbers=left, 
  %numberstyle=\tiny
}      

\lstdefinestyle{myPythonListing}{
  frame=single,   
  backgroundcolor=\color{listinggray},  
  %float=t,
  language=Python,       
  basicstyle=\ttfamily \footnotesize,
  breakautoindent=true,
  breaklines=true
  tabsize=2,
  captionpos=b,  
  %numbers=left, 
  %numberstyle=\tiny
}


\title[Distributed Replica-Exchange Simulations]{Distributed
  Replica-Exchange Simulations}

% \title[Distributed Replica-Exchange Simulations]{ Distributed
%   Replica-Exchange Simulations on Production Environments using SAGA
%   and Migol}

% \title{Reliable Replica-Exchange Simulations of Biomolecular Systems
%   on Production Distributed Environments using SAGA-CPR and Migol}
% Computational Grids using SAGA-CPR and Migol}


\author[Luckow, Jha, Kim, Merzky, Schnor]{
  Andr\'e Luckow$^{1}$, Shantenu Jha$^{2,3,4}$, Joohyun Kim$^{2}$, Andre Merzky$^{2}$ and Bettina Schnor$^{1}$\\
  \small{\emph{$^{1}$Institute of Computer Science, Potsdam University, Germany}}\\
  \small{\emph{$^{2}$Center for Computation \& Technology, Louisiana State University, USA}}\\
  \small{\emph{$^{3}$Department of Computer Science, Louisiana State
      University, USA}}\\
  \small{\emph{$^{4}$e-Science Institute, Edinburgh, UK}}\\
}

%\date{}

\def\acknowledgementname{Acknowledgements}
\newenvironment{acknowledgement}%
{\section*{\acknowledgementname}%
\parindent=0pt%
}

\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\kimnote}[1]{ {\textcolor{green} { ***JK: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***AL: #1 }}}
\newcommand{\amnote}[1]{ {\textcolor{magenta} { ***AM: #1 }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\else
\newcommand{\kimnote}[1]{}
\newcommand{\alnote}[1]{}
\newcommand{\amnote}[1]{}
\newcommand{\jhanote}[1]{}
\fi

\begin{document} 


\maketitle    

\begin{abstract}{REMD, SAGA, Migol, Fault Tolerance}  
  There exists a class of scientific applications for which utilizing
  distributed resources is critical for reducing the
  time-to-solution. However, the ability to orchestrate many
  distributed jobs in a distributed environments is a major challenge.
  We discuss a specific class of applications -- Replica-Exchange
  simulations -- where utilizing as many (often heterogenous)
  distributed resources as possible, is critical for the effective
  solution of the scientific problem.  SAGA is a high-level
  programmatic abstraction layer that provides a standardised
  interface for the primary distributed functionality required for
  application development.

  \jhanote{In this paper, we describe the design, development and
    deployment of a unique framework for constructing fault-tolerant
    distributed simulations.}

  \jhanote{Less emphasis on the SAGA/Migol framework: The framework
    consists of two primary components -- SAGA and Migol, is scalable,
    general purpose and extensible.}

  \jhanote{I think this can go: We provide details of a newly
    developed functionality in SAGA -- the Checkpoint and Recovery
    API. Migol is an adaptive Grid middleware, which addresses the
    fault tolerance of Grid applications and services by providing the
    capability to recover applications from checkpoint files
    transparently.  In addition to describing the integration of
    SAGA-CPR with the Migol infrastructure,}

  \jhanote{We also outline our experiences with running a large scale,
    general-purpose, SAGA-CPR based Replica-Exchange application in a
    production distributed environment.}

\end{abstract}

\section{Introduction}
                           
There exist several types of applications, which are well suited
to distributed environments. Probably the best known and most powerful
example are those that involve an ensemble of decoupled tasks, which
we refer to as {\it pleasingly-distributed} applications; in spite of
its conceptual simplicity, many scientific problems based on parameter
sweeps and/or Monte Carlo simulations, can be solved using
infrastructure that supports this common application class. A slightly
more complicated and challenging class of distributed applications are
those that have a small level of coupling between individual
sub-tasks.  An interesting example of such applications are those
based on \emph{Replica-Exchange (RE)}~\citep{hansmann,Sugita:1999rm}
simulations.  Such applications can be used to understand important
physical phenomena -- ranging from protein folding dynamics to binding
affinity calculations required for computational drug discovery.

Distributed RE simulations must be able to orchestrate different
heterogeneous resources in a complex and dynamic environment.  Writing
such applications is a complex task for a myriad number of reasons,
not least of which is that distributed computing environments are
inherently prone to failures and thus
unreliable~\citep{schroeder,10.1109/E-SCIENCE.2006.93,DBLP:conf/grid/KhaliliHOSC06}.
Some applications can respond to such failures via redundant or
speculative computing.  However, redundant computing has its
limitations, especially when there is a level of heterogeneity and
coupling between tasks.  Speculative computing is still possible, but
its use to mitigate the consequence of distributed failures, leads to
a whole host of load-balancing and scheduling problems.

Partly due to some of these challenges, even though distributed RE
simulations are loosely-coupled, failures can become a major problem
for such applications.  In RE simulations, there is a need to
occasionally attempt an exchange between pairs of replicas; the
pairing of replicas is not a constant but is dynamically evolving.
However, once a pair of replicas has been established, a delay or loss
of one replica will stall the other replica.  Thus over time, due to
the fact that in principle, every replica will ultimately
interact/exchange with another replica, a single failure, if left
uncorrected can ultimately cause the simulation to encounter an
exponentially increasing slow-down.  In the worst case, a single
failing task can render the entire computation worthless.  Thus, it is
essential to provide support for fault tolerance at some level.
\emph{Migol}~\citep{schnorLuckow08} provides fault-tolerance at the
middleware level by supporting the transparent starting, monitoring
and recovery of tasks.

\jhanote{there is repetition here. need to refine} \amnote{the last
  sentence reads strange to me.  What is the 'thus' referring to?  That
  migol provides fault tolerance?}      
\alnote{I reordered the sentences - should be ok now.}

Having motivated the need for a fault-tolerant framework for
application development, in particular for distributed RE simulations,
this paper describes the design of the SAGA~\citep{saga_gfd90}
Checkpoint\,\&\,Recovery package (CPR), and its implementation using
the Migol adaptor. SAGA-CPR~\citep{saga-cpr} represents the first
extension to the core SAGA API specification, and is thus a validation
of the extensibility of the specification.  Additionally Migol can be
regarded as a reference implementation of the GridCPR
architecture~\citep{ogf_cpr_arch}.  We demonstrate this by presenting
the seamless integration of CPR concepts and abstractions into the
SAGA framework; the resulting system is able to provide an easy to
use, high-level programming abstractions for Grid enabled, fault
tolerant applications.  The pairing of Migol and SAGA-CPR thus
provides a natural, portable and a very generic solution to the
problem of a programmatic interface for designing fault-tolerant
applications.  To highlight this and that it is critical that all
aspects of the distributed application life-cycle -- design,
development and deployment -- are made easier, we will discuss our
experiences in developing and deploying a RE application in a
production environment.


Before outlining the structure of the paper, we highlight the main
advantages of our approach: First and foremost it is a general purpose
framework that can be used over a wide-range of distributed production
environments, such as the TeraGrid, UK's NGS, DEISA, etc., as opposed
to WISDOM~\citep{wisdom}, that is essentially confined to
gLITE/EGEE. Secondly, our approach can scale to use resources of
different sizes, as opposed to {\it Folding@home}~\citep{folding} which
is based upon BOINC, and is thus inherently limited in the size of
physical problems that can be solved effectively.  \alnote{Should we
  add citations to Folding@home and wisdom here?}  Thirdly, our
framework is extensible: it can be used to implement many other
application in addition to those based upon RE
simulations~\citep{escience07}. The power to do so arises
from simple design decisions: the use of standard interfaces on the
one hand, and the use of appropriate programmatic and system
abstractions that allow users to do what they can do best
(i.e. provide the simulation and orchestration logic), whilst ensuring
that middleware used provides required services (such as checkpoint
management, application monitoring and recovery) seamlessly and
effectively from the application developers perspective.

The remainder of the paper is structured as follows: In the next
section we provide the basic ideas behind RE and specifically
Replica-Exchange using Molecular Dynamics simulations.  We then
discuss the fault-tolerant Migol framework. In section 4 we present
the SAGA-CPR package and show its relation to Migol. In section 5, we
discuss how SAGA-CPR and Migol are used to implement a fault-tolerant,
distributed framework for RE.  In the subsequent sections we discuss
our experience in implementing REMD and performance figures. We
conclude by providing a detailed analysis of related work, which will
highlight the truly unique features of our implementation.

\section{Replica-Exchange Molecular Dynamics}
\alnote{I restructured this section: 1) general introduction to MD
  problems and further motivation for RE algorithm 2)
  Structure/Classification of REMD algorithms}  In Molecular
Dynamics (MD) approaches, a sufficient sampling of configurations is
an important requirement for connecting atomistic results to
macroscopic or thermodynamic quantities available from experiments.
However, even with the most powerful computing resources at the
moment, straight-forward MD simulations are unable to reach the
relevant time-scales required to study conformational changes and
searches. This is part due to the inherent limitations in the MD
algorithm -- a global synchronization is required at the end of each
time step.  This provides an important motivation for research into
finding ways to accelerate sampling and enhance ``effective''
time-scales studied. Generalized ensemble approaches -- of which
Replica-Exchange Molecular Dynamics (REMD)~\citep{Sugita:1999rm} are a
prominent example -- represent an important and promising attempt to
overcome the general limitations of insufficient time-scales, as well
as specific limitations of inadequate conformational sampling arising
from kinetic trappings.  The fact that one single long-running
simulation can be substituted for an ensemble of shorter-running
simulations, make these ideal candidates for distributed environments.

Replica-Exchange (RE) simulations can be thought of as consisting of
two distinct components: the underlying simulation engine/mechanism
used for each replica, and the coupling-mechanism between the
individual replicas.  It is important to note that RE is in fact a
class of algorithms and not a specific
algorithm~\citep{dpa-paper}; for example, there can be not only
different simulation strategies -- such as the Monte Carlo and the
described MD approach -- but also multiple levels-of-coupling.  


The degree and frequency of coupling and exchange can be either
regular~\citep{hansmann,Sugita:1999rm}, or
irregular~\citep{SPdynamics,pande_bj03}. An example of the latter --
parallel replica dynamics as implemented in Folding@home, involves
coordination between replicas only when an ``event'' occurs.  In
contrast, for regular RE applications, attempts to exchange states
between certain pairs occur at fixed intervals. A major challenge
common to both types however, is the design, development and
deployment of a general purpose RE framework for distributed
environments.

\section{Migol: A Fault-Tolerant Service Framework}     
\label{sec:migol}

\begin{figure}[h]
 \centering
 \includegraphics[width=0.9\textwidth]{migol_architecture}
 \caption{\footnotesize \bf Migol Architecture: Migol provides
   services for supporting the fault tolerance of Grid
   applications. Applications that are managed by Migol are
   transparently monitored and recovered in case of failures. }
 \label{fig:migol_architecture} 
\end{figure}           


Migol guarantees the correct and reliable exe\-cution of applications
or tasks even in the presence of failures. The framework is based on
the Globus Toolkit 4.  Figure~\ref{fig:migol_architecture} shows the
current Migol architecture and the interactions between the different
services.

The fundamental metadata model of Migol is the \emph{Grid Service
  Object (GSO)} schema, which defines a generic and extensible
information model for describing Grid applications.
A GSO stores all relevant information about an application: resource
requirements, the location of binaries and checkpoint files, global
unique identifier (GUID), etc.

Grid Service Objects containing the metadata of all running
applications are stored in the {\em Application Information Service
  (AIS)}.
To avoid a single point of failure, the AIS is replicated using a ring-based
replication protocol, which ensures the data consistency
(see~\citep{Luckow:2008ys} for details).

Applications are started via the {\em Job Broker Service (JBS)}
(step 1, Figure~\ref{fig:migol_architecture}). Before job
submission, the JBS must register the GSO of the application at the
AIS (step 2).  Resource discovery is performed through WS
MDS~\citep{schopf06}  (step 3), which aggregates data of different services,
e.\,g.\ the Network Weather Service (NWS)~\citep{NWS99}.
Available resources are matched by the JBS according to the
application requirements. For execution of the application on
Grid resources, the JBS relies on a custom module, the Advance
Reservation Service, which is also capable of supporting resource reservation
on top of GRAM.

Migol provides several mechanisms for supporting the fault tolerance of 
distributed applications.

To detect failures, the \emph{Monitoring and Restart Service (MRS)}
periodically monitors all services registered at the AIS (step 6).  In
case the MRS discovers an inactive application, it initiates a restart
respectively a migration using the JBS (step 7).

For recovery, Migol relies on application-level checkpointing, i.\,e.\
applications have to be written to accommodate checkpointing and
restart.  The checkpoint metadata maintained by the AIS must be
updated by the application each time a new checkpoint is witten (step
5).

Migol can be considered as GridCPR reference implementation for which
the SAGA-CPR package provides a well-defined, application-level
interface.  In addition, SAGA provides various other useful
abstractions, such as the File and RPC API, which ease the development
of distributed applications. The following sections describes how the
functionality of Migol can be seamlessly integrated with SAGA-CPR.

\section{SAGA Checkpoint Recovery API}

The SAGA API specification~\citep{saga_gfd90} defines the core of
the SAGA API as well as the mechanisms to extend the core API via
additional functional packages.  The SAGA-CPR API
Package~\citep{saga_cpr_draft} is such an extension, which is currently
an OGF working draft.  The SAGA-CPR package provides a clean
abstraction for starting, monitoring and recovering of
checkpoint-restartable jobs.  To support these use cases, applications
can register checkpoints and job metadata with the infrastructure
using this API.

For the management of CPR Grid jobs, SAGA defines the
\texttt{cpr::job} and \texttt{cpr::service} class.
%(see Listing~\ref{lst:saga_job_start}). 
The handling of CPR jobs is similar to regular jobs: A job is defined
by a job description. In contrast however, CPR jobs require
two job descriptions -- one for starting and one for
\textit{re}starting the application.
In addition, CPR jobs can be queried for
checkpoint metadata, and explicitly checkpointed or recovered.
Listing~\ref{lst:saga_init_service} shows how applications can connect
to the CPR infrastructure. During instantiation of the
\texttt{saga::cpr::service} object the adaptor is able to register
itself with the CPR backend. This step can be used for example, to
register a monitoring endpoint. Using the \texttt{saga::cpr::self}
object an application can obtain metadata about the current job from
the application.

\begin{lstlisting}[style=myListing, caption={\footnotesize \bf
SAGA-CPR: Initialize Migol Session\newline\newline}, float=t,
label={lst:saga_init_service}]
saga::cpr::service service (saga::url 
    ("migol://flotta.haiti.cs.uni-potsdam.de:8443/wsrf/services/migol/AIS-JGroups"));
saga::cpr::self = service.get_self ();
\end{lstlisting}

\begin{figure}[ht]
    \centering
          \includegraphics[width=0.7\textwidth]{cpr-statemodel.pdf}
    \caption{\footnotesize \bf SAGA-CPR State Model: The CPR state model introduces the recovering state. This state indicates that an automatic recovery attempt is  occurring.}
    \label{fig:cpr-statemodel}
\end{figure}
 

Further, applications can use a checkpointing API to update checkpoint
metadata at the backend. The SAGA-CPR API allows the hierarchical
organization of checkpoint files.

Listing~\ref{lst:saga_chkpt_reg} illustrates 
the registration of a checkpoint file with the CPR framework.     
\begin{lstlisting}[style=myListing, caption={\footnotesize \bf
SAGA-CPR: Checkpoint Registration}, float=t,
label={lst:saga_chkpt_reg}]
saga::cpr::checkpoint remd_chkpt("remd_chkpt");
remd_checkpoint.add_file (saga::url 
  ("gsiftp://qb.loni.org/work/remd/chkpt.dat"));
\end{lstlisting}

CPR jobs are subject to an extended state model. An application can
query or subscribe to a job's state via the \texttt{cpr::job} object.
Figure~\ref{fig:cpr-statemodel} summarizes the CPR state model. The
CPR model extends the SAGA job state model~\citep{saga_gfd90} by the
new state \texttt{recovering}. This state is used to indicate that the
infrastructure is currently trying to restart a job.  If this recovery
attempt fails, the state of the application is permanently set to
\texttt{failed}.  The application must then deploy an
application-level recovery schema in order to continue the execution.


The Migol adaptor provides a compliant SAGA-CPR stack for the C++
reference implementation~\citep{Kaiser:2006qp}.  A major building block
is the application-level monitoring mechanism used to detect failures.
To support the monitoring of arbitrary SAGA applications, a monitoring
Web service is started by the Migol adaptor with the initialization of
a \texttt{cpr::service} object. This service is implemented using the
gSoap HTTP server~\citep{gsoap}.  The Monitoring Restart Service will
periodically send heartbeat messages to the application's monitoring
service.

Another critical aspect is the management of the application's
metadata. To ensure the recoverability of an application, metadata
such as the job description and information about written checkpoint
files must be available even if the application failed. The Migol
adaptor relies on the AIS as the metadata backend. Every job is
associated with a global unique identifier (GUID).

All metadata belonging to an application can be stored or queried with
reference to the GUID.  The following information is propagated to the
AIS by the Migol adaptor:

\begin{compactitem}
\item The job description for starting and restarting of an
  application is mapped to the resource, service, and file profile of
  the Grid Service Object schema used by Migol. The registration of
  these information is done during the \texttt{create\_job()}
  operation.
\item The SAGA job state is mapped to the more comprehensive Migol
  model, which introduces additional states such as \emph{migrating}
  or \emph{pending}. All state transitions are directly propagated to
  the AIS. State queries, e.\,g.\ using the \texttt{get\_state()}
  operation, are always conducted against the AIS. The Migol state is
  accessible via the \texttt{state\_detail} metric of the job object.
\item After startup of the application, the monitoring endpoint,
  i.\,e.\ the URL of the Web service, is updated.
\item During runtime, metadata about written checkpoint are 
  updated via the \texttt{cpr::checkpoint} object.
\end{compactitem}   
To ensure the availability of these information on failures,
the AIS is actively replicated across multiple Grid nodes.

\section{Implementing Replica-Exchange Using SAGA}

\alnote{I removed parts of the SAGA introduction due to space
  reasons.}  \jhanote{I undid your removal, only because we need to
  emphasis the broad scope with SAGA} The \emph{Simple API for Grid
  Applications (SAGA)}~\citep{saga_gfd90} provides an easy-to-use
standardised API for developing a broad range of distributed
applications, including, but not limited to loosely-coupled data
and/or pleasingly-distributed applications.

In particular, SAGA offers an API for the management of
checkpoint-recoverable jobs and file transfers that can be used over
heterogenous distributed environment. Thus, SAGA is ideally suited for
encoding the orchestration logic of RE simulations.

\jhanote{Joohyun: Put in a paragraph or two of the main points of the
  how it is done here; maybe even discussing the ideas related to
  checkpointing, new jobs, MPI issues discussed etc. etc. Try to
  establish what is specific to the SAGA way of doing things from what
  is a general distributed computing problem/issue}

\begin{figure}[t]
      \centering
          \includegraphics[width=0.7\textwidth]{REMDgManager-architecture.pdf}
          \caption{\footnotesize \bf Components of REMD-Manager: The
            main part of the framework is the replica manager. The
            manager orchestrates a set of replica processes using the
            SAGA/CPR API. The Migol infrastructure ensures that the
            REMD-Manager and all replica processes are monitored and
            recovered if necessary.}
      \label{fig:REMD-Manager-architecture}
\end{figure}

As illustrated in Figure~\ref{fig:REMD-Manager-architecture}, the
proposed framework comprises of three components: The task manager,
also referred to as \emph{REMD-Manager}, is deployed on the user's
desktop and provides the user interface to the overall REMD run. The
second component is the Migol infrastructure that submits, monitors,
and if required, recovers replica simulations.  The last element is
the task agent, the \emph{NAMD-Launcher}, that resides on the High
Performance machines where MD simulations are carried out. The
NAMD-Launcher is triggered by the Grid job and is responsible for
spawning and monitoring the MD run. NAMD~\citep{Phillips:2005gd}, a
highly scalable, parallel MD code, is used to carry out the MD
simulation corresponding to each replica run. It is important to
mention that any other MD code could be used just as simply and
effectively.

The \emph{REMD-Manager} is at the core of the framework; it
orchestrates all replicas, i.\,e.\ the parameterization of replica
tasks, file staging, job spawning and the conduction of the
replica-exchange itself. This component heavily relies on the SAGA
File and CPR API as well as the Python bindings for the implementation
of the RE logic\footnote{The complete REMD-Manager code can be found
  at
  https://svn.cct.lsu.edu/repos/saga-projects/applications/REMDgManager/}.
                                  
\alnote{I am really not sure about this code snippet. It somehow
  represents the logics - however I am not sure whether it is worth
  the space.} \jhanote{I agree, that as written it is not very
  effective. Also, since it does not show the exchange and thus the
  reader does not see how ``distributed'' and ``local'' exchanges are
  handled equally simply. Can we i) use psuedo-code to highlight this?
  ii) mention where the reader can get the actually code?. We should
  definitely do point ii) and possibly point i)}

Depending on the number of configured processes $n$, the REMD-Manager
starts initially $\frac{n}{2}$ pairs of replicas.  Each replica
process is assigned a different temperature. Before launching a job
the REMD-Manager ensures that all required input files are transfered
to the respective resource. For this purpose, the SAGA File API and
the GridFTP adaptor (step 1 in
Figure~\ref{fig:REMD-Manager-architecture}) are used.  The replica job
is then submitted to the Grid resource using the CPR API and
Migol/GRAM (step 2a-2c). Migol ensures that the the job description of
each replica is stored within the Migol backend to ensure a later
recovery. Globus GRAM is used to start the application.
                                               
To integrate NAMD with the SAGA/Migol infrastructure a SAGA based task
agent -- {\it NAMD-Launcher}, is used.  This agent is responsible for
updating the metadata of the application, i.\,e.\ the state,
monitoring endpoint and new checkpoint URLs, at the
Migol backend.  The agent then launches the actual NAMD job using
MPI. During the entire runtime the replica process is monitored by
Migol using the monitoring endpoint of the NAMD launcher. This
NAMD-Launcher enables the flexible orchestration of multiple NAMD jobs
through the REMD-Manager without modification of the NAMD source
itself.

When paired replicas reach a pre-determined states (eg., after a fixed
number of steps), the decision as to whether to exchange
paired-replicas is determined by the Metropolis scheme. If successful,
parameters such as the temperature, are swapped. Both jobs are then
relaunched using the mechanisms described above. Often the Metropolis
scheme returns a negative result, and an exchange is not carried out;
thus it is difficult to respond to an possible exchange speculatively.

In summary, SAGA allows the simple decoupling of the REMD application
and orchestration logic from the underlying distributed
infrastructure. All this whilst remaining general purpose and
extensible, for example, using the Migol adaptor, the application can
also benefit from additional features, such as the automatic
monitoring and the transparent recovery of failed tasks.  \jhanote{May
  want to replace or remove: At the same time, SAGA allows the
  application to easily utilize other infrastructures in conjunction
  to or as replacement to Migol.}




\section{Experiences with REMD}
\label{sec:exp}       
        
To evaluate the performance of the REMD-Manager several
experiments have been conducted on the LONI
Grid~\citep{loni}. The REMD-Manager is used to deploy tasks to
the LONI clusters: QueenBee (QB), Poseidon and Eric.  \jhanote{I think
  we want to say REMD simulations have been deployed on three LONI
  Linux Clusters? Not the managers themselves -- which I think sits on
  users desktop...}  \alnote{adjusted that - should i change figure 4
  too, although for the measurements all jobs have been launched from
  QB. There are some firewalls which limit the desktop experience.}
QB, which is both a LONI and a TeraGrid resource, is the largest LONI
machine and has a peak performance of over 50 TFlops.
Figure~\ref{fig:saga-taskfarming} gives an overview of the testbed.

\begin{figure}[t]
    \centering
        \includegraphics[width=0.7\textwidth]{saga-taskfarming}
        \caption{\footnotesize \bf Fault-Tolerant MD Simulations: The
          REMD-Manager orchestrates a set of distributed replica
          processes using the SAGA API. All processes synchronize
          important metadata with the Migol infrastructure. Migol then
          actively monitors all processes and ensures that, even in
          the presence of failures, all task are eventually
          completed.}
    \label{fig:saga-taskfarming}
\end{figure} 
The main objective of the first set of experiments is the
quantification of the runtime overhead, which Migol-enabled
applications, such as the REMD-Manager encounter.  Scientific results
obtained from using this infrastructure will be reported elsewhere.

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.6\textwidth]{performance/perf_submission.pdf}
    
    \caption{\footnotesize \bf SAGA-CPR Migol Adaptor Overhead}
    
    \label{fig:performance_perf_submission}
\end{figure}           

Figure~\ref{fig:performance_perf_submission} shows the response-times
of SAGA-CPR submissions in comparison to their non fault-tolerant
counterparts. Since each replica exchange step involves the
relaunching of two replica jobs, the efficient spawning of remote
tasks is a critical operation for the REMD-Manager.

Initially, the submission time of a single NAMD task using
SAGA-CPR/Migol is assessed. The experiment showed that a CPR/Migol job
submission is on average 2\,seconds slower than a GRAM
submission. This overhead is mainly attributable to the additional
metadata registration operation at Migol's AIS. For jobs that run on
the order of hours, a couple of seconds overhead is effectively
negligible.

Further, the Migol adaptor showed some additional initialization
overhead. The overall runtime of the NAMD submission task including
the initialization was 6.5\,sec (bar 3 in
Fig.~\ref{fig:performance_perf_submission}), about 4.5\,sec slower
than the GRAM submission task. This overhead can be attributed to the
initialization operations for setting up the HTTP server as well as
the conduction of several metadata updates on the AIS. Since this
initialization only occurs once after the startup of the REMD-Manager,
this overhead is acceptable.
                                                                                                                    
In addition, we investigated how the runtime of a single replica run
is effected by Migol's active monitoring mechanism and the required
checkpoint registration. For this purpose, a medium-size NAMD job was
started with and without Migol support.  Monitoring intervals between
20\,s and 2\,minutes were chosen to study the effect of monitoring
frequency on runtimes. The update interval for checkpoint metadata was
set to five minutes.  Since the time measured for a checkpoint update
operation was on average 1.3\,seconds, we do not expect this to be a
critical factor.  The runtime of the NAMD job on QB without CPR/Migol
amounted to 21.3 minutes.  At worst a 1 minute overhead was observable
with a monitoring interval of 20\,s. With lower monitoring intervals,
overheads were reduced further, e.g., the runtime overhead with a
2\,minute monitoring interval was only 10\,s, which is only slightly
higher than the variance of typical NAMD runtimes.

We also evaluated the performance of the REMD-Manager.  The
REMD-Manager was configured to run a simulation with 2 to 8 replica
processes and 16 replica-exchange steps. To stress test the Migol
infrastructure very short NAMD tasks with only 100 steps have been
used. Table~\ref{tab:app_stats} summarizes the REMD configuration
used. The runtime of a REMD simulation depends to a great extend on
the queuing time at the local resource management system. Thus, we attempted 
to minimize the queueing times during our experiment. However, as the results 
show, small queueing delays could not always be avoided.
\begin{table}
    \centering
	\begin{tabular}{|p{5cm}|l|}
          \hline
          %Molecular Dynamics Code &NAMD\\ \hline
          Number of NAMD steps &100\\ \hline 
          Number of MPI processes per NAMD run &16\\ \hline 
          Required staging files/size &6\,files/10\,MByte\\ \hline
          %Runtime of a single NAMD task (QB) &2\,minutes\\ \hline   
          Number of replica processes &2-8 \\ \hline
          Total number of replica-exchange steps \jhanote{See caption for Q} &16\\ \hline
          %Total Runtime: &??   \\ \hline
	\end{tabular}
	\caption{\footnotesize \bf REMD Application Characteristics\label{tab:app_stats}
          \jhanote{Is 1000 the the total number of exchanges or the
            number of exchanges that a single replica will undergo?}
          For completeness we should probably mention the temperature
          range over which simulations were performed}
          \alnote{Each replica process (NAMD simulation) will conduct 
          100 steps (referred to as NAMD steps in the table). This 
          will be repeated 10 times (number exchange steps) with 
          different temperatures.}         
          \alnote{Maybe we should remove this table and put the not mentioned information 
          into the text}
          
\end{table}   
\begin{figure}[htb]
    \centering
    \hspace*{-20pt}
        \includegraphics[width=0.5\textwidth]{performance/perf_remd.pdf}
        \caption{\footnotesize \bf REMD Runtime: 
         The time that it takes to complete 16 replica exchanges; 
each replica runs for 100 steps, before attempting an exchange with
the paired-replica. Although the simultaneous deployment of
          replicas across multiple resources (labels with QB/Poseidon)
          has scheduling challenges compared to the usage of a single
          cluster (scenario QB), for scenarios studied here there is a
          slight reduction in time-to-completion.
%  The overhead of the Migol is with
%         15\,seconds negligible compared to the overall
%         runtime.
          \jhanote{need some further clarity} }
    \label{fig:performance_perf_runtime}
\end{figure}     
\alnote{This must be reworked with the new results!}
Figure~\ref{fig:performance_perf_runtime} illustrate the results of
this evaluation. Since the total number of replica exchange steps
remained constant, the runtime decreases the more replica processes
are used.  With more than four replica processes a slight
decrease of the efficiency can be observed.
% This is a result of the sequential overhead which proportional 
% grows with the number of replicas: 
The more replica processes, the more dominant the sequential overhead
at the REMD-Manager becomes. To emulate the most general case, where
each exchange step requires the staging\footnote{With a sophisticated
  data management strategy this size can further be reduced} of
different files, in our setup, we staged six files with the total size
of about 10\,MB. This transfer, which required approximately 5-10\,s
on the LONI network.  Due to the small problem set computed by each
replica (only 100 NAMD steps, which require 35\,seconds computation
time on QB), this bottleneck becomes very evident. However, in more
realistic scenarios with larger problem chunks this issue will be
avoided.


On average, with all factors considered, the SAGA/Migol adaptor added
a total runtime overhead of about 15\,seconds to the
time-to-completion.  It is important to note that this does not change
significantly with either the number of replicas, number of
replica-exchanges, nor the runtime of each replica.  Thus the results
indicate that the SAGA/Migol overhead is acceptable, corroborating
earlier findings shown in
Figure~\ref{fig:performance_perf_submission}.

Figure~\ref{fig:performance_perf_runtime} also shows that our approach
can be employed to orchestrate multiple resources concurrently, as
well as different resources (QB/Poseidon/Eric) individually.  During
the coupled distributed run, one half of the processes were allocated to
the smaller machines Eric and Poseidon, while the bulk stayed on QB.  As the
number of replicas gets larger, the concurrent distributed runs have a
lower time-to-completion than when QB was used in isolation was
observed (all else being equal). However it is important to note, that
smaller machines, such as Poseidon, showed long queuing times leading
to high variance in the overall time-to-completion. This overhead is
significant for short-running tasks and less so for longer running
tasks. 

\kimnote{However, this simple scenario faces high overhead since each
  job is submitted to the local scheduler in many cases. \it Shantenu,
  may be you can write here another scenario we discussed as an
  alternative to the simple scenario we are testing.  If I can, I will
  try, too } \alnote{I added some remarks towards a enhanced scenario
  as result of the measurements}
  

Since the probability of a failure during a 10 minute run on a few
resources is rather low, the reliability of the proposed framework was
validated by introducing faults into the systems. We killed selected
replica processes and measured the time required by Migol to restart
the system.  Due to the selected monitoring interval of one minute and
failure threshold of 2 tries, the failure detection time averages to
2.5\, minutes.

As shown in Figure~\ref{fig:performance_perf_submission}, the recovery
time required for the restart of the job is $\sim42$ seconds. This is
mainly caused by the complex interactions conducted by the Migol
backend (cmp. section~\ref{sec:migol}): The monitoring service
initializes the restart at the JBS.  A major performance penalty is
the delegation-on-demand mechanism required to obtain the credential
of the user from the AIS -- this procedure demands the creation of a
public-private key pair, which is very costly. Further, the resource
discovery and selection mechanisms used by Migol's JBS are designed
with a focus on long-running applications, and currently show some
substantial overhead, especially when used for short-running tasks.


While these results show that SAGA-CPR in conjunction with Migol
incurs some overhead, we believe that this is acceptable compared to
the benefits a fault-tolerant, self-healing infrastructure offers. In
addition, it must be noted that further simple yet effective
optimizations are possible. For example, by directly restarting jobs
via the GRAM service a lot of the overhead caused by the dynamic
discovery mechanisms of the JBS can be avoided. Further, we will
evaluate possibilities to decouple the dispatching of replica runs
from allocation of cluster resources to avoid long queuing
delays. Systems, such as Falkon~\citep{1362680} or the Condor
Glide-In~\citep{citeulike:291860} mechanism provide the possibility to
acquire chunks of resources from the resource management systems,
which can then be used to dispatch short tasks. 


\section{Related Work}

\jhanote{This might have to be commented out: Checkpointing and
  rollback recovery is widely used in Grids. For example, the
  Condor/PGRADE system~\citep{DBLP:conf/eagc/KovacsK04} consists of a
  checkpointing mechanism for PVM applications and uses
  Condor-G~\citep{citeulike:291860} for scheduling.  While PGRADE
  emphasises an integrated user-level checkpoint approach, we believe
  that this approach is not suitable for a heterogeneous Grid
  landscape. Further, the framework does not ensure the
  fault-tolerance of the service infrastructure sufficiently.}
                                 
\jhanote{This will have to be commented out: Further, these frameworks
  or schedulers focus on individual aspects, e.\,g.\ Nimrod-G focuses
  on task farming or GridWay on meta-scheduling. Migol aims to provide
  an overall autonomic, self-healing infrastructure, which addresses
  the fault tolerance of Grid applications and the infrastructure
  itself.}

\noindent{\it Previous CPR Efforts:} Several frameworks for
high-throughput computing and task farming exist,
Condor-G~\citep{citeulike:291860}, Nimrod-G~\citep{buyya00nimrodg}, and
Legion~\citep{689541} to name a few. These provide basic fault
tolerance support by automatic re-scheduling failed tasks. Advanced
features such as the management of checkpoints however, are not
supported. Further, these frameworks rely on a very simple failure
detection mechanism -- usually by simply polling the job state at the
Globus gatekeeper. This allows the detection of some errors, but
application-level failure detectors as used by the Migol/SAGA library
can detect much more complex errors. For example, especially parallel
applications can fail quite inconsistently: in the best case the
application aborts, at worst the application hangs indefinitely. These
kind of failures are not visible at Grid resource management system
level.

At the level of related application programming interfaces for
checkpointing, proprietary interfaces are dominant. This is because
applications most often rely on application level checkpointing, and
perform also their own checkpoint management (checkpointing policies,
frequencies, dependencies, staging etc).  \alnote{Should we remove
  this from here - this is already mentioned now at the beginning.}
The Open Grid Forum's\footnote{\texttt{http://www.ogf.org}} GridCPR
group (Grid CheckPoint and Recovery) made an early attempt to describe
a generic CPR architecture, and to define a generic CPR API, which
would support applications to manage their complete
checkpoint/recovery life cycle~\citep{ogf_cpr_arch}.  Based on that
architecture, and on a set of CPR use cases~\citep{ogf_cpr_uc}, the
SAGA group in OGF defined the CPR API package~\citep{saga_cpr_draft}
(work in progress), whose implementation is described in this paper.
The rendering of the CPR API in the SAGA API framework allows (a) to
seamlessly combine CPR operations and other high level Grid
programming abstractions provided by SAGA, and (b) to abstract from
the actual implementation of the CPR mechanism.  The CPR API which has
been demonstrated with the Migol framework, can work as well with
other systems, e.\,g.\, the XtreemOS system level checkpointing
capabilities~\citep{xtreemos_cpr}.


\noindent{\it Other Distributed RE simulations:}
Several projects, such as Folding@home and WISDOM, utilise distributed
infrastructures. While
Folding@home~\citep{PhysRevLett.86.4983}\footnote{Folding@Home~\citep{PhysRevLett.86.4983}
  is parallel replica dynamics but that is a special case of
  replica-exchange; when a certain event happens, there is a need for
  coordination amongst ALL replicas. We should maybe point this out,
  but I think it is fair at this level of detail, to consider
  folding@Home to be in the same application class to effectively
  parallelize simulations} is based on BOINC~\citep{1033223}, the
WISDOM project utilizes the EGEE infrastructure. 
Although WISDOM has similar application characteristics as discussed
here, the project is currently tied to the gLite~\citep{glite}
middleware.  In contrast to WISDOM and Folding@home, our approach is
not restricted to a specific distributed environment. SAGA based
job-launching and file-handling is supported on most general-purpose
Grids via the appropriate adaptors, as SAGA is a community
specification and is soon to be standard~\citep{saga_url}

\alnote{Should we add some details regarding the scientific results of
  WISDOM and Folding@home and how they differ from our REMD with NAMD?
  Or are we just comparing the Grid infrastructure?}  \jhanote{In
  response to immediately preceeding alnote, IMHO we don't need to
  address scientific results, but will just say the ``size of the
  problem'' that can be studied is limited}

%%------------------------------------------------------------------------------
\section{Conclusion and Future Work}

We have developed a fault-tolerant framework that implements a
commonly occurring application usage pattern: loose-coupling of
multiple tightly-coupled applications. The framework is general
purpose and extensible to different usage patterns, deployment
scenarios and specific simulation codes.

The fault-tolerant framework used to implement RE simulations in a
production environment is created using the distributed programming
interfaces provided by SAGA and its coupling to Migol.  SAGA provides
a middleware-independent, programing abstraction for distributed
environments. RE simulations utilize the new SAGA-CPR API to interface
with a checkpoint-recovery infrastructure, such as Migol. Using the
newly developed SAGA adaptor for Migol, any SAGA application can
re-use Migol's fault-tolerant services for monitoring and recovery.
The application developer is not required to provide any special code,
just the Migol adaptor must be configured.  The Migol framework has
strong self-healing capabilities: critical services, such as the
Application Information Service (AIS) are able to automatically detect
failures and reconfigure themselves, and thus addresses common failure
modes in distributed environments without user interaction.
In case of failures, e.\,g.,\ a node-crash, applications are
automatically restarted from the last saved
checkpoint.

In contrast to other RE implementations on distributed simulations, it
is critical to note and emphasise the general usability and
extensibility -- across different infrastructures, across a range of
scientific applications and usage patterns (e.g.  the multiple
variants of the RE) -- of our approach.

\jhanote{In future work, we need to mention that we are deploying this
  infrastructure on a real distributed system (LONI) and are using it
  to study the binding interactions of peptide-RNA (Joohyun, would you
  agree?). We will report on the specific science results obtained
  using this approach in publication TBD but most likely Phil Trans of
  Royal Soc A}

\begin{acknowledgement}
   This work would not have been possible without the efforts and
  support of the wider SAGA team, especially Hartmut Kaiser, Ole
  Weidner and Joao Abecasis. We also thank Daniel Katz for hosting AL
  at the CCT, when this work was conceived.  Important funding for
  SAGA specification and development has been provided by the UK EPSRC
  grant number GR/D0766171/1 (via OMII).  SJ acknowledges the
  e-Science Institute, Edinburgh for supporting the research theme,
  ``Distributed Programming Abstractions''.  This work has also been
  made possible thanks to the internal resources of the Center for
  Computation \& Technology (CCT) at Louisiana State University and
  computer resources provided by LONI.
\end{acknowledgement}

% \begin{thebibliography}  {10}
% \bibitem{dpa-paper}
% S. Jha et al., {\em Programming Abstractions for Large-scale Distributed
%   Applications}, to be submitted to ACM Computing Surveys; draft at
%   \url{http://www.cct.lsu.edu/~sjha/publications/dpa_surveypaper.pdf}.
% \end{thebibliography}

%\bibliographystyle{IEEEtran}
\bibliographystyle{agsm}
\bibliography{saga,literatur}    
\end{document}

