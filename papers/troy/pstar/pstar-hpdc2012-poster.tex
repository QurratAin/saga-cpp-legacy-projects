%\documentclass[conference,final]{IEEEtran}
%\documentclass{acm_proc_article-sp}

\documentclass{sig-alternate}

%\documentclass{acm_proc_article-sp}

\input{head}

\begin{document}
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
% \conferenceinfo{ECMLS'11,} {June 8, 2011, San Jose, California, USA.}
% \CopyrightYear{2011}
% \crdata{978-1-4503-0702-4/11/06}
% \clubpenalty=10000
% \widowpenalty = 10000

\conferenceinfo{HPDC'12,} {June 18--22, 2012, Delft, The Netherlands.} 
\CopyrightYear{2012} 
\crdata{978-1-4503-0805-2/12/06} 
\clubpenalty=10000 
\widowpenalty = 10000

\input{include}

% \author{
%   Andre Luckow$^{1}$, Mark Santcroos$^{2,1}$, Ole Weidner$^{1}$, Andre Merzky$^{1}$, Sharath Maddineni$^{1}$, Shantenu Jha$^{3,1*}$\\[0.5em]
%   \small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State University, USA}}\\[-0.3em]
%   \small{\emph{$^{2}$Bioinformatics Laboratory, Academic Medical Center, University of Amsterdam, The Netherlands}}\\[-0.3em]
%   \small{\emph{$^{3}$ Rutgers University, Piscataway, NJ 08854, USA}}\\[-0.3em]
% %  \small{\emph{$^{4}$ School of Informatics, University of Edinburgh, UK }}\\[-0.3em]
%   \small{\emph{$^{*}$Contact Author: \texttt{shantenu.jha@rutgers.edu}}}\\[-0.3em]
%   \up\up\up }


\title{P*: A Model of Pilot-Abstractions\up}

% \numberofauthors{5}
% \author{
% \alignauthor Andre Luckow\\
%        \affaddr{Center for Computation\newline and Technology}\\
%        \affaddr{Louisiana State University}\\
%        \affaddr{216 Johnston}\\
%        \affaddr{Baton Rouge, LA} \\
%        \email{aluckow@cct.lsu.edu}
% \and
% \alignauthor Mark Santcroos\\
%        \affaddr{Bioinformatics Laboratory}\\
%        \affaddr{Academic Medical Center}\\
%        \affaddr{University of Amsterdam}\\
%        \affaddr{Meibergdreef 9}\\
%        \affaddr{Amsterdam, The Netherlands}
%        \email{m.a.santcroos@amc.uva.nl}
% \and
% \alignauthor Ole Weidner\\
%        \affaddr{Center for Computation\newline and Technology}\\
%        \affaddr{Louisiana State University}\\
%        \affaddr{216 Johnston}\\
%        \affaddr{Baton Rouge, LA}
%        \email{oweidner@cct.lsu.edu}
% \and
% \alignauthor Andre Merzky\\
%        \affaddr{Center for Computation\newline and Technology}\\
%        \affaddr{Louisiana State University}\\
%        \affaddr{216 Johnston}\\
%        \affaddr{Baton Rouge, LA}
%        \email{amerzky@cct.lsu.edu}
% \alignauthor Sharath Maddineni\\
%        \affaddr{Center for Computation\newline and Technology}\\
%        \affaddr{Louisiana State University}\\
%        \affaddr{216 Johnston}\\
%        \affaddr{Baton Rouge, LA}
%        \email{smaddineni@cct.lsu.edu}
% \and
% \alignauthor Shantenu Jha\\
%       \affaddr{CAC/ECE}\\
%      \affaddr{Rutgers University}\\
%       \affaddr{94 Brett Road}\\
%       \affaddr{Piscataway, NJ}
%      \email{shantenu.jha@rutgers.edu}
% }

\date{}

\maketitle


\begin{minipage}[h]{\textwidth} 
\vspace{-6cm}
\begin{tabular}{cccccc}
Andre Luckow$^{1}$ &Mark Santcroos$^{2}$ &Ole Weidner$^{1}$ &Andre Merzky$^{1}$ &Sharath Maddini$^{1}$ &Shantenu Jha$^{3*}$\\
% \multicolumn{4}{c}{\{aluckow, amerzky, oweidern, smaddineni\}@cctl.lsu.edu} &m.a.santcroos@amc.uva.nl &shantenu.jha@rutgers.edu\\

&&&&&\\

\multicolumn{2}{c}{$^1$Center for Computation \& Technology} &\multicolumn{2}{c}{$^{2}$Bioinformatics Laboratory} 
&\multicolumn{2}{c}{$^{3}$ECE}\\
\multicolumn{2}{c}{Louisiana State University} &\multicolumn{2}{c}{Academic Medical Center} &\multicolumn{2}{c}{Rutgers University}\\
\multicolumn{2}{c}{Baton Rouge, LA} &\multicolumn{2}{c}{University of Amsterdam, NL} &\multicolumn{2}{c}{Piscataway, NJ}\\
\end{tabular}
\end{minipage}


\let\thefootnote\relax\footnotetext{$^{*}$Contact Author: shantenu.jha@rutgers.edu}

\vspace{-2cm}
\begin{abstract} 
  \pilotjobs have become one of the most successful abstractions in
  distributed computing. In spite of extensive uptake, there does not
  exist a well defined, unifying conceptual model of \pilotjobs which
  can be used to define, compare and contrast different
  implementations. This presents a barrier to extensibility and
  interoperability. This paper is an attempt to, (i) provide a minimal
  but complete model (P*) of \pilotjobs, (ii) establish the generality
  of the P* Model by mapping various existing and well known \pilotjob
  frameworks such as Condor and DIANE to P*, (iii) demonstrate the
  interoperable and concurrent usage of {\it distinct} pilot-job
  frameworks on different production distributed cyberinfrastructures
  via the use of an extensible API for the P* Model (Pilot-API).
\end{abstract}

% derive an interoperable and extensible API for the P* Model
% (Pilot-API), and (iv) validate the implementation of the Pilot-API by
% concurrently using multiple {\it distinct} pilot-job frameworks on
% distinct production distributed cyberinfrastructures.  \jhanote{We
%   should be careful as to whether to say if this paper addresss (iii)?
%   We don't discuss Pilot-API and we don't validate using the
%   implementation using concurrent pilot-job frameworks?  Maybe
%   rephrase (iii) and (iv)?}

%\category{H.4}{Information Systems Applications}{Miscellaneous}
%terms{Theory}
%\keywords{ACM proceedings, \LaTeX, text tagging}
%\up\up
\upp
\section{Introduction and Overview} 

Distributed cyber/e-infrastructure is by definition comprised of a set
of resources that is fluctuating -- growing, shrinking, changing in
load and capability (in contrast to a static resource utilization
model of traditional parallel and cluster computing systems).  The
ability to utilize a dynamic resource pool is thus an important
attribute of any application that needs to utilize distributed
cyberinfrastructure (DCI) efficiently. \pilotjobs (PJ) provide an
effective abstraction for dynamic execution and resource utilization
in a distributed context. As a consequence of providing a simple
approach for decoupling workload management and resource
assignment/scheduling, \pilotjobs have been one of the most successful
abstractions in distributed computing.  Not surprisingly, there are
multiple, distinct and incompatible implementations of
\pilotjobs. Often these implementations are strongly coupled to the
distributed cyberinfrastructure they were originally designed for.

% The fundamental reason for the success of the PJ abstraction is that
% PJ liberate applications/users from the challenging requirement of
% mapping specific tasks onto explicit heterogeneous and dynamic
% resource pools.  PJ also thus shields application from having to
% load-balance tasks across such resources.  The \pilotjob abstraction
% is also a promising route to address specific requirements of
% distributed scientific applications, such as coupled-execution and
% application-level scheduling~\cite{ko-efficient,DBLP:conf/hpdc/KimHMAJ10}.

% A variety of PJ frameworks have emerged:
% Condor-G/ Glide-in~\cite{condor-g}, Swift~\cite{Wilde2011},
% DIANE~\cite{Moscicki:908910}, DIRAC~\cite{1742-6596-219-6-062049},
% PanDA~\cite{1742-6596-219-6-062041}, ToPoS~\cite{topos},
% Nimrod/G~\cite{10.1109/HPC.2000.846563}, Falkon~\cite{1362680} and
% MyCluster~\cite{1652061} to name a few. Although they are all, for the
% most parts, functionally equivalent -- they support the decoupling of
% workload submission from resource assignment -- it is often impossible
% to use them interoperably or even just to compare them functionally or
% qualitatively.  The situation is reminiscent of the proliferation of
% functionally similar yet incompatible workflow systems, where in spite
% of significant a posteriori effort on workflow system extensibility
% and interoperability (thus providing post-facto justification of its
% needs), these objectives remains difficult if not infeasible.

Our objective is to provide a minimal, but complete model of
\pilotjobs: The P* Model provides a conceptual basis to compare and
contrast different PJ frameworks.  We perform experiments
demonstrating interoperability across middleware, platform and
different PJ frameworks, and to establish the effectiveness of the
Pilot-API.

% the workings of the Pilot-API.  We discuss how existing and widely
% used \pilotjob frameworks, can be used through the Pilot-API.

% It is worth noting that \pilotjobs are used on every major
% national and international DCI, including NSF/XSEDE,
% NSF/DOE Open Science Grid, EU EGI and others, to support hundreds and
% thousands of tasks daily, thus we believe the impact and validation of
% this paper lies in its ability to not only influence but also bridge
% the theory and practice of \pilotjobs, and thus multiple domains of
% science dependent on distributed cyberinfrastructure.





%  which is also partly motivated by
% the status of the usage and availability of the pilot abstraction
% vis-\`{a}-vis the current landscape of distributed applications and
% cyberinfrastructure.

 % Collectively pilot-jobs support
% hundreds of thousands if not millions of tasks every day.  As we will
% show, the Pilot-API provides a single uniform API to PJ-frameworks
% thus providing for the first time... removing barriers to...
% \jhanote{fix..}


\section{The P* Model of Pilot-\\Abstractions}
\label{sec:pilot-model}

The P* model is derived from an analysis of many \pilotjob implementations;
based upon this analysis, we present the common {\it elements} of the P*
Model, followed by a description of the interaction of these elements and the
overall functioning of a \pilotjob framework. The P* Model defines the
following elements:

\begin{figure}[t]
	\upp\upp
    \centering
    \includegraphics[width=0.45\textwidth]{figures/pstar_model_single.pdf}
    \caption{ \textbf{P* Model: Elements and
        Interactions:} The manager has two functions: it manages 1)
      Pilots (step 1-3) and 2) the execution of \cus. After a \cu is
      submitted to the manager, it transitions to an \su, which is
      scheduled to a \pilot by the PM.}
	\upp\upp
    \label{fig:figures_pstar}
\end{figure}

% \noindent 
% \subsection{Elements of the P* Model}

%\noindent 



%\alnote{use pilot NOT pilot-job}
\noindent$\bullet$ \textbf{\pilot (Pilot-Compute):} The \pilot is the
  entity that actually gets submitted and scheduled on a resource.
% via the resource's RM system. 
  The PJ provides application (user)
  level control and management of the set of allocated resources.

  % and is responsible for the execution of \alwave{SUs/tasks}
  % onto the resource.

  % \alnote{\textbf{The RM assigns a slice of the Resource to the
  %     pilot -- that pilot then acts as RM for that resource slice.}
  %   I think simply equating a pilot with a RM is bit too
  %   simplistic. In a sense it is a application-level resource
  %   manger. Not sure what to do}

\noindent$\bullet$ \textbf{\computeunit  (\cu):} A \cu  encapsulates a 
  self-contained piece of work (a task) specified by the application that is
  submitted to the \pilotjob framework. There is no intrinsic notion
  of resource associated with a \cu.

\noindent$\bullet$ \textbf{Scheduling Unit (SU):} SUs are the units of 
  scheduling, internal to the P* Model, i.e., it is not known by or
  visible to an application. Once a \cu is
  under the control of the \pilotjob framework, it is assigned
  to an SU.
  % An SU is created after the submission of
  %   a \cu, i.\,e.\ once a \cu \ has been passed into the control of the
  %   pilot-job framework. 
%  \jhanote{Could we say: Once a \cu has been
%    passed into the control of the pilot-job framework, it is assigned
%    to an SU} \alnote{sounds good. done.}

\noindent$\bullet$ \textbf{Pilot-Manager (PM):} The PM is responsible for (i)
  orchestrating the interaction between the \pilots as well as the
  different components of the P* Model (\cus, \sus) and (ii) decisions
  related to internal resource assignment (once resources have been
  acquired by the \pilotjob).  For example, an SU can consists of one
  or more \cus. Further, \cus and \sus can be combined and aggregated;
  the PM determines how to group them, when \sus are scheduled and
  executed on a resource via the \pilot, as well as how many resources
  to assign to an SU.

% An application kernel is the actual binary that gets executed.  The
% application utilizes a PJ framework to execute multiple instances of
% an application kernel (an ensemble) or alternatively instances of
% multiple different application kernels (a workflow).  To execute an
% application kernel, an application must define a \cu \ specifying the
% application kernel as well as other parameters. This \cu \ is then
% submitted to the PM (as an entry point to the
% \pilotjob framework), where it transitions to an SU. The PM is then
% responsible for scheduling the SU onto a \pilot and then onto a
% physical resource.  

Figure~\ref{fig:figures_pstar} illustrates the interactions between the
elements of the P* Model. First, the application specifies the capabilities of
the resources required using a \pilotjob description (step 1). The PM then
submits the necessary number of \pilots to fulfill the resource requirements
of the application (step~2). Each \pilot is queued at the resource manager,
which is responsible for starting the \pilot (step~3). The application can
submit \cus to the PM at any time (step~4). A submitted \cu \ becomes an SU,
i.\,e.\ the PM is now in control of it. In the simplest case one \cu \
corresponds to one SU; however, SUs can be combined and aggregated to optimize
throughputs and response times. 
% Commonly, a hierarchical M/W model for
% coordination is used internally: the PM uses M/W to coordinate a set of
% \pilots, the \pilot itself functions as manager for the execution of the
% assigned SUs.


\begin{table}[t]
	\upp
 \centering
 \begin{tabular}{|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{2.5cm}|}
  \hline
  \textbf{P* Element}    &\textbf{BigJob} &\textbf{DIANE} &\textbf{Condor-G/Glide-in}  \\\hline
  Pilot-Manager          &BigJob Manager  & RunMaster     & condor\_master\newline 
                                                            condor\_collector\newline 
                                                            condor\_negotiator\newline 
                                                            condor\_schedd                \\\hline
  \pilot                 &BigJob Agent    & Worker Agent  &condor\_master\newline
                                                           condor\_startd                 \\\hline
  \computeunit  \ (CU)   &Task            &Task           &Job                            \\\hline
  Scheduling Unit \ (SU) &Sub-Job         &Task           &Job                            \\\hline
% Dynamic Resources &no/yes &yes (AgentFactories)\\
% \hline
 \end{tabular}
 \caption{\textbf{Mapping P* elements and PJ Frameworks:} While each
   PJ framework maintains its own vocabulary, each of the P* elements
   can be mapped to one (or more) components of the different
   frameworks. }\upp \upp
 \label{table:bigjob-saga-diane}
\end{table}


As shown in table~\ref{table:bigjob-saga-diane}, the above elements
can be mapped to specific entities in different \pilotjobs framework,
e.\,g.\ BigJob~\cite{saga_bigjob_condor_cloud},
Condor~\cite{condor-g-short} and DIANE~\cite{Moscicki:908910}. While
most of these frameworks share many properties, they often differ in
their implementation (e.\,g.\ of the communication mechanism) and
usage modalities. These differences are captured by the
``characteristics'' of the P* Model.

% \subsection{Experiments and Results}
%  \label{sec:exp_res}

The aim of the Pilot-API~\cite{pilot_api} is to provide an abstract,
unified interface to Pilot-Job frameworks that adhere to the P* Model.
Figure~\ref{fig:perf_perf-bfast-bj} shows how the Pilot-API enables
the user to run applications interoperably on different production and
research infrastructures. For this purpose we investigate the
performance of BFAST, a next-generation genome sequencing
application. BFAST is very I/O sensitive -- we observed for example,
an I/O bottleneck if many BFAST CUs are run on the same shared file
system. The Pilot-API enables applications to scale to different
infrastructures in such cases.

 
\begin{figure}[t]
	\upp
\centering
\includegraphics[width=0.48\textwidth]{perf/interop/128-bfast-egi-fg-xsede-osg.pdf}
\caption{\textbf{PJ Framework Performance on XSEDE, FutureGrid, EGI and 
  OSG:} Running 128 BFAST match tasks on 128 cores. The longer runtimes on EGI 
  and OSG are mainly caused by  longer queuing times and the necessity to   stage all input files. }\upp\upp\upp
  \label{fig:perf_perf-bfast-bj}
\end{figure}


\section{Discussion and Future Work} 
\label{sec:discussion-future-work}

Although a variety of PJ frameworks have emerged, which are, for the
most parts, functionally equivalent, it is often impossible to use
them interoperably or even just to compare them. The primary
contribution of this work is the development of the P* Model, the
usage of the P* elements to describe and characterize PJ frameworks
such as DIANE and Condor-G/Glide-in.  We demonstrated the practical
relevance of this work by using different PJ frameworks via a unified
API, and enabling applications to scale across different
infrastructures and utilize performance advantages.

Pilot-abstractions provide significant future research \& development
opportunities, especially in their use for data-intensive
applications.
% While Pilot-Jobs efficiently support late-binding of\computeunits and resources, 
The management, placement and scheduling of data in distributed
systems remains a challenge due to various reasons: (i) the placement
of data is often decoupled from the placement of Compute Units and
Pilots, i.\,e.\ the application must often manually stage in and out
its data using simple scripts; (ii) heterogeneity, e.\,g.\ with
respect to storage, filesystem types and paths, often prohibits late
binding decisions; (iii) absence of capabilities that allow
applications to specify their data dependencies on an abstract,
logical level (rather than on file basis) are not available; (iv) due
to lack of a common treatment for compute and data, optimizations of
data/compute placements are often not possible. In addition,
applications must cope with various other challenging, data-related
issues, e.\,g.\ varying data sources (such as sensors and/or other
application components), fluctuating data rates, transfer failures,
optimizations for different queries, data-/compute co-location
etc. While these issues can be handled in an application-specific
manner, the use of general-purpose, infrastructure independent
capabilities, such as a common \pilot-based abstraction for compute
and data presents several advantages. This motivates an abstraction
for data, that is analagous to Pilot-Jobs, that we refer to as
\emph{\pilotdata (PD)}.


% PD provides late-binding capabilities for data
% by separating the allocation of physical storage and application-level
% data units. Further, it provides an abstraction for expressing and
% managing relationships between data units and/or compute units. These
% relationships are referred to as \emph{affinities}.



% \jhanote{best to rewrite..}

% The primary intellectual contribution of this work has been the
% development of the P* Model, the mapping of P* elements to PJ
% frameworks such as DIANE and Condor-G/Glide-in and the design and
% development of the Pilot-API -- that reflects the P* elements and
% characteristics.
 
% The P* Model provides a common abstract model for describing and
% characterizing Pilot-abstractions.  We validate the P* Model by
% demonstrating that the most widely used PJ frameworks, viz., DIANE and
% Condor-G/Glide-in can be compared, contrasted and analyzed using this
% analytical framework.  Furthermore we demonstrate the use of the
% Pilot-API -- which provides a common access layer to different PJ
% frameworks, with multiple PJ frameworks over distributed production
% cyberinfrastructure, such as XSEDE, OSG, EGI and FutureGrid. The
% Pilot-API also enables the concurrent use of multiple PJ frameworks,
% thus providing interoperability and extensibility.  Although the aim
% of our experiments is the demonstration of the interoperable use of
% hitherto distinct and disjoint \pilotjobs, in the process we highlight
% the performance advantages that can emanate from the ability to
% seamlessly distribute (I/O intensive) workloads in a scalable manner.


% \section*{Acknowledgements}
% This work is funded by NSF CHE-1125332 (Cyber-enabled Discovery and
% Innovation), HPCOPS NSF-OCI 0710874 award, NSF-ExTENCI (OCI-1007115)
% and NIH Grant Number P20RR016456 from the NIH National Center For
% Research Resources. Important funding for SAGA has been provided by
% the UK EPSRC grant number GR/D0766171/1 (via OMII-UK) and the
% Cybertools project (PI Jha) NSF/LEQSF
% (2007-10)-CyberRII-01. SJ acknowledges the e-Science Institute,
% Edinburgh for supporting the research theme. ``Distributed Programming
% Abstractions'' \& 3DPAS. MS is sponsored by the program of BiG Grid,
% the Dutch e-Science Grid, which is financially supported by the
% Netherlands Organisation for Scientific Research, NWO. SJ acknowledges
% useful related discussions with Jon Weissman (Minnesota) and Dan Katz
% (Chicago). We thank J Kim (CCT) for assistance with BFAST.  This work
% has also been made possible thanks to computer resources provided by
% TeraGrid TRAC award TG-MCB090174 (Jha) and BiG Grid.  This document
% was developed with support from the US NSF under Grant No. 0910812 to
% Indiana University for ``FutureGrid: An Experimental, High-Performance
% Grid Test-bed''.

% %\bibliographystyle{plain}
%\bibliographystyle{IEEEtranS}
\bibliographystyle{IEEEtran}
\bibliography{pilotjob,saga,saga-related}


% \bibliography{pstar-hpdc2012}

\end{document}
