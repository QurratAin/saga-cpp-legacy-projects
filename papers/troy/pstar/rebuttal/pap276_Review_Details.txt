

Reviewer 1:

  - Two main issues with Pilot Job are not addressed correctly in this
    paper: security and data.

    --> not factual, clarify if space
    --> MS: The security issue makes sense to some degree, we discussed glExec 
    at some point and discarded that. Of course there is always the space 
    permits issue.
    The data complaint is a bit void, as we explicitly address that as part of
    our model.


  - the Pilot-Data API, which more or less mimics the PilotJob API is
    not very convincing in the sense that the interaction between the
    2 APIs are not discussed at all. Maybe Pilot-Data should be
    presented as part of the P* model and section 6 should be
    dedicated to the implementation of the 2 APIs to data-intense
    computing.

    --> MS: Interestingly enough the reviewer doesn't consider data as part
    of the model. Apparently we failed to communicate that clearly.


Reviewer 2:

  - Some of the choices and assumptions in the paper are not obvious.
    These could be elaborated upon.  
    What about other frameworks? Can they be incorporated? 
    Why were these particular ones selected?  
    
    --> clarification: most widely used, we
    --> MS: Maybe better articulate why we choose this set of frameworks 
    for comparison?

    Is this really the first work of its kind?  Are there no other related
    attempts?

    --> we think it is the first - review seems to agree ;)
    --> MS: Agreed.




Reviewer 3:

  - though the goal of a unified system to integrate all pilot systems
    is a rather elusive problem.

    --> clarify that aim of API is not to unify via API, nor to
    integrate all PJ systems, but to expose the inherent P* model
    through the API.


  - One of the challenging aspects of this paper is the determining
    how to evaluate it. The authors present results that demonstrate
    that the tool they have built is working -- however, while some of
    the figures present performance numbers, the authors acknowledge
    that the evaluation should not be considered a performance
    analysis of their framework, as the performance is based largely
    on the computing infrastructure the pilot frameworks they consider
    are designed for.

    --> not factual: dominant cost is backend, we just show client /
    access layer.  Acknowledge that we can do better...


  - It would however be a substantial improvement for the introduction
    to clearly define pilot jobs, and motivate the specific problem
    the authors are trying to solve. The primary motivation given is
    that in other frameworks that bear some similarity to pilot jobs,
    it has been hard to unify them.  

    --> rebut, point to 'terms and usage' in Sec. 2

    
  - In the end, the paper seems like a pilot job system for pilot job
    systems. 
    
    --> rebut, while we show such a 'system of system', we rather want
    to expose the p* model *inherent* to the different fraeworks.


  - While the work seemed sound, I was unconvinced by the paper that
    another layer of indirection and abstraction was the right
    solution for connecting together pilot job systems.

    --> rebut: it is not about another layer of indirection.  This is
    not a solution of interop, this is a validation of the P* model.
    Also see 4.1!

    
  - Perhaps another element which could strengthen the paper is a
    description, based on the authors analysis of pilot job systems,
    why pilot job systems cannot be unified in other ways.  
   
    --> rebut: left out more detailed discussion due to space
    limitations (federation, horizontal, vertical..).  We *do* briefly
    discuss service level vs. app level interop (4.1)
    


Reviewer 4:

  - This work seems important in practical relevance, but it is not
    clear how much fundamental work there really is here.

<<<<<<< .mine

=======
    --> we are heartened that there is universal agreement between the
    reviewers that work is of practical relevance.


>>>>>>> .r6849
  - Unfortunately, the experimental results are not very convincing.
    Firstly, the overhead caused by interoperability (P*) is not
    provided. And the introduced overheads are critical in
    understanding if it is a worthwhile approach. 

    --> rebut: we provide the P* overhead,

    --> clarify again that interop is means to demonstrate validity of
    model, but not the main purpose of the paper.  Pilot API is an
    *API* and is thus not performance critical (call stack latency of
    O(ns) vs. remote op latency of O(ms)). There is *no* additional
    semantics.  Also, see SAGA paper on API overheads.  Acknowledge
    that there are no hard numbers provided - but we focus on
    performance critical parts, like 
    
  - Secondly, the
    demonstration of interoperability is overly simplified. For
    example, so of these pilot job systems were never meant to operate
    decoupled from their computational frameworks....

    --> rebut: we state: 
    
    "Tools and implementations are often highly dependent on and tuned
    to a specific execution environment, further impacting
    portability, reusability and extensibility. Semantic and interface
    incompatibility are certainly barriers, but so is the lack of a
    common architecture and conceptual framework upon which to develop
    similar tools a barrier."

    But we go on and show that all the PJ frameworks actually
    implement and expose P*. 


  - If the complex workflow dependency management is taken out of the
    equation, then why bother to talk about the features of these
    workflow systems, such as the data from Table 2? 

    --> AM: rebut: I don't think we are talking about WF systems in table
    2?


  - More metrics like traffic, slowdown, and job speedup are needed to
    be measured to be convincing.
    --> AM: rebut: convincing for what?  Again, experiments are supporting
    model, and not claiming to lead to deeper understanding or better
    performance (how could an API or a uniform model do that?)

 
<<<<<<< .mine
  - The model seems unnecessary, straight forward, with not much
    substance.
=======
  - The model seems unnecessary, straight forward, with not much
    substance. 
>>>>>>> .r6849

    --> AM: do not answer!
    

  - Much of section 6 is repetitive The experiments are by far the
    weakest part of this paper. 

    --> AM: do not answer (again)!

    
  - For the experiments parts, the authors didn't explain very well
    how they test the interoperability between Pilot-Jobs and
    infrastructures exist. The results are also hard to follow, why
    certain experiments are done, and how to interpret the results.

    --> AM: not factual wrong, acknowledge space for improvement of
    presentation?


  - Experiments are not clear enough to support the arguments. Does
    not say how to handle the case if the function in the framework
    can not be map to the P* Model. Also, I was expecting that more
    time would be spend on the Pilot-Data model, and how this
    translated to the various pilot-based systems.

    --> AM: Data discussion was limited due to space constraints


  - Other trivial flaws: Section 4.1: "approach (ii) requires" should
    be approach (i) 

    --> *blush*

    
  - Figure 3: Physical Resource layer is confusing

    --> AM: how so?


  - Section 5.1: Alamo and Sierra? 
  
    --> *blush*


  - Figure7: "The longer runtimes on EGI and OSG are mainly caused by
    the longer queuing times." is inconsistent with the figure.

    --> AM: check!


  - contradiction between first paragraph and later?

    --> AM: what?


Reviewer 5:

    --> MS: Rebut: why did you even bother? :-)


