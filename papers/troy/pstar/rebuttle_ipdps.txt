Given that more than two Billion dollars of science every year depends
upon Pilot-Jobs, the lack of a theoretical framework to analyze,
compare and contrast Pilot-Jobs-- even if incomplete or approximately,
is a serious issue. 

[Do we really want to throw in this 2B number? (MS)] [without a
reference? (AM)] [Will change from $ to many projects as done in the
paper (SJ)]

The primary contribution of this paper is the formulation of an
abstract model (P*) of Pilot-Jobs and the demonstration that, (i) P*
is minimally complete but extensible along multiple dimensions, e.g.,
supports the common treatment of dynamic compute jobs and dynamic
data, (ii) validation of P* model and its implementation (TROY) by
demonstrating the use of TROY across different PJ frameworks (with
otherwise very different compute-job semantics) (iii) demonstration of
concurrent interoperability of multiple PJ frameworks across different
infrastructures, when used via TROY API.

To the best of our knowledge, (iii) has not been demonstrated; this is
consistent with known barriers to interoperability... Thus, we believe
(i), (ii) and (iii) are all unique contributions of this work.

[Use the above to reinforce the message/value of the model (SJ)]

We acknowledge the reviewers observation of less than perfect
language, clarity and expressions. We believe these can be addressed
in a day's work. As an example of such improvements, to aid clarity --
conceptual and expression, we will (i) [SJ: Please include specific
remark about PJ in Introduction] (ii) distinguish between the TROY API
and the TROY Runtime-system.  Further, we will improve the motivation
for the P* model in section II.  In general, these can only serve to
enhance and convey the technical message and impact better.

In spite of the constructive suggestions and observations of the
reviewers, we strongly believe the following statements are factually
inaccurate in the reviews:

[Two comments: (i) Please quote the statement we are claiming as
factually incorrect (ii) ite where in the text this done (SJ)].

* "No Related Work": Although we do not call it "Related Work" we have
 an entire section, where e present and contrast different related
 Pilot-Job implementations in section 4.  Also, we have upwards of 20
 external references (<30% self-refs).

* Performance overhead of TROY: We have shown that the overhead
introduced by the TROY layer is negligible in particular when
considering the benefits of the ability to utilize different
infrastructure concurrently. Also, we demonstrated the scalability of
TROY to larger number of cores.

* Relation to SAGA?  There is no comparison to SAGA, but rather a
statement of shared origin and concepts, and a statement that BigJob
is a SAGA-based pilot job implementation.  [SJ: Explain "SAGA-based"
implementation i.e., common job-model etc. An extended "package?" i.e.
resource management package?]

* "The paper does not present new implementations and techniques":
This paper presents the implementation of (i) the TROY framework
consisting of the API a unique runtime system that supports the
concurrent execution of multiple PJs, and (ii) the DIANE and BigJob
adaptor.  Further, we (iii) significantly enhanced the BigJob
framework: BigJob was extended to support different distributed
cyberinfrastructure, as well as different internal communication &
coordination systems. In section V we extensively analyze the
performance characteristics of these BigJob extensions, which
demonstrates that existing M-W coordination approach can/will scale to
required number of jobs/WUs. [Please check above paragraph (SJ)].

Finally, we believe some suggestions by referees points to future
work: The work currently focuses on Pilot-Jobs, we will extend both
the theoretical framework and implementation toward data. Further, we
will explore application-level data/compute scheduling as well as
heuristics for dynamic execution. Also, we will investigate the
performance of the unified TROY implementation of Pilot-Jobs and
Pilot-Data.


**********
Please make changes above this

**********


I. Overview/Opening saying what we think is/are the unique
contribution(s) of the paper.

Given that more than two Billion dollars of science every year depends
upon Pilot-Jobs, the lack of a theoretical framework to analyze,
compare and contrast Pilot-Jobs-- even if incomplete or approximately,
is a serious issue.
 
The primary contribution of this paper is the formulation of an
abstract model (P*) of Pilot-Jobs and the demonstration that, (i) P*
is minimally complete but extensible along multiple dimensions, e.g.,
supports the common treatment of dynamic compute jobs and dynamic
data, (ii) validation of P* model and its implementation (TROY) by
demonstrating the use across different PJ implementations (with
otherwise very different compute-job semantics) (iii) demonstration of
interoperability of different PJ implementations across different
infrastructures.

To the best of our knowledge, (ii) has not been demonstrated; this is
consistent with known barriers to interoperability, Thus, we believe
(i), (ii) and (iii) are all unique contributions of this work.


%P* model that supports and promotes



II. Acknowledge short comings, and address how we will fix them. Does
not have been specific but could be high-level.  Important to point
out how in-spite of shortcomings main contributions as in I are not
undermined, i.e., shortcomings can be/will be fixed (i.e.  urge
shepherding). [SJ/AL]

- motivation for P* model: We will improve the motivation for section II.

- respectfully claim that TROY overhead have been addressed

- only covers Pilot-Jobs => future work: pilot-data, scheduling,
  etc. Application to data has never been done before

- no related work: we have references (<30% self-refs); section 4 is a
discussion of related work [factual inaccuracy]

- experiments Pilot-Data => future work

III. Address Specific comments, and either dispute or
acknowledge. Either way focus on how we will improve the individual
sections.

In turn, each section in III has two tracks: (i) why we agree/disagree
with specific comments, (ii) How we will improve individual sections,
either taking into account the referee remarks or of our own volition
(again making the case for shepherding).

Section 1: SJ
Section 2: AL

- provide a better motivation for P* model: 

(i) It is important to understand 
properties and characteristics (SJ: properties and characteristics of what? PJs?)
in order to map application requirements, available infrastructure and PJ framework. 
(map to what?)

(ii) Provide a simple API for the core functionality, hide the complexity and semantic heterogeneity 
of different PJ implementations.

- Describe the benefits of interoperability using a real-life
scenario, e.g. XSEDE - EGI interoperability.

- highlight the sub-set of functionality of PJ frameworks that is exposed via 
TROY and give an example of what kind of functionality is hidden (different, 
complex resource specifications, specific initialization and termination
commands,...).

Section 3: MS 

SJ: Would it help to highlight the difference between the API and the
"runtime-system" aspects of TROY. P* and TROY-API provide conceptual
uniformity; the TROY runtime-system provides interoperability.

Improve descriptions of TROY and PJ framework relationship and on how
TROY implements the P* characteristics.

TROY is an implementation of the P*-model, and therefore also "minimal
but complete" by design.  Reviewer C questions the incentive to
"implement everything over TROY" related to performance.  As TROY
builds on top of other systems, in the single backend scenario, one
should not expect a performance increase. We show that the performance
overhead is negligible though.  [SJ: "we show" or "we have shown" ?]

The real advantage in using TROY is the 
concurrent execution of jobs on multiple pilot-job implementation
backends. We would like to explicitly mention that this also enables
the concurrent execution of jobs on (totally) different
infrastructures.  

TROY will not "implement the characteristics" of the
various Pilot-Jobs, it rather exposes these characteristics in a
unified way, without the need to change an application when switching
infrastructure.

We take the comments from the reviewers as a sign that we need to
explain these benefits more clearly.

Section 4: OW
No explicit reviewer comments regarding secion 4 - PJ Frameworks. 

Some general remarks:
  I think the word IMPLEMENTATION is used far to often throughout this 
  paper (45 times to be exact). Everything is an implementation of 
  an implementation of a model of an abstraction... or something like that ;-)

  grep -v '^%' pstar-ipdps2011.tex  | grep "implementation" | wc -l  
  40
  grep -v '^%' pstar-ipdps2011.tex  | grep "Implementation" | wc -l
  5

  E.g., "BigJob: A SAGA-based Pilot-Job Implementation for TROY".
  Why not call it "A SAGA-based Pilot-Job for TROY"? 

SJ: So the rebuttal point is what? We will address confusing and over-use
of implementation in different contexts.? 

Section 5: AL (with MS and AM)
- extended evaluation of TROY overhead and break down to sub-components 
(startup time, coordination, ...)

- present experiments of PD

Section 6: AL
- present use cases (coupling data-compute, discovery, in-situ access, 
streaming) of PD (see D3MA)
- present API of PD
- present experiments of PD

Section 7: SJ

