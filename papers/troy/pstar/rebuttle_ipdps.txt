Given that a significant number of small-scale and large-scale science
(ATLAS/CERN, LIGO) projects critically depend upon Pilot-Jobs (PJ),
the lack of a theoretical framework -- even if partial, to understand
and analyze PJs is a serious issue.

The primary contribution of this paper is the formulation of an
abstract model (P*) of PJ and demonstration that, (i) P* is minimally
complete but extensible along multiple dimensions, e.g., supports the
common treatment of dynamic compute jobs and dynamic data, (ii)
validation of P* model and its implementation (TROY) by demonstrating
the use of TROY across different PJ frameworks (with otherwise very
different compute-job semantics), (iii) demonstration of concurrent
interoperability of multiple PJ frameworks across different
infrastructures, when used via TROY API.

In principle, comparing different PJ frameworks is possible without a
well-defined underlying model (as Reviewer C suggests), however sans
such a model, contribution (i) (i.e., unification of dynamic compute
and data) would not be possible, and (ii) and (iii) would not be
scalable and/or extensible.  In fact (iii) has not been demonstrated,
which is consistent with known barriers to interoperability and lack
of an underlying theoretical framework.

P* is minimally complete and is itself extensible, thus providing a
unifying framework for existing but disjointed PJ efforts, as well as
emerging PJ frameworks.  P*/TROY provides a scalable approach ---
scale-up and out, and across different frameworks.

We acknowledge the reviewer's observation of less than perfect
language and clarity. These will be addressed. As examples of
improvements in clarity -- conceptual and expression, we will: (i)
provide greater context and details of PJ earlier in the paper, and
thus also improve the motivation for the P* model in section II, and
(ii) distinguish between the TROY API and the TROY runtime-system.

In spite of the constructive suggestions and observations of the
reviewers, we strongly believe the following statements are factually
inaccurate in the reviews:

* "..There is no related work section":

Although not called "Related Work" we have an entire section, where we
present and contrast existing and related PJs in Sec. 4.  Also, we
have upwards of 20 external references (<25% self-refs) to relevant
and related work.

* ".. it seems that the proposed solution is yet another software
 layer that will generate execution overhead..":

We have shown (Fig. 7) that the overhead introduced by the TROY layer
is negligible -- both absolute and relative to the benefits arising
from the ability to utilize different infrastructure
concurrently. Also, we demonstrate the scalability of TROY.

* " .. still do not see the real interest when compared to SAGA..":

We do not compare to SAGA, but allude to the shared concepts (e.g.,
builds upon existing SAGA job-model etc.) and a statement that BigJob
is a SAGA-based PJ implementation.

* "The paper does not present new implementations and techniques":

This paper presents the implementation of a novel framework -- TROY,
consisting of an API and a runtime system that supports the concurrent
execution of multiple PJs, and associated DIANE and BigJob adaptors.
Further, we significantly enhanced the BigJob framework to support
different distributed cyberinfrastructure, as well as different
internal communication & coordination systems (Sec V). The later is
important to validate that existing M-W coordination approach can/will
scale to required number of jobs/WUs.

Finally, we believe some suggestions by referees points to future
work: This work currently focuses on PJ; we will extend both
the theoretical framework and implementation to data. Further, we
will explore application-level data/compute scheduling as well as
heuristics for dynamic execution. Also, we will investigate the
performance of the unified TROY implementation of PJs and
Pilot-Data.

**********
Please make changes above this

**********


I. Overview/Opening saying what we think is/are the unique
contribution(s) of the paper.

Given that more than two Billion dollars of science every year depends
upon Pilot-Jobs, the lack of a theoretical framework to analyze,
compare and contrast Pilot-Jobs-- even if incomplete or approximately,
is a serious issue.
 
The primary contribution of this paper is the formulation of an
abstract model (P*) of Pilot-Jobs and the demonstration that, (i) P*
is minimally complete but extensible along multiple dimensions, e.g.,
supports the common treatment of dynamic compute jobs and dynamic
data, (ii) validation of P* model and its implementation (TROY) by
demonstrating the use across different PJ implementations (with
otherwise very different compute-job semantics) (iii) demonstration of
interoperability of different PJ implementations across different
infrastructures.

To the best of our knowledge, (ii) has not been demonstrated; this is
consistent with known barriers to interoperability, Thus, we believe
(i), (ii) and (iii) are all unique contributions of this work.


%P* model that supports and promotes



II. Acknowledge short comings, and address how we will fix them. Does
not have been specific but could be high-level.  Important to point
out how in-spite of shortcomings main contributions as in I are not
undermined, i.e., shortcomings can be/will be fixed (i.e.  urge
shepherding). [SJ/AL]

- motivation for P* model: We will improve the motivation for section II.

- respectfully claim that TROY overhead have been addressed

- only covers Pilot-Jobs => future work: pilot-data, scheduling,
  etc. Application to data has never been done before

- no related work: we have references (<30% self-refs); section 4 is a
discussion of related work [factual inaccuracy]

- experiments Pilot-Data => future work

III. Address Specific comments, and either dispute or
acknowledge. Either way focus on how we will improve the individual
sections.

In turn, each section in III has two tracks: (i) why we agree/disagree
with specific comments, (ii) How we will improve individual sections,
either taking into account the referee remarks or of our own volition
(again making the case for shepherding).

Section 1: SJ
Section 2: AL

- provide a better motivation for P* model: 

(i) It is important to understand 
properties and characteristics (SJ: properties and characteristics of what? PJs?)
in order to map application requirements, available infrastructure and PJ framework. 
(map to what?)

(ii) Provide a simple API for the core functionality, hide the complexity and semantic heterogeneity 
of different PJ implementations.

- Describe the benefits of interoperability using a real-life
scenario, e.g. XSEDE - EGI interoperability.

- highlight the sub-set of functionality of PJ frameworks that is exposed via 
TROY and give an example of what kind of functionality is hidden (different, 
complex resource specifications, specific initialization and termination
commands,...).

Section 3: MS 

SJ: Would it help to highlight the difference between the API and the
"runtime-system" aspects of TROY. P* and TROY-API provide conceptual
uniformity; the TROY runtime-system provides interoperability.

Improve descriptions of TROY and PJ framework relationship and on how
TROY implements the P* characteristics.

TROY is an implementation of the P*-model, and therefore also "minimal
but complete" by design.  Reviewer C questions the incentive to
"implement everything over TROY" related to performance.  As TROY
builds on top of other systems, in the single backend scenario, one
should not expect a performance increase. We show that the performance
overhead is negligible though.  [SJ: "we show" or "we have shown" ?]

The real advantage in using TROY is the 
concurrent execution of jobs on multiple pilot-job implementation
backends. We would like to explicitly mention that this also enables
the concurrent execution of jobs on (totally) different
infrastructures.  

TROY will not "implement the characteristics" of the
various Pilot-Jobs, it rather exposes these characteristics in a
unified way, without the need to change an application when switching
infrastructure.

We take the comments from the reviewers as a sign that we need to
explain these benefits more clearly.

Section 4: OW
No explicit reviewer comments regarding secion 4 - PJ Frameworks. 

Some general remarks:
  I think the word IMPLEMENTATION is used far to often throughout this 
  paper (45 times to be exact). Everything is an implementation of 
  an implementation of a model of an abstraction... or something like that ;-)

  grep -v '^%' pstar-ipdps2011.tex  | grep "implementation" | wc -l  
  40
  grep -v '^%' pstar-ipdps2011.tex  | grep "Implementation" | wc -l
  5

  E.g., "BigJob: A SAGA-based Pilot-Job Implementation for TROY".
  Why not call it "A SAGA-based Pilot-Job for TROY"? 

SJ: So the rebuttal point is what? We will address confusing and over-use
of implementation in different contexts.? 

Section 5: AL (with MS and AM)
- extended evaluation of TROY overhead and break down to sub-components 
(startup time, coordination, ...)

- present experiments of PD

Section 6: AL
- present use cases (coupling data-compute, discovery, in-situ access, 
streaming) of PD (see D3MA)
- present API of PD
- present experiments of PD

Section 7: SJ

