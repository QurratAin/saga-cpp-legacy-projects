AllPairs - Test for Royal Paper.
*didn't change much (5 seconds up or down between tests*
8x8 each chunk 287 megabytes CloudStore 1 machine 1 worker = 3 min 49 secs
8x8 each chunk 287 megabytes CloudStore 1 machine 2 worker = 2 min 22 secs
8x8 each chunk 287 megabytes CloudStore 1 machine 4 worker = 2 min 18 secs
8x8 each chunk 287 megabytes CloudStore 1 machine 8 worker = 2 min 19 secs
 >What we see here is there is an obvious cap for reading data using CloudStore.  No
  matter how many workers we add we see it stops decreasing.  This could be due
  to time to read a file in CloudStore.  Adding more workers doesn't stop because a
  singl test takes around 2 minutes and there are 8 workers and 8 sets of files
  to compare.

16x16 each chunk 287/2 megabytes CloudStore 1 machine 1 worker = 2 min 19 secs

time to cat CloudStore file 287 megabytes = 2.984 seconds
time to cat local file 287 megabytes=1.760 seconds
 >This difference is acceptable because there is a program in between the fs
 and accessing data.  It must figure out where the data is (even though it is
 only on this machine).  Also, it must keep track of file locks and chunk the
 file back together (it is stored internally as chunks).

64 comparisons = 78.336 more seconds extra read time

============2 chunservers================
(Note this has a replication value of 2 so each machine did have the data
locally) *This test has higher variance* (last test were all about 1/3 of this)
[Must not access all local trying to minimize bottleneck, but network was bottleneck]
[Network speeds variant?]
8x8 each chunk 287 megabytes CloudStore 2 machine 1 worker = 25 min 21 secs
8x8 each chunk 287 megabytes CloudStore 2 machine 2 worker = 13 min 25 secs
8x8 each chunk 287 megabytes CloudStore 2 machine 4 worker = 07 min 11 secs
8x8 each chunk 287 megabytes CloudStore 2 machine 8 worker = 04 min 41 secs

16x16 each chunk 287/2 megabytes CloudStore 2 machines 1 worker = > 3 of 16 took 8 minutes
 >It is estimated to have taken about 40 minutes to run.  It must be slow for
  CloudStore to open files and find where they are.  This is significantly slower than
  I anticipated.  To read 8 files with only one chunkserver took 2 min 19
  seconds.  This test could be wrong, and I should retry before saying this as
  truth. TODO: Redo this test

time to cat CloudStore file 287 megabytes = 4.754 seconds
time to cat file 287 megabytes=1.196 seconds
 >It is only fair to think that as the filesystem's number of machines grows
  that this time would grow.  It must access more machines and wait longer
  because of network traffic and download time. (Note: I'm not sure where the
  file was.  There may have been no downloading involved (local)  TODO: I
  should take an average.)

64 comparisons = 227.712 more seconds extra read time


============1 chunservers================
(Note this has a replication value of 1 and all files are remote) [worked on
eric and data on louie]

8x8 each chunk 287 megabytes CloudStore 2 machine 1 worker = 31 minutes 21 seconds
8x8 each chunk 287 megabytes CloudStore 2 machine 2 worker = 16 minutes 17 seconds
8x8 each chunk 287 megabytes CloudStore 2 machine 4 worker =  8 minutes 41 seconds
8x8 each chunk 287 megabytes CloudStore 2 machine 8 worker =  6 minutes 10 seconds

16x16 each chunk 287/2 megabytes CloudStore 2 machines 1 worker = > 

time to cat CloudStore file 287 megabytes = 16.104 seconds
time to cat file 287 megabytes     = 1.947 seconds

64 comparisons = 906  more seconds (15 min 6 secs) extra read time


============2 chunservers (distributed workers)================ (eric1, louie1)
Up servers: 2
s=208.100.73.21, p=30000, total=1107.84(GB), used=2.23517(GB), util=0.201759%, nblocks=40, lastheard=59 (sec), ncorrupt=0, nchunksToMove=0
s=208.100.69.21, p=30000, total=970.862(GB), used=2.23517(GB), util=0.230226%, nblocks=40, lastheard=59 (sec), ncorrupt=0, nchunksToMove=0

(Note this has a replication value of 2 so each machine did have the data locally)
8x8 each chunk 287 megabytes CloudStore 2 machine 2 worker (1 on each machine) = 7 min 39 seconds
8x8 each chunk 287 megabytes CloudStore 2 machine 4 worker (2 on each machine) = 5 min 27 seconds
8x8 each chunk 287 megabytes CloudStore 2 machine 8 worker (4 on each machine) = 2 min 20 seconds

(Change replication factor to 1)
Up servers: 2
s=208.100.73.21, p=30000, total=1107.63(GB), used=0.83819(GB), util=0.0756745%, nblocks=15, lastheard=30 (sec), ncorrupt=0, nchunksToMove=0
s=208.100.69.21, p=30000, total=970.778(GB), used=1.39698(GB), util=0.143904%, nblocks=25, lastheard=30 (sec), ncorrupt=0, nchunksToMove=0
genome-0 - on 208.100.73.21
genome-1 - on 208.100.73.21
genome-2 - on 208.100.73.21

genome-3 - on 208.100.69.21
genome-4 - on 208.100.69.21
genome-5 - on 208.100.69.21
genome-6 - on 208.100.69.21
genome-7 - on 208.100.69.21

(replication value of 1 so each machine may/may not have data)
8x8 each chunk 287 megabytes CloudStore 2 machine 2 worker (1 on each machine) = 7 min 25 seconds
8x8 each chunk 287 megabytes CloudStore 2 machine 4 worker (2 on each machine) = 5 min 36 seconds
8x8 each chunk 287 megabytes CloudStore 2 machine 8 worker (4 on each machine) = 3 min 11 seconds
 >Not sure why this performed slightly better, probably statistical fluke.  I
 suppose that CloudStore is really good at handling data requests.
