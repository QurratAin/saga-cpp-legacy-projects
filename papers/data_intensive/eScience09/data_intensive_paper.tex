%\documentclass[times, 10pt, twocolumn]{article}
%\documentclass[conference,final]{IEEEtran}
     
\documentclass{rspublic}   

%------------------------------------------------------------------------- take
%the % away on next line to produce the final camera-ready version
%\pagestyle{empty}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}
\usepackage{multirow}
\usepackage{listings}
\usepackage{times}
\usepackage{paralist}
\usepackage{wrapfig}
\usepackage[small,it]{caption}
\usepackage{multirow}
\usepackage{ifpdf}
\usepackage{subfig}
%\usepackage[pdftex]{graphicx}
%\usepackage{harvard}
%\usepackage{pdfsync}
%\usepackage{subfigure}

%Bibliography     
\usepackage{natbib}                
\usepackage{listings}
\usepackage{keyval}
\usepackage{color}
\definecolor{listinggray}{gray}{0.95}
\definecolor{darkgray}{gray}{0.7}
\definecolor{commentgreen}{rgb}{0, 0.4, 0}
\definecolor{darkblue}{rgb}{0, 0, 0.4}
\definecolor{middleblue}{rgb}{0, 0, 0.7}
\definecolor{darkred}{rgb}{0.4, 0, 0}
\definecolor{brown}{rgb}{0.5, 0.5, 0}

\lstdefinestyle{myListing}{ frame=single, backgroundcolor=\color{listinggray},  
  %float=t,
  language=C,       basicstyle=\ttfamily \footnotesize, breakautoindent=true,
breaklines=true tabsize=2, captionpos=b, aboveskip=0em,
  %numbers=left, numberstyle=\tiny
}      

\lstdefinestyle{myPythonListing}{ frame=single,
backgroundcolor=\color{listinggray},  
  %float=t,
  language=Python,       basicstyle=\ttfamily \footnotesize,
breakautoindent=true, breaklines=true tabsize=2, captionpos=b,  
  %numbers=left, numberstyle=\tiny
}

\title[Understanding Performance Implications of Distributing Data for
Data-Intensive Applications]{Understanding Performance Implications of
Distributing Data for Data-Intensive Applications}


\author[Miceli, Miceli, Rodriguez-Milla, Jha]{ Christopher Miceli$^{1}$,
Michael Miceli$^{1}$, Bety Rodriguez-Milla$^{1}$, Shantenu Jha$^{1,2,*}$ \\
\small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State
University, USA}} \\  \small{\emph{$^{2}$Department of Computer Science,
Louisiana State University, USA}} \\ {\footnotesize {\hspace{0.0 in}
$^*$Corresponding Author sjha@cct.lsu.edu}} }

%\date{}

\def\acknowledgementname{Acknowledgements} \newenvironment{acknowledgement} 

% {\section*{\acknowledgementname}%\parindent=0pt% }

\newif\ifdraft \drafttrue \ifdraft \newcommand{\fixme}[1]{ { \bf{ ***FIXME: #1
}} } \newcommand{\jhanote}[1]{ {\textcolor{red} { ***Jha: #1 }}}
\newcommand{\micnote}[1]{ {\textcolor{blue} { ***Michael: #1 }}} \else
\newcommand{\jhanote}[1]{} \newcommand{\micnote}[1]{} \newcommand{\fixme}[1]{}
\fi

\begin{document} \maketitle

\micnote{This can't be more than 200 words.  The summary should be concise and
informative.  It should be complete by itself, and must not contain references
or unexplained abbreviations.  It should not only indicate the general scope of
the article but also state the main results and conclusions.  Please note that
footnotes are not used.}

\begin{abstract}{data-intensive computing, distributed computing, cloud
computing, grid computing} Grids, clouds and cloud-like infrastructures are
capable of supporting large problems.  While the capability of these systems is
great, unique performance issues appear as data-sets become extremely large.
As the volume of data increases, scalable placement and management techniques
are required.  Often, it is hard to predict where to place data to where it
minimizes data transfer.  This paper uses two techniques to manage data
placement.  One focuses on data placement and the other on worker placement.
Distributed Filesystems (DFS) focus on data placement.  They simplify the
management of distributed data by providing a single access protocol and a
common name-space, and encapsulating data placement issues.  Although this
simplifies the management of data, there are potential performance trade-offs.
Users can no longer control data placement, which possibly has performance
implications.  Another approach is for the work to be aware of the data and
adjust accordingly. This technique can move the work to the data or vice-versa
has different performance issues than a DFS.  The goal of this paper is to
understand techniques for distributing data in a distributed environment and
understand performance issues associated with these
techniques.\micnote{conclusion needs to go here *can be 12
words*}\end{abstract}

\section{Introduction} Data-intensive computing is a fast growing area of
computer science.  A good example of this is Google, which processes around 20
petabytes of data per day ~\citep{google}, and trends show continuing growth.
It has become very important that a distributed application developer takes
precautions when placing, scheduling and managing large volumes of data.
Careless placement can adversely affect system performance greatly.  There are
two ways to handle this issue: distributed filesystems, which focus on data
placement, or an intelligent framework, which focuses on worker placement.

A distributed filesystem (DFS) controls the data placement and provides a
uniform interface for accessing files.  Frequently, there is more than one copy
of the input data for fault-tolerance reasons, then the added issue of deciding
between the two or more replicas becomes relevant.  While a DFS removes the
responsibility of replica management and data server placement, the abstraction
often increases the difficulty in determining where in the DFS the data is
being stored difficult.  This puts pressure on a DFS's protocols and internal
algorithms to perform well.  Despite this, the DFS replication may alleviate
this issue by placing replicas in locations where computational resources
reside.

When working with distributed systems and large data-sets together, determining
whether to move input data to the computational resource, or the computational
workload to the input data becomes very important.  A downfall of DFSs is the
inability to make this decision.  It can only focus on minimizing poor data
management.  Another method is determining where data is and where the work
should go.  This is the opposite mode of thinking than a DFS.  Determining data
location can be as simple as looking at the IP address of the worker and seeing
geographically where it is located or as complicated as using network analyses
tools to determine the optimal data transfer minimization time.

Our research focused on comparing these two techniques and managing how they
handle different data-sets and what there performance patterns were.  We use an
implementation of All-Pairs in SAGA (Simple API for Grid Applications).
All-Pairs is an abstraction for creating a $MxN$ comparison matrix.  This
matrix compares all possible permutations of pairs in a set and creates a
matrix for evaluation.  SAGA is ...

If the question is ``To distribute data or not, is the question'', we provide
some initial approaches to answer this question.  orker network performance.


%There are at least two types of data-intensive applications: the first where
%the actual data generated is large; the second type is where the data
%generated is small, but the volume of data on which computation occurs is very
%large.  The application we used, has relatively small input and relatively
%small output, but the manner of processing causes many data reads.  This type
%of application can be classified as having a large data throughput.
%\jhanote{Can you elaborate on different types of data-intensive applications?
%What kind is an ImageMagic based application?}

It has large input $O$(GB) and relatively small output $O$(KB).  However, the
manner of processing causes many data reads.  This type of application can be
classified as having a large data throughput.  To handle seamlessly the DFS and
gridFTP based data stores, the application uses the Simple API for Grid
Applications (SAGA) ~\citep{saga_web}.  This allows the same exact application
to be used for all of our experiments.  The result of this application is
stored in a matrix.  The application spawns distributed jobs to run sets of
these pairs.  The problem becomes determining which pairs to put into a set,
and with which distributed resource to run that set.  If transferring data to
the job takes too long, we spend more time on data management than computation.
There may be a resource capable of the work that may be slower than others, but
network-close (able to be accessed in a relatively quick manner via the network
supporting the distributed system) enough to the data to make up for its lack
of computational ability.  In our experiments, we used CloudStore (formerly
KFS), an open-source high performance distributed filesystem that builds upon
ideas from Google's distributed filesystem GFS ~\citep{cloudstore_web}.
CloudStore was chosen for its high performance focus, C++ implementation, and
its source code availability.

\section{SAGA}

\section{All-Pairs} In this paper, we use an application based upon a
grid-enabled All-Pairs abstraction, which applies an operation on two data-sets
such that every possible pair containing one element from the first set and one
element from the second set has a some operation applied to it.~\citep{Interop,
AllPairs}  Essentially, All-Pairs is a function of two sets which creates a
matrix where each element is the result of the operation applied to that
element's row and column.  \begin{equation} All-Pairs(A, B, function\ F)
\rightarrow M_{|A|X|B|}\ |\ M_{i,j} = F(A_{i},B_{j}) \end{equation} Examples of
problems that fall into this category are image comparison for facial
recognition, and genome comparison.  This paper uses genome comparisons to find
the best matching gene in a genome. \micnote{I think we should have a small
graphic here to show the matrix and how it works}

\section{CloudStore} With the advent of several stable open source distributed
filesystem (DFS) projects (motivated in part, by developments in cloud
computing), which can be deployed without explicit vendor support are now
useful and effective tools to consider for data-intensive scientific
applications.  CloudStore is a DFS written in C++ released under the Apache
License Version 2.0.  It is based off the highly successful Google Filesystem,
which is closed source and unavailable for research.  A DFS provides a means to
access data on multiple hosts from a common interface.  CloudStore also
provides a means to automatically replicate data on different hosts to provide
efficient data access and fault tolerance.  The most common parameters in
determining the performance of using a DFS are the performance overhead
compared to a normal local filesystem, number of replicas of each data/file,
and the number of servers.  One goal of this paper is understand the
performance trade-offs of a DFS compared to ''regular'' distribution and
placement techniques as well as more advanced intelligent distribution methods.
Also, this papers aims to determine how sensitive the performance is in the
context of a real data-intensive distributed applications.

\section{GridFTP} The Globus Toolkit is an open source software toolkit
released under the Apache License version 2.0.  It provides tools used to
create and manage grid infrastructures.  GridFTP is a tool provided Globus that
is used to transfer files across machines in a grid.  It is specifically
designed for high-bandwidth networks.

\section{Performance Measurement and Analysis} We developed three types of
experiments in order to compare distributed filesystems with manual file
management.  Does a distributed filesystem grow more slowly than manual
placement of data?  When manually handling data, what are the advantages of
being able to move work to data to the work?  We focused on three variables to
measure data:  degree of distribution, data dependency, and workload.

\subsection{Experimental Configuration}

The following experiements can be described by a tuple of the following form
(\textbf{d}, \textbf{c}, \textbf{dc}, \textbf{fs}, \textbf{m}, \textbf{r})
where

\begin{itemize} 
\item \textbf{d} is the total amount of data in each set
\item \textbf{c} is the number of elements in each set
\item \textbf{dc} is a comma seperated list of pairs describing the
capabilities of each machine involved in the experiment
\item \textbf{fs} is the type of filesystem used
\item \textbf{m} is the method used to access that filesystem
\item \textbf{r} is the amount of replication utilized in the experiment
\end{itemize}

Define C as  tuple of (i) Chunk Size (number), (ii) Infrastructure,
(iii) FS configuration, (iv) Data-Access protocol.

Mention for CloudStore we also need to include a replication factor.

\subsubsection{Modeling the Time to Completion}

$T_c = T_x + T_{I/O} + T_{compute}$

\subsection{Base line Performance: Experiment I} In the first experiment, we run the
SAGA-based All-Pairs application on up to 4 machines on a Grid (LONI),
without any specific data placement strategy; also, no replication or
fault-tolerance takes place.  The application sequentially assigns
data to the first available computational resource, and all data is
accessed via the gridFTP protocol.
% \includegraphics[width=\textwidth]{data/graphs/ConventionalFigure.eps}
% \includegraphics[width=\textwidth]{data/graphs/LocalFigure.eps}

\includegraphics{data/graphs/CloudStoreFigure}
\includegraphics{data/graphs/CloudStoreNoComputeSmallerDataSet}
\includegraphics{data/graphs/ConventionalFigure}
\includegraphics{data/graphs/IntelligentExtremes}
\includegraphics{data/graphs/IntelligentVsConventionalFigure}
\includegraphics{data/graphs/LocalFigure}
\includegraphics{data/graphs/StagingAsAPortionOfIntelligenceFigure}

\subsection{with Computation}
The previous experiments had no computation involved.  This was to
evaluate data dependencies without the added variable of computation.
In these tests we added computation to the equation.

\jhanote{We need data for compute (comparision) and I/O (only) for
different data-set sizes} \micnote{We need data for three and four
machines (just one graph going from 1 machine to four machines}

%Staging experiment
\subsection{Experiment II} The second experiment is similar to the
first, except the All-Pairs application takes the data's location into
consideration when determining whether or not to assign a certain
data-set to an idle job.  Inspired by earlier work~\citep{netperf},
this version of the application performs an extra step that determines
the performance of the network by pinging hosts that may be involved,
and utilises this information when deciding which data-set to assign
to a job requesting work.  Though not very sophisticated, it is a
first-approximation to performance model aware data-placement strategy.
This approach knows where the files are located and then determines
whether to move the work to where the data is, or move the data to the
work.  \jhanote{Data-aware placement is also required, i.e., managing
location of files.}

We introduce the idea of network-closeness.  A network-close data-set
takes a small amount of time to transfer to the work location.  A
network-far data-set is just the opposite.  A network-far data-set takes
a long time to transfer to the work location.  If there is an
unprocessed data-set collocated or network-close with the job, then the
assignment of that worker to that data-set would have benefits.  If
there is no unprocessed data-set that is network-close to the job, still
we assign data that may be network-far, in case the network-close job
failed or there is no available jobs network-close to the data-set.

%\includegraphics[width=\textwidth]{data/graphs/IntelligentVsC.pdf}

\subsection{Experiment III} The third experiment provides information
into DFSs performance in handling data locality issues.  The same
All-Pairs application as in Experiments 1 and 2 is used, except all data
is stored in the distributed filesystem CloudStore under various
configurations.  Some variables include number of data server that store
data, replications value for data in these data servers, and as above,
placement and number of computational resources.  All read and writes
also utilise the distributed filesystem.  Since the performance of this
test is dependent on where the DFS stores data relative to the
computational resources of the system, we place block-servers on every
machine that may be capable of performing work.  This places all
responsibility on the DFS in determining where to place data.
% \includegraphics[width=\textwidth]{data/graphs/CloudStoreFigure.pdf}
% \includegraphics[width=\textwidth]{data/graphs/CloudStoreComputeFigure.pdf}

\section{Conclusion} Our results show that a DFS greatly changes the
performance of a distributed application in a positive manner.  Our
experiments that utilised the DFS to access and store data outperformed
their gridFTP counterparts by an order of magnitude in most cases. Our
results also indicate that the DFS scales better as file sizes and
number of files grow, although both seem to scale linearly.  Before any
conclusions may be drawn, there are issues that need to be addressed.
Our application utilised SAGA to access DFS based and gridFTP based
files.  Although there is clearly a SAGA induced overhead, this overhead
is constant.  \jhanote{ We need to discuss performance issue: Ole has
performance numbers that contradict this, i.e., the overhead that SAGA
introduces for file/gridFTP is very small compared to native globus
calls.  This may be due to SAGA's adaptive nature, where lots of
computation is spent determining how to make a distributed call.  Other
reason's may be the gridFTP SAGA adaptor being separated from the
application, being able to make only general decisions, allowing no
performance tweaks.  Accessing files through this abstraction
with gridFTP seemed to perform sub-optimally in comparison to using the
globus tools directly.} In addition, we were unable to utilise our
entire distributed system, using at most 8 jobs to handle our work.
With a replication level of two in the DFS, data was almost certainly
co-located with the computational resource.  In the second experiment,
utilizing information from first staging the network did improve upon
the results of the naive first experiment, but still did not approach
the DFS's performance levels.

Distributed filesystems are important abstractions for a data-intensive
distributed application developers to consider.  It also appears that
staging is worth the time required to build a graph representing the
network.  Also to note, the second experiment is also naive in the way
that it attempts to optimise data and work assignments.  Our staging
only performed pings, not data transfer trials or reliability tests.  A
job could have low latency, but poor bandwidth.  Perhaps CloudStore's
performance can be attributed to recent work that has shown that data in
large scale distributed applications tends to be accessed together,
despite being seemingly unrelated in the input data-set.  Such
correlation in data-access has been observed elsewhere, and specific
abstractions to support the access of ``aggregation of such files'' has
been referred to as a filecule, an application specific group of
files~\citep{filecule}.  Attempting to determine if analogous
abstractions could enhance performance for the All-Pair application
could be interesting.  In a DFS, however, if the data store is also
capable of data processing, then the DFS is placing commonly used files
together on machines needing them for work; in essence, the DFS is
finding these groups for the developer.  The fault tolerance, for which
distributed filesystems are already well renowned for, also has added
benefits to grid application developers in terms of performance.  The
distributed application does not have to be aware of where data has been
copied to previously when assigning work; the distributed filesystem
uses the best replica when data is being accessed.

\fixme{This should be changed to be appropriate \bf{**Shantenu**}}
{\bf Acknowledgment:} Important funding for SAGA has been provided by
    the UK EPSRC grant number GR/D0766171/1 (via OMII-UK) and HPCOPS
    NSF-OCI 0710874. SJ acknowledges the e-Science Institute,
    Edinburgh for supporting the research theme, ``Distributed
    Programming Abstractions'' and theme members for shaping many
    important ideas. This work has also been made possible thanks to
    the internal resources of the Center for Computation \& Technology
    at Louisiana State University and computer resources provided by
    LONI. 

%\bibliographystyle{IEEEtran}
\bibliographystyle{kluwer} 
\bibliography{data_intensive_paper}
\end{document}
