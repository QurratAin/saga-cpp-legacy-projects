%\documentclass{article}
\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}
\usepackage{multirow}
\usepackage{listings}
\usepackage{times}
\usepackage{paralist}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[hypertex]{hyperref}
\usepackage{subfigure}
\usepackage{color}

%\documentclass{rspublic}

\usepackage{ifpdf}

\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\BI}[1]{\textbf{\textit{#1}}}
\newcommand{\T}[1]{\texttt{#1}}

\pdfpagewidth 8.5in
\pdfpageheight 11in 

\setlength\topmargin{0in}
\setlength\headheight{0in}
\setlength\headsep{0in}
\setlength\textheight{9in}
\setlength\textwidth{6.5in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\setlength\parindent{0.1in}
\setlength\parskip{0.25em}

\ifpdf
 \DeclareGraphicsExtensions{.pdf, .jpg}
\else
 \DeclareGraphicsExtensions{.eps, .ps}
\fi

\newif\ifdraft
\drafttrue

\ifdraft
\newcommand{\fixme}[1]{ { \bf{ ***FIXME: #1 }} }
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***Jha: #1 }}}
\newcommand{\yyenote}[1]{ {\textcolor{blue} { ***yye00: #1 }}}
\else
\newcommand{\jhanote}[1]{}
\newcommand{\yyenote}[1]{}
\newcommand{\fixme}[1]{}
\fi

\begin{document}

\title{\large The Performance Effect of Distributed FileSysystems on Data-Intensive Applications}

\author{Christopher Miceli$^{1}$, Michael Miceli$^{1}$, Bety Rodriguez-Milla$^{1}$, Shantenu Jha$^{1,2}$\\
  \small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State University, USA}}\\
  \small{\emph{$^{2}$Department of Computer Science, Louisiana State
      University, USA}}}

\maketitle

\section{New Abstract}

Grids and, more recently, Clouds are showing themselves capable of handling large problems. While the capability of these systems is great, unique performance issues appear with the trend of extremely large data sets, such as Google's 20 petabytes of data processed per day.  An distrubuted application developer must take care when accessing this data or else performance could be greatly adversely affected. Distributed filesystems are applications that handle replication and distribution of files across mutiple machines to provide fault-tolerance. This abstraction removes pressure from developers, but also has performance implications.  The aim of this paper is to address what these may be and why they are important.

When working with distributed systems and large data sets together, determining whether to move input data to the computational resource, or the computational resource to the input data becomes very important. Frequently there will be more than one copy of the input data for fault-tolerance reasons, then the added issue of deciding between the two replicas becomes an issues as well. While distributed filesystems remove the responsibility of replica management, the abstraction often makes determining where where in the dfs the data is being stored impossible.  This property removes the ability to move computation resources to the data, and instead leaves the developer hoping the dfs will perform well. Despite this, the dfs's replication may alleviate these issues by placing replicas in locations where computational resources reside.

%There are at least two types of data-intensive applications: the first where the actual data generated is large; the second type is where the data generated is small, but the volume of data on which computation occurs is very large. The application we used, has relatively small input and relatively small output, but the manner of processing causes many data reads.  This type of application can be classified as having a large data throughput. \jhanote{Can you elaborate on different types of data-intensive applications? What kind is
%an ImageMagic based application?}
%
%\jhanote{The assumption in the next sentence is that all Data Intensive applications are distributed, thus add the word, ``Distributed''.} Data intensive grid applications need to be very mindful of the location of the data being operated on.  In some cases, moving data to a distributed source of computation may be advantageous, compared to utilizing the computation resource co-located with the data (volume of data might be very large thus multiple threads of compute, or I/O requirements might be very large, thus saturating I/O channels). In general, the traditional (simple) assumptions of data being shipped to whereever the computation was to occur are no longer valid. What are the factors that determine the movement of data, computation or both?
%\jhanote{Address this question. (i) application type, (ii) infrastructure used, (iii) degree of distribution .. }
%
%To understand the landscape of the challenges associated with developing and deploying Data-Intensive Applications, we have developed a framework that implements the well known All-Pairs Patterns, and we utilize this framework to perform Image Matching, where individual elements can be placed over different 
%distributed resources.

In this paper, we use a grid-enabled All-Pair algorithm application.  This application applies an operation on the input data set such that every possible pair in the set will be input to the operation.  The result of this application is stored in a matrix.  This application spawns jobs on the grid to run sets of these pairs.  The problem becomes determining which pairs to put into a set, and with which distributed resource to run that set.  If transferring data to the grid job takes too long, we will spend more time on data management than computation.  There may be an entity in the grid capable of the work that may be slower than others, but close enough to the data to make up for its lack of computational ability. In our experiments, we used CloudStore, an open-source high performance distributed filesystem that builds upon ideas from Google's distributed filesystem GFS.  CloudStore was chosen for its high performance focus, C++ implementation, and its source code availability.

\jhanote{references needed. for example, CloudStore, All-Pairs, KFS.. }

We use the results of three different experiments when determining the effect of distributed filesystems.  In the first experiment, we run our All-Pairs application on a grid without regard to data placement.  No replication or fault-tolerance will take place. The application will sequentially assign data to the first available computational resource. The second experiment will be similar to the first, except the All-Pairs application will take the data's location into consideration when determining whether or not to assign a certain data set. This version of the application performs an extra step that determines the performance of the network by pinging hosts that may be involved, and utilizes this information when deciding which data set to assign to a job requesting work.  \jhanote{OK to start off with ping, but something more sophisticated than ping will be required. Use iperf? or other similar tools (netperf?)}  If there is an unprocessed data set collocated or network-close with the job, then the assignment of that worker to that data set would have benefits.  If there is no unprocessed data set network-close to the job, still assign data that may be network-far, in case the network-close job failed or there is no available jobs network-close to the data set. The third experiment will provide information into distributed filesystem's performance in handling data locality issues.  \jhanote{What about replication factor as a variable of the experiment?}  The All-Pairs application will be the same as the first, except all data will be stored in the distributed filesystem CloudStore with a replication factor of two (each file is guaranteed to be replicated at least twice).  All read and writes will also utilize the distributed filesystem.  Since the performance of this test dependent on the distributed filesystem's location relative to the computational resources, we decided to make block-servers on every machine that may be capable of performing work.  This will put all responsibility on the DFS in determining where to place data.

RESULTS

Distributed filesystems do affect the performance of data-intensive distributed applications.  Notably, the second experiment is also na√Øve in the way that it attempts to optimize data and work assignments.  Recent work has shown that data in large scale grid applications tends to be accessed together, despite being seemingly unrelated in the data set.  The data accessed frequently together being called a filecule \jhanote{reference!}.  Attempting to determine these filecules in the All-Pair application would be very difficult \jhanote{filecules -- or the lack thereof, are application specific, and possible compute-function dependent}. If the chunkserver is also capable of data processing, then the dfs will be placing filecules together, and on machines needing them for work.  The fault tolerance that distributed filesystems are already well renowned for also have added benefits to grid application developers.  The grid application does not have to be aware of where data has been copied to previously when assigning work.  The distributed filesystem will use the best data replica when data is being accessed.

\jhanote{Ok, so what? Need a few sentences, outlining the importance of this work and where
things stand and/or where things will go?}

\bibliographystyle{IEEEtran} 
\bibliography{saga}
\end{document}
