%\documentclass{article}
\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}
\usepackage{multirow}
\usepackage{listings}
\usepackage{times}
\usepackage{paralist}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[hypertex]{hyperref}
\usepackage{subfigure}
\usepackage{color}

%\documentclass{rspublic}

\usepackage{ifpdf}

\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\BI}[1]{\textbf{\textit{#1}}}
\newcommand{\T}[1]{\texttt{#1}}

\pdfpagewidth 8.5in
\pdfpageheight 11in 

\setlength\topmargin{0in}
\setlength\headheight{0in}
\setlength\headsep{0in}
\setlength\textheight{9in}
\setlength\textwidth{6.5in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\setlength\parindent{0.1in}
\setlength\parskip{0.25em}

\ifpdf
 \DeclareGraphicsExtensions{.pdf, .jpg}
\else
 \DeclareGraphicsExtensions{.eps, .ps}
\fi

\newif\ifdraft
\drafttrue

\ifdraft
\newcommand{\fixme}[1]{ { \bf{ ***FIXME: #1 }} }
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***Jha: #1 }}}
\newcommand{\yyenote}[1]{ {\textcolor{blue} { ***yye00: #1 }}}
\else
\newcommand{\jhanote}[1]{}
\newcommand{\yyenote}[1]{}
\newcommand{\fixme}[1]{}
\fi

\begin{document}

\title{\large The Performance Effect of Distributed FileSysystems on Data-Intensive Applications}

\author{Christopher Miceli$^{1}$, Michael Miceli$^{1}$, Bety Rodriguez-Milla$^{1}$, Shantenu Jha$^{1,2}$\\
  \small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State University, USA}}\\
  \small{\emph{$^{2}$Department of Computer Science, Louisiana State
      University, USA}}}

\maketitle

\section{New Abstract}

Grids and, more recently, Clouds are showing themselves capable of handling large problems. While the capability of these systems is great, unique performance issues appear as the trend of extremely large data sets, such as Google's 20 petabytes of data processed per day, continues to grow. A distributed application developer must take precautions when accessing this data or else performance could be adversely affected greatly. Distributed filesystems (DFSs) are applications that handle replication and distribution of files across multiple machines to provide fault-tolerance. This abstraction removes pressure from developers, but also has performance implications. The aim of this paper is to address what these may be and why they are important.

When working with distributed systems and large data sets together, determining whether to move input data to the computational resource, or the computational resource to the input data becomes very important. Frequently there will be more than one copy of the input data for fault-tolerance reasons, then the added issue of deciding between the two or more replicas becomes an issues as well. While DFSs remove the responsibility of replica management, the abstraction often makes determining where where in the DFS the data is being stored impossible. This property removes the ability to move computation resources to the data, and instead leaves the developer hoping the DFS will perform well. Despite this, the DFS's replication may alleviate these issues by placing replicas in locations where computational resources reside.

%There are at least two types of data-intensive applications: the first where the actual data generated is large; the second type is where the data generated is small, but the volume of data on which computation occurs is very large. The application we used, has relatively small input and relatively small output, but the manner of processing causes many data reads. This type of application can be classified as having a large data throughput. \jhanote{Can you elaborate on different types of data-intensive applications? What kind is
%an ImageMagic based application?}

In this paper, we use a grid-enabled All-Pair algorithm application. This application applies an operation on the input data set such that every possible pair in the set will be input to the operation. The result of this application is stored in a matrix. This application spawns jobs on the grid to run sets of these pairs. The problem becomes determining which pairs to put into a set, and with which distributed resource to run that set. If transferring data to the job takes too long, we will spend more time on data management than computation. There may be an entity in the grid capable of the work that may be slower than others, but close enough to the data to make up for its lack of computational ability. In our experiments, we used CloudStore (formerly KFS), an open-source high performance distributed filesystem that builds upon ideas from Google's distributed filesystem GFS. CloudStore was chosen for its high performance focus, C++ implementation, and its source code availability.

\jhanote{references needed. for example, CloudStore, All-Pairs, KFS.. }

We use the results of three different experiments when determining the effect of distributed filesystems. In the first experiment, we run our All-Pairs application on a grid without regard to data placement. No replication or fault-tolerance will take place. The application will sequentially assign data to the first available computational resource, and all data will be accessed via the gridftp protocol. The second experiment will be similar to the first, except the All-Pairs application will take the data's location into consideration when determining whether or not to assign a certain data set. This version of the application performs an extra step that determines the performance of the network by pinging hosts that may be involved, and utilizes this information when deciding which data set to assign to a job requesting work. \jhanote{OK to start off with ping, but something more sophisticated than ping will be required. Use iperf? or other similar tools (netperf?)} If there is an unprocessed data set collocated or network-close with the job, then the assignment of that worker to that data set would have benefits. If there is no unprocessed data set network-close to the job, still assign data that may be network-far, in case the network-close job failed or there is no available jobs network-close to the data set. The third experiment will provide information into DFSs' performance in handling data locality issues. \jhanote{What about replication factor as a variable of the experiment?} The All-Pairs application will be the same as the first, except all data will be stored in the distributed filesystem CloudStore with a replication factor of two (each file is guaranteed to be replicated at least twice). All read and writes will also utilize the distributed filesystem. Since the performance of this test dependent on the DFS's location relative to the computational resources, we decided to make block-servers on every machine that may be capable of performing work. This will put all responsibility on the DFS in determining where to place data.

Our results do show that a DFS greatly change the performance on an distributed application in a positive manner. Our experiments that utilized the DFS to access and store data outperformed their gridftp counterparts by an order of magnitude in most cases. Our results also indicate that the DFS scales better as file sizes and number of files grow, although both seem to scale linearly. Before any conclusions may be drawn, there are issues that need to be addressed. Our applicationutilized the Simple API for Grid Applications (SAGA) to access DFS based and gridftp based files. Accessing files through this abstraction with gridftp seemed to perform sub-optimally in comparison to using the globus tools directly. In addition, our distributed system was not extremely large, so with our replication level of two in the DFS, data was almost certainly co-located with the computational resource. Staging did improve upon the results of the naïve first experiment, but still did not approach the DFS's performance levels. 

Distributed filesystems are important abstractions for a data-intensive distributed application developers to consider. It also appears that staging is worth the time required to build a graph representing the network. Also to note, the second experiment is also naïve in the way that it attempts to optimize data and work assignments. Our staging only performed pings, not data transfer trials or reliability tests. A job could have low latency, but poor bandwidth. Perhaps CloudStore's performance can be attributed to recent work that has shown that data in large scale distributed applications tends to be accessed together, despite being seemingly unrelated in the input data set. The data accessed frequently together being called a filecule, an application specific group of files \jhanote{reference!}. Attempting to determine these filecules in the All-Pair application would be very difficult\jhanote{filecules -- or the lack thereof, are application specific, and possible compute-function dependent}. In a DFS, however, if the data store is also capable of data processing, then the DFS will be placing filecules together, and on machines needing them for work. The fault tolerance that distributed filesystems are already well renowned for also have added benefits to grid application developers in terms of performance. The distributed application does not have to be aware of where data has been copied to previously when assigning work; the distributed filesystem will use the best replica when data is being accessed.

\bibliographystyle{IEEEtran} 
\bibliography{saga}
\end{document}
