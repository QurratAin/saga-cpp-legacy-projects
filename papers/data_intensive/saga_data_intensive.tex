%%%%%%%%%%%%%%%
% RSGUIDE.TEX %
%%%%%%%%%%%%%%%

% Guide to preparing TeX articles for Royal
% Society articles using RSPUBLIC.CLS
% Use this file as a test file

\documentclass{rspublic}  

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}    
\usepackage{multirow}    
\usepackage{listings}   
\usepackage{times}     
\usepackage{paralist}    
\usepackage{wrapfig}    
\usepackage[small,it]{caption}
\usepackage{multirow}
\usepackage{ifpdf}    
\usepackage{subfig} 
                  


\title{Programming Abstractions for Data Intensive Computing}

\author{Michael Miceli$^{12}$,Shantenu Jha$^{12}$,Chris Miceli$^{12}$, 
  Hartmut Kaiser$^{1}$, Andre Merzky$^{1}$, Ole Weidner$^{12}$ \\
  \small{\emph{$^{1}$Center for Computation \& Technology, Louisiana
      State University, USA}}\\
  \small{\emph{$^{2}$Department of Computer Science, Louisiana State
      University, USA}}\\
  \small{\emph{$^{3}$e-Science Institute, Edinburgh, UK}}\\
}

\newif\ifdraft
%\drafttrue
\ifdraft
\newcommand{\kimnote}[1]{ {\textcolor{green} { ***JK: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***AL: #1 }}}
\newcommand{\amnote}[1]{ {\textcolor{magenta} { ***AM: #1 }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\else
\newcommand{\kimnote}[1]{}
\newcommand{\alnote}[1]{}
\newcommand{\amnote}[1]{}
\newcommand{\jhanote}[1]{}
\fi

\begin{document}

\maketitle

\begin{abstract}{SAGA, ..., search, sequencing.. data-intensive}
  SAGA~\cite{saga_gfd90} is a high level API that provides a simple,
  standard and uniform interface for the most commonly required
  distributed functionality.  SAGA can be used to encode grid
  applications~\cite{saga_escience07, saga_tg08}, tool-kits to manage
  distributed applications as well as implement abstractions that
  support commonly occurring programming, access and usage patterns.
  The focus of this paper is on the latter set, i.e.  the use of SAGA
  in implementing well known abstractions for data intensive
  computing.  In this paper, we will implement MapReduce and All-Pairs
  abstractions using SAGA and use them to solve commonly encountered
  genomic tasks.  We will show how multiple sequence alignment can be
  orchestrated using the SAGA-Allpair implementation, and genome
  searching can be implemented using MapReduce.  In addition, the aim
  of this paper is to show (validate) that SAGA is sufficiently
  complete and has a high-level interface to support these programming
  abstractions.  Figure~\ref{fig:data_intensive_app_saga} illustrates
  the software architecture of the implementation, highlighting the
  different abstraction levels that allow the reuse of most of the
  system for both algorithms and for different genomic applications.
  We will highlight the salient points of our implementations, and how
  we handle common considerations such as when to move the data to the
  machine or when to process it locally.  The implementation of these
  abstractions encapsulates details such as latency hiding,
  performance and other variables (such as cluster sizes, and queue
  sizes).  The user should be able to easily add a few function calls
  without worrying about many considerations required by most grid
  computing applications.  We will discuss other performance issues
  that arise when implementing abstractions specific for
  data-intensive computing.  A grid application's design should not
  focus on the bandwidth of the network, the dispatch latency, the
  number of machines available, and data reliability.  Even something
  as simple as process size can be a tough challenge to optimize.  If
  a job is too small, then network traffic becomes a bottleneck and
  the design is inefficient.  If a job is too large, it is difficult
  to tell when it is hanging or still computing.  Also, if another job
  with a higher priority takes a machine over, the application will be
  waiting on jobs longer.  The main point of this paper is to show how
  a flexible, extensible implementation of programming data-intensive
  abstractions using SAGA can shield the application developer many of
  these considerations.

\end{abstract}

\section{Introduction} {\bf Jha} 

\jhanote{Outline the work in this paper}

\jhanote{Motivation for why this work is important}

% \section{Data Intensive Computing} 

\subsection{Example 1: MapReduce}

MapReduce~\cite{mapreduce-paper} is a programming framework which
supports applications which operate on very large data sets on
clusters of computers.  MapReduce relies on a number of capabilities
of the underlying system, most related to file operations.  Others are 
related to process/data allocation.  The Google File System, and other
global file systems, provide the relevant capabilities, such as atomic
file renames.  Implementations of MapReduce on these global file systems
are free to focus on implementing the the data-flow pipeline, which is the
algorithmic core of the MapReduce framework.

We have recently implemented MapReduce in SAGA, targeting general
purpose Grid systems, where the system capabilities required by
MapReduce are usually not natively supported -- instead, a general
purpose grid provides a much larger set of lower level operations.
Some semantics, such as again the atomic file rename, is provided by
the SAGA API layer, others, such as data/compute allocation are not.
Our implementation is thus required to interleave the core logic with
explicit instructions on where processes are to be scheduled.  Note that
some of the required capabilities can be provided by higher level grid
services -- those are, however, often not standardized, and often not
available in general purpose Grids.

The advantage of this approach is obviously that our implementation is
no longer bound to run on a system providing the appropriate semantics
originally required by MapReduce, but is portable to other, more
generic systems as well.  The drawback of the approach is obvious as
well: our implementation is relatively more complex, as it needs to
add semantic system capabilities at some level or the other, and it is
inherently slower, as it is for these capabilities very difficult or
near impossible to obtain system level performance on application
level.  But many of these are due to the early-stages of the
implementation of SAGA, and not a fundamental limitation of the design
or concept.

%  when operating on specific parts of the data set~\cite{gsoc-saga}.


\subsubsection{Details}

MapReduce is a programming framework developed by Google for running
data-intensive processes over a large cluster of commodity machines.
The main idea is to provide an easy interface for users to solve a
specific domain of problem with grids without having the client worry 
about the semantics of the grid, for example, dispatch latency, machine
failure, data distribution, and network bandwidth.  All of these are
problems that could make some naïve implementations on a grid slower
than serial implementations.  When using a MapReduce implementation,
the programmer interacts with only two functions: map and reduce.  
The map function will go through the datasets and create a map of data
to some type of output.  A map is a datatype in computer science that
relates one object to another, i.e. a name t o a phone number.  The reduce
function will go through all of the maps and combine them to produce
another output.  The de facto example is word count.  Say, one has a
large set of documents and he wants to find how many of each word is
present in all the documents.  The map function would go through each
document and for every word produce a map from the document to a list
of 1’s.  An example for the word “cat” may look like this cat -> 1, 1,
1, 1, 1.  Then, the reduce function will add up the 1’s to produce a
final output, i.e. cat -> 5.  At first this framework may seem limiting,
but this is in part what makes it so successful.  Its structure is 
is very easily parallelized.   Also, there have been a large number
of distinct applications written using it, and its popularity has been
steadily increasing.  This shows that the framework is well suited for
solving data-intensive problems.


\subsection{Our Implementation:} 

There are a few implementations of MapReduce, most notably Google's and
Hadoop's.  These are what our implementation is based on.  It is able
to handle most of the problems associated with grid applications.  We use
the idea of a master/slave type framework.  The programmer would compile
different slave applications for every different type of machine he 
would expect to encounter in the grid.  The slave application is
where the map and reduce functions are.  Then the master will handle
submitting these jobs on other machines as well as keeping track of
the state of the machines -- idle, done, or failed.  This allows for
easy running of an application using large girds that have frequent
machine failures.  Our MapReduce also handles data distribution in the
same manner Google does, by creating partitions of data so the entire
data set will never be on one machine, thus limiting network bandwidth
and data distribution.  These files could then recognized by a
distributed file systems such as Hadoop File System.  Although, our
current implementation is written to avoid excessive network
bandwidth, it does not change depending on current network
availability.


\subsection{Example 2: All-Pairs}

\section*{Infrastructure}

\subsection*{Hadoop}

Provide 1 paragraph description of Hadoop

Discuss the C interface to Hadoop

\subsection*{Bigtable, HBase}

Bigtable is a specific type of database system created by Google to 
have better control over scalability and performance than other 
databases.  It is very flexible and has many advantages over normal 
databases.  The main difference is that it is meant to store extremely 
large datasets, into the petabytes, over thousands of machines~\cite{bigtable}.  
Another advantage is the ability to be accessed through MapReduce.  Since 
MapReduce is inherently very well parallelized, accessing Bigtable is very 
efficient.  Due to the success of Bigtable, HBase was developed by Hadoop as 
an open source alternative for use with their MapReduce.  Both of them are 
implemented in a similar way.  They split up large tables and replicate them 
over many machines to avoid node failure.  Also, since the data is partitioned 
accessing it does not create a large strain on the grid's network bandwidth.

%Provide 1 paragraph description of Bigtable and its specific
%implementation -- HBase

\subsection*{Interfacing SAGA to Hadoop and Bigtable}
%We used hadoop as a metadata storage, work still needs to be done
%to see how well SAGA-MapReduce is at accessing elements as part of mapping

\section{Applications}

\subsection*{Alignment}

\subsection*{Search}

\section*{Experiments}

\subsection*{MapReduce using Advert Service}

\subsection*{MapReduce using HBase}

\subsection*{Native Implementation: MapReduce using HBase and Hadoop}

\subsection*{SAGA Based MapReduce}

Performance figures when using the Advert service -- on one machine,
on >1 machine, finding the bottleneck

\jhanote{These figures can then be used to justify some of the 
  development efforts proposed in OMIISAGA-2}

\section{Acknowledgments}

\bibliographystyle{plain}
\bibliography{saga_data_intensive}

\end{document}

