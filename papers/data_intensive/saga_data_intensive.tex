\documentclass{article}

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[hypertex]{hyperref}
\usepackage{subfigure}  
\usepackage{color}
\usepackage{srcltx}
\usepackage{url}
\usepackage[small,it]{caption}


\parskip   = 0.5em
\parindent = 0.0em

\textwidth      = 6.5315 in
\textheight     = 9.0315 in
\oddsidemargin  = 0.0 in
\evensidemargin = 0.0 in
\topmargin      = 0.0 in
\headheight     = 0.0 in
\headsep        = 0.0 in
\textfloatsep   = 0.1in

\newenvironment{shortlist}{
  \vspace*{-0.5em}
  \begin{itemize}
  \setlength{\itemsep}{-0.3em}
}{
  \end{itemize}
  \vspace*{-0.5em}
}

\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\BI}[1]{\textbf{\textit{#1}}}
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\NL}{\newline}

\newif\ifdraft
\drafttrue
\ifdraft
 \newcommand{\jhanote}[1]{  {\textcolor{red}    { ***Shantenu: #1 }}}
 \newcommand{\katznote}[1]{ {\textcolor{cyan}   { ***Dan:      #1 }}}
 \newcommand{\amnote}[1]{   {\textcolor{magenta}{ ***Andre:    #1 }}}
 \newcommand{\hknote}[1]{   {\textcolor{blue}   { ***Hartmut:  #1 }}}
\else
 \newcommand{\jhanote}[1]{}
 \newcommand{\katznote}[1]{}
 \newcommand{\amnote}[1]{}
 \newcommand{\hknote}[1]{}
\fi


\usepackage{ifpdf}
\ifpdf
 \DeclareGraphicsExtensions{.pdf, .jpg}
\else
 \DeclareGraphicsExtensions{.eps, .ps}
\fi

\newcommand{\up}{\vspace*{-1em}}

% CFP:
%
%
% October 22 and 23, 2008
% 
% Dramatic growth in data and equally rapid decline in the cost of
% highly integrated clusters has spurred the emergence of the data
% center as the platform of choice for a growing class of
% data-intensive applications. To encourage conversations between
% those developing applications, algorithms, software, and hardware
% for such "cloud" platforms, we are convening the first workshop on
% Cloud Computing and its Applications (CCA'08).
% 
% This workshop will include a mixture of invited and contributed
% talks on cloud computing, data intensive scalable computing, and
% related topics.
% 
% Topics of interest include:
% 
%   - compute and storage cloud architectures and implementations
% * - map-reduce and its generalizations
% * - programming models and tools
%   - novel data-intensive computing applications
%   - data intensive scalable computing
%   - distributed data intensive computing
%   - content distribution systems for large data
%   - data management within and across data centers
% 
% If you would like to give a contributed talk, please submit a five
% page extended abstract by August 15, 2008. These submissions will
% also be used to select papers for a poster session. Extended
% abstracts should be submitted to:
% https://cmt.research.microsoft.com/CCA2008.


\begin{document}

\title{\large Programming Abstractions for Clouds}

\author{Michael Miceli$^{12}$,Shantenu Jha$^{12}$,Chris
  Miceli$^{12}$, \\  Hartmut Kaiser$^{1}$, Andre
  Merzky$^{1}$, order-to-be-determined
 \\[1em]
        %
        $^1$ \small
          Center for Computation and Technology, 
          Louisiana State University\\[-0.3em]
        $^2$ \small
          Department of Computer Science, 
          Louisiana State University\\[-0.3em]
        }

\maketitle

\begin{abstract}

  \noindent

 %  Clouds seem like 'Grids Done Right', including scalability,
%   transparency, and ease of management. Virtual Machines are the
%   dominant application environments for compute Clouds, however, that
%   does not make application programming any less relevant than
%   ``non-virtualized'' environments.  The limited set of successful
%   Cloud applications show that distributed programming patterns of the
%   type of
% %  MapReduce, AllPairs, and BigTable are required by a large set of
%   MapReduce and All-Pairs are
%   required % by a large set of applications,
%   to make Cloud infrastructure a viable compute environment for a
%   large class of problems.  The existence of multiple implementations
%   of these programming paradigms also makes clear, that application
%   portability is, even for Clouds an emerging problem which needs
%   addressing beyond the level of system virtualization. %\\[-0.5em]
%   This paper discusses these and other challenges around cloud
%   applications programming and development, and through a discussion
%   of several applications, demonstrates potential solutions. We
%   discuss how using the right abstractions -- programming interfaces,
%   frameworks that support commonly occurring programming and execution
%   patterns -- enable efficient, extensible and importantly {\it
%     system-independent} implementations of common programming patterns
%   such as MapReduce, i.e. same application is usable seamlessly on both
%   traditional Grids and Clouds systems. We further discuss that
%   lessons learned from programming applications for Grid environment
%   also apply, to some extent, to Cloud environments.

% The deluge of data is upon us and there exist a critical need to be
% able to access, manage and mine this data. The need for 
% abstractions to assist in these requirements has never been greater.

% The power of Google is based upon a simple programming abstractions --
% MapReduce~\cite{mapreduce}.  Mapreduce is a framework for splitting up
% large data and keeping track of which machine the data is on.
% There exist additional powerful abstractions, such as
% AllPairs~\cite{allpairs}, which is both a programming abstractions and
% a system-level abstraction.  Allpairs is a framework for when every
% element of some data needs every other element.

% We will discuss two specific abstractions and the performance issues
% that arise when using them.  This paper will focus on how to create a
% useful and efficient implementation of both MapReduce and AllPairs
% using SAGA.  The implemention of these abstractions encapsulates
% details such as latency hiding, performance and other variables (such
% as cluster sizes, and queue sizes).  The user should be able to easily
% add a few function calls without worrying about many considerations
% required by most grid computing applications.

% SAGA is a high level API that is provides a simple, standard and
% uniform interface to the most commonly required distributed
% functionality. SAGA can be used to encode grid applications, tool-kits
% to manage distributed applications as well as implement abstractions
% that support commonly occuring programming, access and usage patterns.
% The focus of this paper is on the latter set, ie the use of SAGA in
% implementing well known abstractions for data intensive computing.

% A grid applications design
% should not focus on the bandwidth of the network, the dispatch
% latency, the number of machines available, and data reliability.  Even
% something as simple as process size can be a tough challenge to
% optimize.  If a job is too small, then network traffic becomes a
% bottleneck and the design is inefficient.  If a job is too large, it
% is difficult to tell when it is hanging or still computing.  Also, if
% another job with a higher priority takes a machine over, the
% application will be waiting on jobs longer.


SAGA~\cite{saga_gfd90} is a high level API that provides a simple,
standard and uniform interface to the most commonly required
distributed functionality.  SAGA can be used to encode grid
applications~\cite{saga_escience07, saga_tg08}, tool-kits to manage
distributed applications as well as implement abstractions that
support commonly occurring programming, access and usage patterns.
The focus of this paper is on the latter set, i.e.  the use of SAGA in
implementing well known abstractions for data intensive computing.

In this paper, we will implement MapReduce and All-Pairs abstractions
using SAGA and use them to solve commonly encountered genomic tasks.
We will show how multiple sequence alignment can be orchestrated using
the SAGA-Allpair implementation, and genome searching can be
implemented using Map-reduce.  In addition, the aim of this paper is
to show (validate) that SAGA is a sufficiently complete and high-level
interface so as to support these programming abstractions.

Figure~\ref{fig:data_intensive_app_saga} illustrates the software
architecture of the implementation, highlighting the different
abstraction levels that allow the reuse of most of the system for both
algorithms and for different genomic applications.  We will highlight
the sailent points of our implementations, and how we handle common
considerations such as when to move the data to the machine or when to
process it locally.  The implemention of these abstractions
encapsulates details such as latency hiding, performance and other
variables (such as cluster sizes, and queue sizes).  The user should
be able to easily add a few function calls without worrying about many
considerations required by most grid computing applications.

We will discuss other performance issues that arise when implementing
abstractions specific for data-intensive computing.  A grid
application's design should not focus on the bandwidth of the network,
the dispatch latency, the number of machines available, and data
reliability.  Even something as simple as process size can be a tough
challenge to optimize.  If a job is too small, then network traffic
becomes a bottleneck and the design is inefficient.  If a job is too
large, it is difficult to tell when it is hanging or still computing.
Also, if another job with a higher priority takes a machine over, the
application will be waiting on jobs longer.  The main point of this
paper is to show how a flexible, extensible implementation of
programming data-intensive abstractions using SAGA can shield the
application developer many of these considerations.

\end{abstract}

\section{Introduction} {\bf Jha} 


% \amnote{the text below seems a repetition from above?}
% 
% The Simple API for Grid Applications (SAGA) can be used to
% pro grammatically develop a very wide-range of distributed
% applications.  In this paper we describe how SAGA has been used to
% develop two different applications from the following classes of
% distributed applications (i) applications based upon the loosely
% coupled of homogeneous sub-tasks and, (ii) applications based upon
% loosely coupled simulations of heterogeneous sub-tasks. The specific
% applications developed are Replica-Exchange simulations using
% Molecular Dynamics and Kalman-Filter based application for reservoir
% simulation.  We briefly discuss the specific applications developed
% and the typical science problems tackled using these applications.
% We will describe the application characteristics of the two
% case-studies, with a focus on the distributed logic of these
% simulations, and not the core simulation logic of the applications.  

% \amnote{Shantenu, I don't think we should focus once again on how
% SAGA manages to hide middleware details.  We made that the focus of
% quite a number of papers already, and it like beating a dead horse
% :-)  I would not be surprised if reviewers say: oh no, yet another
% SAGA paper...  Instead, I'd vote for focusing on the
% Application-in-a-cloud problem, and to only covert advertising for
% SAGA...}
% 
% The paper analyses and
% contrasts the application characteristics of the examples, and shows
% how they are supported using SAGA, often in conjunction with other
% programming frameworks such as Cactus.  The primary aim of this paper
% is to demonstrate how SAGA can be an effective tool for
% programmatically representing and implementing the logic of
% coordination and orchestrating multiple, distributed tasks, while
% remaining agnostic to the actual mechanism, i.e. details of the
% distributed environment. 

% \amnote{And here also: I think we should highlight the need of
% exposing the right system affinities, not on using the right
% frameworks.  Yes, frameworks can deliver that, but can never be as
% efficient as system level features, IMHO.}
% 
% We will highlight the importance of programming abstractions and how
% frameworks that provide common programming patterns can be used to
% simplify the construction of distributed applications. 

% We will conclude our discussion with a proposed procedure for defining
% scientific-computing oriented Cloud properties, in
% Section~\ref{sec:conclusion}.

\up
\section{Data Intensive Computing} 

 
In ~\cite{cloud-saga-paper} it was shown how Grid system interfaces
(in particular for general purpose Grids) tend to be complete
(i.e. they try to expose a complete set of available system
capabilities), whereas Cloud interfaces tend to be minimalistic
(i.e. they expose only a limited set of capabilities, just enough to
'do the job').
 
 \up
 \subsection{Usage Modes}

  It is important to understand the reason for this difference.  In
  our experience, general purpose Grids are mostly designed bottom-up:
  existing, often heterogeneous resources are federated as VOs, and
  their combined capabilities, plus additional capabilities of higher
  level Grid services, are offered to the end-user.  This is not
  applicable for Clouds: the design of Clouds seems to be, mostly, top
  down. Clouds are designed to serve a limited, specific set of use
  cases and usage modes, and the Cloud system interface is designed to
  provide \I{that} functionality, and no other.  Furthermore, the
  Cloud system itself, and in particular its high level services, may
  be designed to implement specific target use cases, while not
  supporting others (e.g., a Cloud could be homogeneous by design).
  These differences do not imply that Clouds are trivial to implement.
  In practice the opposite is most likely true (due to issues of
  scale, amongst other things). Clouds may very well build upon
  general purpose Grids, or narrow Grids, and at least face the same
  challenges; but their system interfaces do not expose those internal
  capabilities.

  Specific users and user communities tend to create different
  applications but with shared characteristics.  For example, the
  particle data community tends to focus on very loosely coupled, data
  intensive parameter sweeps involving Monte Carlo simulations and
  statistical analyzes.  Systems used by these communities are thus
  designed to support these application classes before others.
  
  The \I{Usage Mode} tries to catch the dominant
  properties of the main application classes, insofar they are
  relevant to the design of the system, and to the operational
  properties of the system.  For example, the usage mode \I{'massively
  distributed, loosely coupled'} implies that the system's design
  prioritizes on compute resources (e.g. cycle scavenging, large
  clusters), and to a lesser degree on communication (no need for fast
  links between application instances), or on reservation and co
  scheduling.

  In contrast, the usage mode \I{'massively distributed,
    tightly-coupled'} would imply a system's design to focus on compute
  resources, but importantly also on fast communication between near
  nodes, and on (physical) co-location of processes.


 \up
 \subsection{Affinities}

  Currently Clouds seem to be designed to mostly support exactly one
  usage mode, e.g.  data storage, \I{or} high throughput computing,
  \I{or} databases, etc.  This does not preclude Clouds targeting more
  than one domain or usage mode, however.  The overarching design
  guideline to support the main target usage modes of Cloud systems,
  we defined as its \BI{affinity}.  In other words, affinity is the
  term we use to indicate the type of computational characteristics
  that a Cloud supports.  That property can very often be expressed as
  the need to use different aspects or elements of a system
  \I{together} (hence the term 'Affinity', in the sense of
  'closeness').  

  For example, the usage mode \I{distributed, tightly coupled}
  implies that an application requires the use of multiple compute
  resources, which need to be 'near' to each other, together with fast
  communication links between these compute resources.  The system
  needs to have a \I{compute-communication affinity}, and a
  \I{compute-compute affinity}.

  Affinities are, however, not always mappable
  to 'closeness'.  For example, we say that a system that supports
  'persistent storage, data replication, data intensive' usage mode,
  may have 'bulk storage affinity' -- in the sense that it needs to be
  designed to have bulk storage properties (availability guarantees,
  long term consistency guarantees etc).  This example also shows that
  affinities are, in some sense, related to Quality of Service (QoS)
  properties exposed by the system, and thus to Service Level
  Agreements (SLAs) about these qualities of service.


 \up
 \subsection{Affinities and Programming Abstractions}

  Affinity is thus a high level characterization of the kind of
  application that could be beneficially executed on a particular
  Cloud implementation, without revealing the specifics of the
  underlying architecture. In some ways, this is the ``ideal
  abstraction'' for the end-user who would like to use infrastructure
  as a black-box.  Some classic examples of affinity are:
  tightly-coupled/MPI affinity, high-throughput affinity (capacity),
  fast-turnaround affinity (capability), or bulk storage affinity.
  Our observation is that Clouds have at least one affinity, a
  corollary to which is that Cloud system interfaces are, designed to
  serve at least one specific set of users or usage modes

  % One can argue that narrow Grids also expose affinity, e.g. that a
  % Data Grid has data affinity.  That may well be true, and we think
  % that the term affinity may be useful for the discussion of narrow
  % Grids as well, but the main difference between Clouds and Grids
  % remain that the interfaces of narrow Grids are still designed so as
  % to expose the complete set of capabilities related to the affinity
  % of narrow Grids, whereas Cloud system interfaces expose the minimal
  % set of capabilities related to its affinities.  For the application
  % developer, but more likely the application deployer, information
  % about the affinity of Clouds should be complemented by SLA
  % information, e.g. providing replicated data in case of loss,
  % co-scheduling at the application level, or low latency
  % communication.  Traditionally SLAs are, implicitly or explicitly,
  % provided by the ``service provider'' based upon infrastructure,
  % policy, usage modes, or negotiation.  For Clouds, SLAs are an
  % implicit part of the system interface: the Cloud's affinities imply
  % a certain QoS to be met, for every use of the system.

  An affinity being the 'ideal system abstraction' has another
  important consequence, as it allows to express suitable programming
  abstractions easily, and natively.  For example, it is certainly
  possible to implement MapReduce on a general purpose Grid, with no
  affinity supporting data replication, or data-compute-colocation.
  The implementation of that abstraction, i.e. the Map Reduce
  application framework, must then however implement these
  capabilities itself, \I{on top} of the system it uses.  On the other hand, if a
  data/compute affine cloud provides these capabilities natively, as
  is the case for, for example, googles proprietary cloud with its google file
  system~\cite{gfs}, then the MapReduce framework can focus on the
  core logic of the programming abstraction, i.e. on the algorithmic
  abstractions, and is thus much easier to implement.
  Note that for the application using the MapReduce framework, there
  is no difference~\cite{gsoc-saga}.

  Clearly, we are arguing for a separation of concerns: we argue that
  application frameworks should not have to deal with exposing,
  expressing, or implementing capabilities which are required \I{by}
  them, but are not part of their algorithmic core.  Those should be
  provided at the system level, which makes the application frameworks
  \I{easily} implementable on any system providing these capabilities,
  i.e. on any system, which has the appropriate affinity.


\up
\section{Distributed Applications Usage Modes}
\label{sec:apps}

 Table~\ref{tab:classes} shows an overview of a number of application
 classes~\cite{dpa_paper} which are widely used in scientific
 computing, and outside.  Often applications in the same class, have
 similar programming models or use programming patterns; for example,
 \I{'pleasingly distributed'} applications, such as the numerous
 \I{'XYZ@Home'} type applications, all share the Master-Worker model,
 in one incarnation or the other.  As compute affine Clouds (aka
 compute Clouds) support that programming paradigm, these applications
 can immediately utilize compute cloud resources with great success.
 For other application classes, such as \I{'tightly coupled,
 heterogeneous'} applications, this is not so obvious, as a compute
 cloud without compute-communication affinity can not easily run a
 communication intensive application efficiently.
 
 \begin{table}[h]
  \begin{center}
   \footnotesize
   \begin{tabular}{|p{.25\textwidth}|p{.27\textwidth}|p{.39\textwidth}|}
     \hline
 
     \B{Application Class}                                 &
     \B{Data    Driven}                                    &
     \B{Compute Driven}                                    \\\hline
 
     \B{Pleasingly Distributed}                            &
        SETI$@$home                                        &
        Monte Carlo Simulations of Viral Propagation       \\\hline
 
     \B{Loosely Coupled,\NL Homogeneous}                    &
        Image Analysis                                     &
        Replica Exchange Molecular Dynamics of Proteins    \\\hline
 
     \B{Tightly Coupled,\NL Homogeneous}                    &
        Semantic Video Analysis                            &
        Heme Lattice-Boltzmann Fluid dynamics              \\\hline
 
     \B{Loosely Coupled,\NL Heterogeneous}                 &
        Multi-Domain Climate Predictions                   &
        Kalman-Filter Fluid Dynamics                       \\\hline
 
     \B{Dynamic Event Driven}                              &
        Disaster support                                   &
        Visualization                                      \\\hline
 
     \B{First Principle, Distributed}                      &
        MapReduce-Based Web indexing                       &
        MapReduce-Based Motif Distributed search           \\\hline
 
   \end{tabular}
   \caption{\footnotesize \B{Examples of primary categories 
            of distributed applications\cite{dpa_paper}.}}
   \label{tab:classes}
  \end{center}
 \end{table}

 We want to demonstrate using two examples, how the implementation of the
 respective programming patterns used by these application classes can
 be supported by the Cloud affinities\footnote{Both applications have
 been implemented using SAGA~\cite{saga-core}, but we do not, in this
 paper, intent to focus on SAGA as a solution to the discussed problem
 space, but merely use it as means to an end.}.

 \up
 \subsection{Example 1: MapReduce}

  MapReduce~\cite{mapreduce-paper} is a programming framework which
  supports applications which operate on very large data sets on
  clusters of computers.  MapReduce relies on a number of capabilities
  of the underlying system, most related to file operations, but also
  related to process/data colocation.  The Google file system, and
  other global file systems, provide the relevant capabilities, such
  as atomic file renames.  Implementations of MapReduce on these file
  systems can focus on implementing the the dataflow pipeline, which
  is the algorithmic core of the MapReduce framework.

  We have recently implemented MapReduce in SAGA, targeting general
  purpose Grid systems, where the system capabilities required by
  MapReduce are usually not natively supported -- instead, a general
  purpose grid provides a much larger set of lower level operations.
  Some semantics, such as again the atomic file rename, is provided by
  the SAGA API layer, others, such as data/compute colocation are not.
  Our implementation is thus required to interleave the core logic
  with explicit instructions on where processes are to be scheduled
  when operating on specific parts of the data set~\cite{gsoc-saga}.
  Note that some of the required capabilities can be provided by
  higher level grid services -- those are, however, often not
  standardized, and often not available in general purpose Grids.

  The advantage of this approach is obviously that our implementation
  is no longer bound to run on a system providing the appropriate
  semantics originally required by MapReduce, but is portable to
  other, more generic systems as well.  The drawback of the approach
  is obvious as well: our implementation is relatively more complex,
  as it needs to add semantic system capabilities at some level or the
  other, and it is inherently slower, as it is for these capabilities
  very difficult or near impossible to obtain system level performance
  on application level.  But many of these are due to the early-stages
  of the implementation of SAGA, and not a fundamental limitation of
  the design or concept.

  \B{Summary:} A data affine, and compute/data affine environment with
  the capabilities listed above provides a clear separation of
  concerns for MapReduce implementations.


 \up
 \subsection{Example 2: All-Pairs}

\section{Conclusions}
\label{sec:conclusion}



\section{Acknowledgments}
\label{sec:acks}

OMII-UK, Google, SAGA

\footnotesize
\bibliographystyle{plain}
\bibliography{saga_data_intensive}

\end{document}

