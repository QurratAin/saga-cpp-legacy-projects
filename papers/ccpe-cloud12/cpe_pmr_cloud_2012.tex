% cpedoc.tex V2.0, 13 May 2010

\documentclass[times]{cpeauth}

\usepackage{moreverb}
\usepackage{xspace}

\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\onote}[1]{ {\textcolor{cyan} { (***Ole: #1) }}}
\newcommand{\terminology}[1]{ {\textcolor{red} {(Terminology used: \textbf{#1}) }}}
\newcommand{\owave}[1]{ {\cyanuwave{#1}}}
\newcommand{\jwave}[1]{ {\reduwave{#1}}}
\newcommand{\alwave}[1]{ {\blueuwave{#1}}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***shantenu: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{green} { ***andreL: #1 }}}
\newcommand{\amnote}[1]{ {\textcolor{blue} { ***andreM: #1 }}}
\newcommand{\smnote}[1]{ {\textcolor{brown} { ***sharath: #1 }}}
\newcommand{\pmnote}[1]{ {\textcolor{blue} { ***Pradeep: #1 }}}
\newcommand{\msnote}[1]{ {\textcolor{cyan} { ***mark: #1 }}}
\newcommand{\note}[1]{ {\textcolor{magenta} { ***Note: #1 }}}
\else
\newcommand{\onote}[1]{}
\newcommand{\terminology}[1]{}
\newcommand{\owave}[1]{#1}
\newcommand{\jwave}[1]{#1}
\newcommand{\alnote}[1]{}
\newcommand{\amnote}[1]{}
\newcommand{\athotanote}[1]{}
\newcommand{\smnote}[1]{}
\newcommand{\pmnote}[1]{}
\newcommand{\jhanote}[1]{}
\newcommand{\msnote}[1]{}
\newcommand{\note}[1]{}
\fi

\newcommand{\cloud}{cloud\xspace}
\newcommand{\clouds}{clouds\xspace}
\newcommand{\pilot}{Pilot\xspace}
\newcommand{\pilots}{Pilots\xspace}
\newcommand{\pilotjob}{Pilot-Job\xspace}
\newcommand{\pilotjobs}{Pilot-Jobs\xspace}
\newcommand{\pilotcompute}{Pilot-Compute\xspace}
\newcommand{\pilotmapreduce}{PilotMapReduce\xspace}
\newcommand{\pilotdata}{Pilot-Data\xspace}
\newcommand{\pd}{PD\xspace}
\newcommand{\pj}{PJ\xspace}
\newcommand{\pjs}{PJs\xspace}
\newcommand{\pds}{Pilot Data Service\xspace}
\newcommand{\computeunit}{Compute-Unit\xspace}
\newcommand{\computeunits}{Compute-Units\xspace}
\newcommand{\dataunit}{Data-Unit\xspace}
\newcommand{\dataunits}{Data-Units\xspace}
\newcommand{\du}{DU\xspace}
\newcommand{\dus}{DUs\xspace}
\newcommand{\cu}{CU\xspace}
\newcommand{\cus}{CUs\xspace}
\newcommand{\su}{SU\xspace}
\newcommand{\sus}{SUs\xspace}
\newcommand{\schedulableunit}{Schedulable Unit\xspace}
\newcommand{\schedulableunits}{Schedulable Units\xspace}
\newcommand{\cc}{c\&c\xspace}
\newcommand{\CC}{C\&C\xspace}

\usepackage[
%dvips,
colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}

\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\def\volumeyear{2012}

\begin{document}

\runningheads{S. Jha et al.}{Pilot-Abstractions for MapReduce-based  Cloud Applications}

\title{Pilot-Abstractions for MapReduce-based Cloud Applications}

%on Clouds and Grids}

%Extensible, Scalable, Interoperable 

\author{Andre Luckow, Pradeep Mantha, Melissa Romanus, Shantenu
  Jha\corrauth}

\address{Radical Research Group, Rutgers University}

\corraddr{Journals Production Department, John Wiley \& Sons, Ltd,
The Atrium, Southern Gate, Chichester, West Sussex, PO19~8SQ, UK.}

\begin{abstract}
% The data generated by scientific applications is experiencing an
% exponential growth. Addressing the consequences of this exponential
% growth has spawned the field of BigData. Given that data and compute
% resources cannot always be co-located, an important component of the
% BigData problem is how to utilize the efficient use of distributed
% resources so as to make meaningful sense of all of the data produced.
% In recent work we showed how MapReduce can be used to efficiently
% process distributed data across a distributed set of resources.
% Pilot-MapReduce (PMR) is a flexible, infrastructure-independent
% runtime environment for MapReduce. PMR is based on \pilot-abstractions
% for compute (Pilot-Jobs) and data (Pilot-Data). Pilot-Jobs are used to
% couple the map phase computation to the nearby source data, and
% Pilot-Data is used to move intermediate data using parallel data
% transfers to the reduce computation phase.

  The data generated by scientific applications is experiencing an
  exponential growth. The ability to analyze prodigious volumes of
  data require efficient compute-data co-placement.  Capabilities such
  as runtime environments and methods that support scalable yet
  flexible analytical approaches are required.  This paper proposes
  the use of Pilot-abstractions as one way to achieve such flexible
  usage of clouds.  In particular, we focus on Pilot-Data and the use
  of clouds as one element in a broader and heterogeneous distributed
  cyberinfrastructure.  To that end, we build upon existing
  capabilties of integrating MapReduce with pilot-abstractions (called
  Pilot-MapReduce (PMR)).  Pilot-MapReduce (PMR) is a flexible,
  infrastructure-independent runtime environment for MapReduce. PMR is
  based on \pilot-abstractions for compute (Pilot-Jobs) and data
  (Pilot-Data). Pilot-Jobs are used to couple the map phase
  computation to the nearby source data, and Pilot-Data is used to
  move intermediate data using parallel data transfers to the reduce
  computation phase.

  In this paper, we discuss the generalization of the
  pilot-abstraction to clouds and investigate the use of PMR on a
  variety of cloud systems, as well as interoperably with other
  distributed resources.  We show how Pilot abstractions and PMR
  enable the efficient processing of distributed data on heterogeneous
  distributed infrastructure including different academic and
  commercial clouds (e.g. Amazon EC2, Google Compute Engine and
  FutureGrid). We investigate the effectiveness of PMR based
  applications on cloud infrastructure for different next-generation
  gene sequencing applications.  The flexible runtime provided by the
  Pilot-abstraction, helped establish PMR's effectiveness for
  distributed data scenarios on clusters that are nodes on a
  WAN. Given the different network characteristics within clouds, we
  investigate whether performance determinants for clusters, are valid
  for clouds.  We analyze different resource and MapReduce
  configurations, such as both hierarchical and distributed MapReduce
  using an NGS application.  Our analysis covers multiple scenarios
  exposing the different (typical) trade-offs: e.g., the overhead
  times in spawning virtual machines, different storage types,
  geographic distribution, and establishes that the \pilots provide a
  powerful abstraction for \clouds as well.
\end{abstract}

\keywords{Cloud Computing, MapReduce, Grid Computing, Data-Intensive}

\maketitle


\vspace{-6pt}

\section{Introduction}
\vspace{-2pt}

% Intro Big Data
Data has become a critical factor in many science disciplines, e. g. in the 
areas of fusion energy (ITER), bioinformatics (metagenomics), climate (Earth 
System Grid), and astronomy (LSST)~\cite{Jha:2011fk}. An increasing number of 
these applications utilizes either science or commercial cloud infrastructures 
in addition to traditional cyberinfrastructure, i.\,e.\ the number of 
applications that need to efficiently support both compute and data on 
heterogeneous infrastructures rises.

% Why clouds?
Cloud's provide novel opportunities for science \& engineering applications:
clouds offer a relative simple, easy-to-use environment with respect to
resource management, capacity planning capabilities, software environment \&
control etc. The ability to exploit these attributes will lead to applications
with new and interesting usage modes and dynamic execution on clouds and
therefore new application capabilities~\cite{Jha:2010kx, fourthparadigm}.
Clouds provide greater scheduling flexibility, for example, when the set of
resources needed to run an application changes (perhaps rapidly), the
resources employed can actually be changed (new resources can be added, or
existing resources can be removed from the pool used by the job). With this 
capabilities, clouds provide an interesting complement to existing CI. 

Another unique capability of clouds is the possibility to bootstrap custom
runtime environments, which is often difficult with grids. For example, a
number of applications are difficult to build, due to runtime dependencies, or
complicated non-portable build systems. This provides an additional rationale
for cloud environments. After an application environment is created once, it
can be loaded on to various systems, working around issues related to
portability on the physical systems.

% Intro pilot abstractions
\pilotjobs have proven to be a successful abstraction for distributed and
high-performance applications, whereby they decouple the workload
specification from its execution. A \pilotjob provides the ability to utilize
a placeholder job as a container for a dynamically determined set of compute
tasks. Recent work on the P* (Pstar) Model~\cite{pstar12} has provided a
formal theoretical basis for Pilot-Jobs and provided a conceptual framework
upon which to analyze and compare distinct implementations. Significantly, the
P* Model provided a symmetrical but logical extension to Pilot-Jobs to data,
and introduced the concept of Pilot-Data, which akin to Pilot-Jobs for
computational tasks, provides an efficient and extensible route to decoupling
the ultimate location of data storage and/or consumption from its production.

\subsection*{The Case for Distributed, Data-intensive Computing}

What are our contributions, challenges of distributed, data-intensive computing?
\begin{itemize}
	\item choosing the right cloud... 
	\item how to couple different clouds
	\item transport in and out to cloud
	\item support late-binding: late binding as a fundamental attribute in 
	distributed systems
\end{itemize}
Pilot-Jobs help us to address these requirements.


% Outline

\begin{verbatim}

Application perspective versus Infrastructure perspective: (I)
 Application Motivation/Challenges: Scaling data intensive
 applications - Why is this a problem? Any real application requires
 this problem to be solved?  - CMS, Atlas generates PBs of data/ day.
  (II) Infrastructure Motivation: Emerging infrastructure, data
 oriented infrastructure, scalability of infrastructure, possibly
 better abstractions and capabilities.

Note our focus and context: (i) distributed data scenarios (ii) pilot
 abstraction to address heterogeneity, scalability and extensibilty

Some Research Questions: (i) How does Pilot-Abstraction work in the
 clouds (we've addressed pilot-jobs before, now focus on pilot-data)
 (ii) how to address data distribution in the cloud? (iii) cloud data
 localization requirements: how and when does that become a barrier?
  (iv) related to previous, Can we say something when to use cloud or
 When Grid?

\end{verbatim}

\jhanote{Other Issues worth mentioning: not sure what these are tying
 to say: Does minimizing queue wait time, distributed nature of
 Pilot-abstractions motivate Domain Scientists to use freely available
 Grid resources?  Waiting time and cost increases as Number of
 instances required increase?  HOw is it beneficial than Grids?}


\pmnote{ Scientists believe clouds provide computing infrastructure on demand with minimal or negligible waiting time..  Production cyberinfrastructures involve waiting time if they don't take advantage of Pilot abstractions...  So, what I want to say is- scientists are ignoring existing abstractions and simply moving to cloud.. Is that because of hype? }

\note{Why Domain scientists are moving to cloud? Hype? Due to
  non-availability of necessary simple abstractions to scale
  applications on Grid?} \jhanote{I don't think we should be making
    that argument here for it undermines our own point of view that
    Pilot abstractions have been relatively / modestly successful on
    grids}

\pmnote{ I mean , Given the success the Pilot abstractions on Grids , Domain scientists should start using Pilot abstractions rather than using on demand cloud services - which are expensive - I think its an opportunity to exploit the importance of BigJob and SAGA;  Can we mention some usecases where PJ's effectively supported real science(Tom Bishop usecase)? 
I think as PJ's popularity increases - requirement to use clouds decreases ( since no waiting time between compute units) )}
\pmnote{Does clouds support real science}

\note{These are design objectives: Interoperability, Scalability,
  Extensibility/Flexibility/usability}

%\note{Not in scope any more: Why Iterative MapReduce?What
%  Application?  ( k-means?)Why k-means?  - twister mapreduce used
%  k-means?  - k-means implemented using windows azure}




\section{Related Work}

\subsubsection*{Prior Work:}
\alnote{needs to be aligned with 6}
Distributed CyberInfrastructure is dynamic and also involve widely distributed
heterogenous computing and storage clusters. Pilot abstractions proved to be
effective on DCI for distributed compute and data management. On the other
hand, MapReduce programming model is a popular solution for distributed data
processing and involves map and reduce (compute) phases, and a shuffle (data
movement) phase. Traditional MapReduce implementations like Hadoop are
typically designed for single clusters and when scaled across widely
distributed clusters lead to performance problems. Deployment of Hadoop
cluster on shared dynamic resource pool also involves manual management of
compute and data resources and managing workflows involving MapReduce
execution patterns becomes tedious. Policies of cyberInfrastructures doesn't
allow to execute Hadoop on more than one cluster. These limitations of
traditional implementations motivated to develop a Pilot abstractions based
MapReduce programming model - Pilot-MapReduce for distributed data analysis.

Pilot-MapReduce decouples the logic/pattern of MapReduce from the actual
management of the distributed compute, data and network resources. By
decoupling job scheduling and monitoring from the resource management, PMR can
efficiently reuse the resource management and late-binding capabilities of
BigJob and BigData, which are SAGA based Pilot-Job and Pilot-Data
implementations.

PMR exposes an easy-to-use interface which provides the complete functionality
needed by any MapReduce-based application, while hiding the more complex
functionality, such as chunking of the input, sorting the intermediate
results, managing and coordinating the map and reduce tasks, etc., these are
generically implemented by the
framework~\cite{Mantha:2012:PEF:2287016.2287020}.



\subsubsection*{Related Work} 
\note{Melissa}

\begin{itemize}
	\item Coaster
	\item Venus C (Generalized Worker)
\end{itemize}


\section{Cloud-based Infrastructures and Data-Intensive Applications}

At a high level, cloud computing is defined by Mell/Grance~\cite{nist_cloud}
as a model for enabling convenient, on-demand network access to a shared pool
of configurable computing resources (e.g., networks, servers, storage,
applications, and services) that can be rapidly provisioned and released with
minimal management effort or service provider interaction.

\alnote{Do we want to talk about the ``old'' SaaS, PaaS and IaaS stuff?}

Talk about the evolution of PaaS and IaaS towards general services/building 
blocks

Amazon API
GData API
OpenStack API
OCCI


% Commercial Clouds vs. Science Clouds
Frameworks such as OpenStack~\cite{openstack} and Eucalyptus~\cite{euca} aim
to provide a framework for building a private cloud environment which similar
capabilities as EC2 and S3. FutureGrid's cloud environment currently supports
both framework. Currently, different science cloud infrastructures, e.\,g.\ 
FutureGrid~\cite{futuregrid}.

\begin{itemize}
	\item Resource Model
	\item Provisioning Model
	\item Business Model
\end{itemize}

\subsection{Cloud Storage Services}

An important component for data-intensive cloud applications are the cloud
storage services. Many clouds offers various kinds of storage services with
different characteristics, e.\,g.\ these services usually differ in their read
and/or write performance, supported data volumes, interfaces, reliability and
scalability (see Baron~\cite{baron2010} for an overview of Amazon's storage
services). While clouds often provide storage services on different 
abstraction levels (including e.\,g.\ relation databases), in the following we 
focus on general file-based storage types. In general, cloud storage can be 
classified as follows:
\begin{enumerate}
	\item \textbf{Local Storage:} describes local hard disk directly attached 
	to the compute resource.
	\item \textbf{Network-attached Filesystems:} refers to different forms of 
	distributed (possible parallel) filesystems. The filesystem is commonly 
	exported via the Posix API and a virtual filesystem layer.
	\item \textbf{Distributed Storage:} refers to a highly distributed type of 
	storage systems that spawn across multiple data centers. Access to such 
	storage systems via a common -- often simplified -- namespace and API. For 
	example, cloud systems, such as the Azure Blob Storage, Amazon S3 and 
	Google Storage, provide only a namespace with a 1-level hierarchy. 
\end{enumerate}

A particular novelty for clouds are distributed storage services, such as
Amazon S3, Google Storage and the Azure Blob storage. The storage systems are
optimized for storing large volumes of data and for achieving high
read throughputs. For example, Amazon S3 automatically replicates data across 
multiple data centers within a region. 


Table~\ref{tab:storage-systems} shows an overview of distributed storage 
systems. The focus of this analysis are file-based storage systems. Structured
storage types (e.g. relational databases) and key-/values stores are not 
considered.

\begin{table}[t]
\centering
\begin{tabular}{|p{1.7cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.4cm}|p{1.4cm}|p{1.3cm}|p{1.2cm}|}
	\hline
	\textbf{Storage Type} &\textbf{Azure} &\textbf{Amazon} &\textbf{Google} &\textbf{Open\-Stack} &\textbf{Euca\-lyptus} &\textbf{XSEDE}  &\textbf{OSG} \\
	\hline
	Local	&yes &yes &yes &yes &yes &yes &yes\\
	\hline
	Network Filesystem &Azure Drive &EBS &GCE Block Storage &Nova Volumes &? &Lustre, GPFS 
	&no\\
	\hline
	Distributed Storage &Azure Blob Storage &S3 &Google Storage &Swift & Walrus &GFFS
	 &SRM\\
	\hline	
\end{tabular}
\caption{File-based storage types for different infrastructures (key/value and 
SQL-based storage types omitted) \label{tab:storage-systems}}
\end{table}


% Limitations
While some cloud services that support a high degree of geographic 
distribution have emerged, e.\,g.\ content delivery services such as Akamai 
and Amazon CloudFront, dealing with geographically distributed data remains a 
challenge. However, both Google and Facebook internally deploy systems that 
supports the management of large data volumes across multiple, geographically
dispersed locations. Both Google Spanner~\cite{dean09} and Facebook 
Prism~\cite{Metz12} aim to provide one logical namespace and handle the 
automatic replication and movement of data. However, these capabilities are 
currently not available as external cloud services. If an application requires 
such capabilities, it needs to implement these manually. In this paper we show 
how \pilot abstractions can help to manage geographically distributed data.


\subsection{Data-intensive Applications}

Scenarios
\begin{itemize}
	\item Data is in the cloud or can be brought to the cloud
	\item Distributed data: data resides on different infrastructures, different resources (within a cloud)
\end{itemize}


\subsubsection{Hadoop in the Cloud}

\alnote{How do the different cloud providers chose to re-package Hadoop and 
provide an integrated service?}

\begin{itemize}
	\item Elastic MapReduce
	\item Hadoop on Azure
\end{itemize}

\subsubsection{Other Applications}

1000 Genome project





\section{Pilot Abstractions for Clouds}

% Intro to P* model
The P* model~\cite{pstar12} is an attempt to provide the first minimal but
complete model for describing and analyzing \pilotjobs. The P* model defines
common elements of both compute and data pilot implementations. In this 
section we show how the unified pilot abstractions can be used to support 
data-intensive applications in clouds as well as hybrid grid/cloud 
environments.

Analogous to \pilotjobs (referred to as \pilotcompute in the P* model), the
{\it Pilot-Data} (PD) abstraction provides late-binding capabilities for data
by separating the storage allocation and application-level
\dataunits~\cite{pstar12}. For this purpose, the API defines the {\it
\pilotdata (PD)} and {\it \dataunit (DU)} entity: A \pd function as a
placeholder object that reserves storage spaces for a set of \dus. 

The \pilot-API provides a well-defined interface for accessing the
capabilities of a P* compliment PJ framework. It supports late-binding of
compute and data units by decoupling resource assignment from resource usage.
BigJob~\cite{saga_bigjob_condor_cloud} and 
BigData~\cite{Mantha:2012:PEF:2287016.2287020} provide a unified runtime 
environment for compute and data pilots.



\subsection{Motivation}

\pilotjobs and \pilotdata decouple resource allocation from resource binding
and allow the efficient utilization of resources.\pilot abstractions are a
resource management abstraction that has been used by many communities to (i)
improve the utilization of resources, (ii) to reduce wait times of a
collection of tasks, (iii) to facilitate bulk or high-throughput simulations
where multiple jobs need to be submitted which would otherwise saturate the
queuing system, and (iv) as a basis to implement application specific
scheduling decisions and policy decisions.


% Why pilot abstractions for clouds?
Pilot abstraction have been heavily used on both HPC and HTC infrastructures. 
\begin{itemize}
	\item Cloud/Grid interoperability
	\item Hybrid infrastructures 
	\item Pilots as resource management abstraction (scheduling, affinities)
	\item Pilots as basis for building capabilities in grids/clouds (PMR as 
	example)
\end{itemize}


Pilot abstractions provide a suitable mean to marshall heterogeneous sets of 
both compute and data resources and support the efficient utilization of 
different kinds of commercial as well science cloud resources.

Describe job and data perspective for each of these points
submit to clouds through grids (e.g. OSG/EGI)
Cloud bursting as use case
Data perspective to hybrid infrastructure: data-distribution problem requires different kinds of operations

distributed cyberinfrastructures via the use of an extensible API for the P* Model (Pilot-API).
timeframe of infrastructure change is much larger than for application change
Does the api evolvement make it difficult to applications to follow: what 
about apps that want to use EUCA, OpenStack


\subsection{Data Services and Management}

In general, the following strategies for managing compute and
data in distributed environments exist:

\begin{itemize}
\item A: Data is naturally distributed, processing happens locally to the data. 
\item B: Data is originally localized, but the data needs to be distributed to match the distribution of compute resources.
\item C: Data is moved so as to be localized (i.e., raw data is moved)
\end{itemize}
We want to use hybrid infrastructures in all of these modes 



In most cases, application data is initially pushed to a cloud storage 
service, such Amazon S3, Google Storage and Azure Storage. These service, such 
as S3, are well suited for storing large amounts and support typical 
data-intensive workloads (e.\,g.\ with a lot of sequential reads)

Recently, various markets for data in the cloud have emerged. Cloud providers,
such as Amazon and Microsoft published various scientific data set in these
markets. Amazon e.\,g.\ provides access to various datasets of the 1000 genome
project.



Pilot-Abstractions can marshal the difference between the different cloud storage types and provide a seamless, unifying environment to the application.


\subsection{BigJob and BigData for Clouds}

Pilot-Data provides a unifying interface to a heterogeneous set of data 
sources removing the need to interoperate with different data sources, e.\,g.\ 
repositories, databases etc, providing a virtualized data service layer.

Abstract where and how data is stored...



\begin{figure}[t]
	\centering
		\includegraphics[width=0.7\textwidth]{figures/cloud_pilot_job.pdf}
	\caption{Pilot Abstractions and Clouds\alnote{todo: differentiate between 
	storage and transfer protocol}}
	\label{fig:figures_cloud_pilot_job}
\end{figure}

Figure~\ref{fig:figures_cloud_pilot_job} shows how the Pilot-API and 
BigJob/BigData can be used to manage a heterogenous set of both cloud and grid 
resources. BigJob supports various resource types via a flexible plugin 
architecture. In addition to SAGA, BJ can natively utilize both the EC2 and 
GCE API to launch BJ agents. Similarly, BigData supports various data access
protocols and storage types -- currently, SSH, Globus Online, Google Storage
and Amazon S3.



\begin{figure}[t]
	\centering
		\includegraphics[width=0.7\textwidth]{figures/storage-types.pdf}
	\caption{Pilot-Compute and Pilot-Data on Different Types of Storage Resources}
	\label{fig:figures_storage-types}
\end{figure}

The aim of Pilot abstractions is to marshal the differences in the different 
cloud storage types (see Figure~\ref{fig:figures_storage-types}) and aims to 
provide a unified interface to these services. Affinities are an essential 
tool for modeling the different storage characteristics and to allow an 
effective reasoning about different storage topologies and data/compute 
placements.


Figure~\ref{fig:figures_data-flow} shows the interactions and the data flow 
between \computeunits and \dataunits.
\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.7\textwidth]{figures/data-flow.pdf}
	\caption{DU and CU Interactions and Data Flow}
	\label{fig:figures_data-flow}
\end{figure}



\section{Pilot-MapReduce for Clouds}

Pilot-MapReduce (PMR)~\cite{Mantha:2012:PEF:2287016.2287020} is a pilot-based
implementation of the MapReduce programming model, which decouples the
logic/pattern of MapReduce from the actual management of the compute, data and
network resources. By decoupling job scheduling and monitoring from the
resource management, PMR can efficiently reuse the resource management and
late-binding capabilities of BigJob and BigData.

PMR exposes an easy-to-use interface which provides the complete
functionality needed by any MapReduce-based application, while hiding
the more complex functionality, such as chunking of the input, sorting
the intermediate results, managing and coordinating the map and reduce
tasks, etc., these are generically implemented by the
framework.

PMR allocates both storage and compute resources using \pilot abstractions, in
particular the BigJob and BigData implementation. Having spawned a set of
\pilots, PMR can flexibly utilized for them for executing map and reduce tasks
as well as for managing the input, intermediate and output data.


\alnote{Add layered diagram (incl. PMR) of software stack for different 
infrastructures}
\pmnote{ is it just adding a Pilot-MapReduce layer to Fig:1 ? }

\section{Experiments}

\subsection{Infrastructure}

Compute:
\begin{itemize}
	\item FutureGrid/XSEDE: Sierra, Kraken 
	\item FutureGrid India/Eucalyptus
	\item FutureGrid India/OpenStack
\end{itemize}

Data:
\begin{itemize}
	\item 
\end{itemize}

	
\subsection{Word Count}

Experiments:
\begin{itemize}
	\item  Scale input data: 1GB, 10GB, 100GB, 1000 GB
	\item  Different amounts of intermediate data: 1GB, 10GB, 100GB, 1000 GB
	\item  Number of Resources: 1, 2, 4, 8, 16, 32 VMs
\end{itemize}

\subsection{Pilot-MapReduce}
\begin{itemize}
	\item grid only
	\item cloud only
	\item grid / cloud concurrently
\end{itemize}

\section{Conclusion and Future Work}

\bibliographystyle{wileyj}
\bibliography{literatur,saga,saga-related,local}

\end{document}
