\documentclass{article}

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[hypertex]{hyperref}
\usepackage{subfigure}  
\usepackage{color}
\usepackage{srcltx}
\usepackage{url}
\usepackage[small,it]{caption}


\parskip   = 0.5em
\parindent = 0.0em

\textwidth      = 6.5315 in
\textheight     = 9.0315 in
\oddsidemargin  = 0.0 in
\evensidemargin = 0.0 in
\topmargin      = 0.0 in
\headheight     = 0.0 in
\headsep        = 0.0 in
\textfloatsep   = 0.1in

\newenvironment{shortlist}{
  \vspace*{-0.5em}
  \begin{itemize}
  \setlength{\itemsep}{-0.3em}
}{
  \end{itemize}
  \vspace*{-0.5em}
}

\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\BI}[1]{\textbf{\textit{#1}}}
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\NL}{\newline}

\newif\ifdraft
\drafttrue
\ifdraft
 \newcommand{\jhanote}[1]{  {\textcolor{red}    { ***Shantenu: #1 }}}
 \newcommand{\katznote}[1]{ {\textcolor{cyan}   { ***Dan:      #1 }}}
 \newcommand{\amnote}[1]{   {\textcolor{magenta}{ ***Andre:    #1 }}}
 \newcommand{\hknote}[1]{   {\textcolor{blue}   { ***Hartmut:  #1 }}}
\else
 \newcommand{\jhanote}[1]{}
 \newcommand{\katznote}[1]{}
 \newcommand{\amnote}[1]{}
 \newcommand{\hknote}[1]{}
\fi


\usepackage{ifpdf}
\ifpdf
 \DeclareGraphicsExtensions{.pdf, .jpg}
\else
 \DeclareGraphicsExtensions{.eps, .ps}
\fi

\newcommand{\up}{\vspace*{-1em}}

% CFP:
%
%
% October 22 and 23, 2008
% 
% Dramatic growth in data and equally rapid decline in the cost of
% highly integrated clusters has spurred the emergence of the data
% center as the platform of choice for a growing class of
% data-intensive applications. To encourage conversations between
% those developing applications, algorithms, software, and hardware
% for such "cloud" platforms, we are convening the first workshop on
% Cloud Computing and its Applications (CCA'08).
% 
% This workshop will include a mixture of invited and contributed
% talks on cloud computing, data intensive scalable computing, and
% related topics.
% 
% Topics of interest include:
% 
%   - compute and storage cloud architectures and implementations
% * - map-reduce and its generalizations
% * - programming models and tools
%   - novel data-intensive computing applications
%   - data intensive scalable computing
%   - distributed data intensive computing
%   - content distribution systems for large data
%   - data management within and across data centers
% 
% If you would like to give a contributed talk, please submit a five
% page extended abstract by August 15, 2008. These submissions will
% also be used to select papers for a poster session. Extended
% abstracts should be submitted to:
% https://cmt.research.microsoft.com/CCA2008.


\begin{document}

\title{\large Programming Abstractions for Clouds}

\author{Shantenu Jha$^{12}$,
        Andre Merzky$^{1}$,
        Geoffrey Fox$^{34}$\\[1em]
        %
        $^1$ \small
          Center for Computation and Technology, 
          Louisiana State University\\[-0.3em]
        $^2$ \small
          Department of Computer Science, 
          Louisiana State University\\[-0.3em]
        $^3$ \small
          Community Grids Lab, 
          Indiana University\\[-0.3em]
        $^4$ \small
          Department of Computer Science, 
          Indiana University
       }

\maketitle

\begin{abstract}

  \noindent

%   Clouds seem like 'Grids Done Right', including scalability,
%   transparency, and ease of management. 

  Virtual Machines are the dominant application environments for
  compute Clouds, however, that does not make application programming
  any less relevant than ``non-virtualized'' environments.  The
  limited set of successful scientific Cloud applications show that
  distributed programming patterns of the type of
%  MapReduce, AllPairs, and BigTable are required by a large set of
  MapReduce and All-Pairs are
  required % by a large set of applications,
  to make Cloud infrastructure a viable compute environment for a
  large class of problems.  The existence of multiple implementations
  of these programming paradigms also makes clear, that application
  portability is, even for Clouds, an emerging problem which needs
  addressing beyond the level of system virtualization. %\\[-0.5em]
  This paper discusses these and other challenges around cloud
  applications programming and development, and through a discussion
  of several applications, demonstrates potential solutions. We
  discuss how using the abstractions -- programming interfaces and
  frameworks that support commonly occurring programming and execution
  patterns, enables the efficient, extensible and importantly {\it
    system-independent} implementations of common programming patterns
  such as MapReduce, i.e. same application is usable seamlessly on
  both traditional Grids and Clouds systems. We further discuss that
  lessons learned from programming applications for Grid environment
  also apply, to some extent, to Cloud environments.

\end{abstract}

% \tableofcontents

% \newpage
% 
% -- intro
% 
%   - dominant compute intensive cloud applications are, at the moment,
%     pleasingly distributed or very loosely coupled.  
%   - dominant data intensive cloud applications use novel programming
%     paradigms, e.g. MapReduce, BigTable, etc.
%   - what about other application classes?
% 
% -- Clouds
% 
%   - Clouds seem to be designed to support the first two categories, by
%     their affinity
%   - explain affinity etc etc etc
%   - discuss how affinity allows high level programming abstractions,
%     such as map reduce (would be tough to implement on a not-data
%     affine grid, as performant/redundant GFS would be missing)
% 
% -- programming models and affinity
% 
%   - discuss what programming models are required for other apps
%     classes in table
%   - pick one or two, and details, and derive required affinities
%     - loosely coupled heterogeneous:
%       - long living applications
%       - fault tolerant
%       - compute intensive, CPU colocation within components
%     - tightly coupled, homogeneous
%       - if small tasks:
%         - need to avoid queuing delay (slide in, pools, ...)
%       - if large tasks
%         - need fault tolerance
%       - all need co-location, fast channels, reservation?
% 
%   - separation of concerns
%   - focus on MapReduce (implemented), RepEx (discussing)
% 
% -- conclusions 
% 
%   - Clouds are young, and one should anticipate Clouds with other
%     affinities, and target application classes
%   - for academia, the reverse procedure may prove useful: define the
%     application classes, derive programming models, derive affinities,
%     design Clouds supporting those.  implement on Grids for a good
%     measure ;)


\up
\section{Introduction}

Going by interest garnered, Clouds seem to have emerged as a clear
winner of the (perceived) battle of distributed infrastructures for a
subset of applications, and for the time being at least.  They allow
loosely-coupled, data-intensive applications to be run with an ease,
and with absolutely competitive scalability and throughput.  Not
surprisingly, dominant cloud applications utilize novel and hitherto,
Cloud specific computing paradigms, such as MapReduce, BigTable, or
Hadoop.  These paradigms are supported by the inherent system
capabilities of todays Clouds, which we call
\I{affinities}~\cite{cloud-saga-paper}.  Section~\ref{sec:affine}
discusses these in some more detail.

%We wonder, however, how 

For Clouds to be relevant to the wider scientific computing community
and in general, beyond internet-backed computing, it is important to
understand how other application classes will fare when migrated to Clouds,
or Cloud-like distributed systems.  At the moment it seems unclear if
the system properties and affinities as offered by todays Clouds
support, or even allow, for other application types to perform equally
well.  We will discuss several application classes, which seem to be
most relevant for the academic computing community, in
Section~\ref{sec:apps}.  Instead of just waiting for the 'right'
affinities to emerge with new Cloud incarnations, we propose to rather attempt to
predict what these affinities are, by abstracting the relevant
programming patterns for these application classes, and deriving the
respective system properties required to support these programming
patterns (see Section~\ref{sec:apps}).
 

We outline two specific applications, which to the best of our
knowledge have not been used on Clouds: the first a replica-exchange
based applications, which belongs to loosely-coupled ensemble of
tightly-coupled, homogeneous distributed applications. The second
example is MapReduce based application, as an example of
loosely-coupled data driven applications. We specifically discuss
dominant programming pattern, and describe exemplary Grid based
implementations.  We will then discuss, how these implementation
change (i.e. simplify) for a Cloud system with the appropriate system
affinities.  

% \amnote{the text below seems a repetition from above?}
% 
% The Simple API for Grid Applications (SAGA) can be used to
% pro grammatically develop a very wide-range of distributed
% applications.  In this paper we describe how SAGA has been used to
% develop two different applications from the following classes of
% distributed applications (i) applications based upon the loosely
% coupled of homogeneous sub-tasks and, (ii) applications based upon
% loosely coupled simulations of heterogeneous sub-tasks. The specific
% applications developed are Replica-Exchange simulations using
% Molecular Dynamics and Kalman-Filter based application for reservoir
% simulation.  We briefly discuss the specific applications developed
% and the typical science problems tackled using these applications.
% We will describe the application characteristics of the two
% case-studies, with a focus on the distributed logic of these
% simulations, and not the core simulation logic of the applications.  

% \amnote{Shantenu, I don't think we should focus once again on how
% SAGA manages to hide middleware details.  We made that the focus of
% quite a number of papers already, and it like beating a dead horse
% :-)  I would not be surprised if reviewers say: oh no, yet another
% SAGA paper...  Instead, I'd vote for focusing on the
% Application-in-a-cloud problem, and to only covert advertising for
% SAGA...}
% 
% The paper analyses and
% contrasts the application characteristics of the examples, and shows
% how they are supported using SAGA, often in conjunction with other
% programming frameworks such as Cactus.  The primary aim of this paper
% is to demonstrate how SAGA can be an effective tool for
% programmatically representing and implementing the logic of
% coordination and orchestrating multiple, distributed tasks, while
% remaining agnostic to the actual mechanism, i.e. details of the
% distributed environment. 

% \amnote{And here also: I think we should highlight the need of
% exposing the right system affinities, not on using the right
% frameworks.  Yes, frameworks can deliver that, but can never be as
% efficient as system level features, IMHO.}
% 
% We will highlight the importance of programming abstractions and how
% frameworks that provide common programming patterns can be used to
% simplify the construction of distributed applications. 

We will conclude our discussion with a proposed procedure for defining
scientific-computing oriented Cloud properties, in
Section~\ref{sec:conclusion}.


\up
\section{Cloud Usage Modes and System Affinities}
\label{sec:affine}
 
In ~\cite{cloud-saga-paper} it was shown how Grid system interfaces
(in particular for general purpose Grids) tend to be complete
(i.e. they try to expose a complete set of available system
capabilities), whereas Cloud interfaces tend to be minimalistic
(i.e. they expose only a limited set of capabilities, just enough to
'do the job').
%~\cite{cloud-saga-paper}.
 
 \up
 \subsection{Usage Modes}

  It is important to understand the reason for this difference.  In
  our experience, general purpose Grids are mostly designed bottom-up:
  existing, often heterogeneous resources are federated as VOs, and
  their combined capabilities, plus additional capabilities of higher
  level Grid services, are offered to the end-user.  This is not
  applicable for Clouds: the design of Clouds seems to be, mostly, top
  down. Clouds are designed to serve a limited, specific set of use
  cases and usage modes, and the Cloud system interface is designed to
  provide \I{that} functionality, and no other.  Furthermore, the
  Cloud system itself, and in particular its high level services, may
  be designed to implement specific target use cases, while not
  supporting others (e.g., a Cloud could be homogeneous by design).
  These differences do not imply that Clouds are trivial to implement.
  In practice the opposite is most likely true (due to issues of
  scale, amongst other things). Clouds may very well build upon
  general purpose Grids, or narrow Grids, and at least face the same
  challenges; but their system interfaces do not expose those internal
  capabilities.

  Specific users and user communities tend to create different
  applications but with shared characteristics.  For example, the
  particle data community tends to focus on very loosely coupled, data
  intensive parameter sweeps involving Monte Carlo simulations and
  statistical analyzes.  Systems used by these communities are thus
  designed to support these application classes before others.
  
  The \I{Usage Mode} tries to catch the dominant
  properties of the main application classes, insofar they are
  relevant to the design of the system, and to the operational
  properties of the system.  For example, the usage mode \I{'massively
  distributed, loosely coupled'} implies that the system's design
  prioritizes on compute resources (e.g. cycle scavenging, large
  clusters), and to a lesser degree on communication (no need for fast
  links between application instances), or on reservation and co
  scheduling.

  In contrast, the usage mode \I{'massively distributed,
    tightly-coupled'} would imply a system's design to focus on compute
  resources, but importantly also on fast communication between near
  nodes, and on (physical) co-location of processes.


 \up
 \subsection{Affinities}

  Currently Clouds seem to be designed to mostly support exactly one
  usage mode, e.g.  data storage, \I{or} high throughput computing,
  \I{or} databases, etc.  This does not preclude Clouds targeting more
  than one domain or usage mode, however.  The overarching design
  guideline to support the main target usage modes of Cloud systems,
  we defined as its \BI{affinity}.  In other words, affinity is the
  term we use to indicate the type of computational characteristics
  that a Cloud supports.  That property can very often be expressed as
  the need to use different aspects or elements of a system
  \I{together} (hence the term 'Affinity', in the sense of
  'closeness').  

  For example, the usage mode \I{distributed, tightly coupled}
  implies that an application requires the use of multiple compute
  resources, which need to be 'near' to each other, together with fast
  communication links between these compute resources.  The system
  needs to have a \I{compute-communication affinity}, and a
  \I{compute-compute affinity}.

  Affinities are, however, not always mappable
  to 'closeness'.  For example, we say that a system that supports
  'persistent storage, data replication, data intensive' usage mode,
  may have 'bulk storage affinity' -- in the sense that it needs to be
  designed to have bulk storage properties (availability guarantees,
  long term consistency guarantees etc).  This example also shows that
  affinities are, in some sense, related to Quality of Service (QoS)
  properties exposed by the system, and thus to Service Level
  Agreements (SLAs) about these qualities of service.


 \up
 \subsection{Affinities and Programming Abstractions}

  Affinity is thus a high level characterization of the kind of
  application that could be beneficially executed on a particular
  Cloud implementation, without revealing the specifics of the
  underlying architecture. In some ways, this is the ``ideal
  abstraction'' for the end-user who would like to use infrastructure
  as a black-box.  Some classic examples of affinity are:
  tightly-coupled/MPI affinity, high-throughput affinity (capacity),
  fast-turnaround affinity (capability), or bulk storage affinity.
  Our observation is that Clouds have at least one affinity, a
  corollary to which is that Cloud system interfaces are, designed to
  serve at least one specific set of users or usage modes

  % One can argue that narrow Grids also expose affinity, e.g. that a
  % Data Grid has data affinity.  That may well be true, and we think
  % that the term affinity may be useful for the discussion of narrow
  % Grids as well, but the main difference between Clouds and Grids
  % remain that the interfaces of narrow Grids are still designed so as
  % to expose the complete set of capabilities related to the affinity
  % of narrow Grids, whereas Cloud system interfaces expose the minimal
  % set of capabilities related to its affinities.  For the application
  % developer, but more likely the application deployer, information
  % about the affinity of Clouds should be complemented by SLA
  % information, e.g. providing replicated data in case of loss,
  % co-scheduling at the application level, or low latency
  % communication.  Traditionally SLAs are, implicitly or explicitly,
  % provided by the ``service provider'' based upon infrastructure,
  % policy, usage modes, or negotiation.  For Clouds, SLAs are an
  % implicit part of the system interface: the Cloud's affinities imply
  % a certain QoS to be met, for every use of the system.

  An affinity being the 'ideal system abstraction' has another
  important consequence, as it allows to express suitable programming
  abstractions easily, and natively.  For example, it is certainly
  possible to implement MapReduce on a general purpose Grid, with no
  affinity supporting data replication, or data-compute-colocation.
  The implementation of that abstraction, i.e. the Map Reduce
  application framework, must then however implement these
  capabilities itself, \I{on top} of the system it uses.  On the other hand, if a
  data/compute affine cloud provides these capabilities natively, as
  is the case for, for example, googles proprietary cloud with its google file
  system~\cite{gfs}, then the MapReduce framework can focus on the
  core logic of the programming abstraction, i.e. on the algorithmic
  abstractions, and is thus much easier to implement.
  Note that for the application using the MapReduce framework, there
  is no difference~\cite{gsoc-saga}.

  Clearly, we are arguing for a separation of concerns: we argue that
  application frameworks should not have to deal with exposing,
  expressing, or implementing capabilities which are required \I{by}
  them, but are not part of their algorithmic core.  Those should be
  provided at the system level, which makes the application frameworks
  \I{easily} implementable on any system providing these capabilities,
  i.e. on any system, which has the appropriate affinity.


\up
\section{Distributed Applications Usage Modes}
\label{sec:apps}

 Table~\ref{tab:classes} shows an overview of a number of application
 classes~\cite{dpa_paper} which are widely used in scientific
 computing, and outside.  Often applications in the same class, have
 similar programming models or use programming patterns; for example,
 \I{'pleasingly distributed'} applications, such as the numerous
 \I{'XYZ@Home'} type applications, all share the Master-Worker model,
 in one incarnation or the other.  As compute affine Clouds (aka
 compute Clouds) support that programming paradigm, these applications
 can immediately utilize compute cloud resources with great success.
 For other application classes, such as \I{'tightly coupled,
 heterogeneous'} applications, this is not so obvious, as a compute
 cloud without compute-communication affinity can not easily run a
 communication intensive application efficiently.
 
 \begin{table}[h]
  \begin{center}
   \footnotesize
   \begin{tabular}{|p{.25\textwidth}|p{.27\textwidth}|p{.39\textwidth}|}
     \hline
 
     \B{Application Class}                                 &
     \B{Data    Driven}                                    &
     \B{Compute Driven}                                    \\\hline
 
     \B{Pleasingly Distributed}                            &
        SETI$@$home                                        &
        Monte Carlo Simulations of Viral Propagation       \\\hline
 
     \B{Loosely Coupled,\NL Homogeneous}                    &
        Image Analysis                                     &
        Replica Exchange Molecular Dynamics of Proteins    \\\hline
 
     \B{Tightly Coupled,\NL Homogeneous}                    &
        Semantic Video Analysis                            &
        Heme Lattice-Boltzmann Fluid dynamics              \\\hline
 
     \B{Loosely Coupled,\NL Heterogeneous}                 &
        Multi-Domain Climate Predictions                   &
        Kalman-Filter Fluid Dynamics                       \\\hline
 
     \B{Dynamic Event Driven}                              &
        Disaster support                                   &
        Visualization                                      \\\hline
 
     \B{First Principle, Distributed}                      &
        MapReduce-Based Web indexing                       &
        MapReduce-Based Motif Distributed search           \\\hline
 
   \end{tabular}
   \caption{\footnotesize \B{Examples of primary categories 
            of distributed applications\cite{dpa_paper}.}}
   \label{tab:classes}
  \end{center}
 \end{table}

 We want to demonstrate using two examples, how the implementation of the
 respective programming patterns used by these application classes can
 be supported by the Cloud affinities\footnote{Both applications have
 been implemented using SAGA~\cite{saga-core}, but we do not, in this
 paper, intent to focus on SAGA as a solution to the discussed problem
 space, but merely use it as means to an end.}.

 \up
 \subsection{Example 1: MapReduce}

  MapReduce~\cite{mapreduce-paper} is a programming framework which
  supports applications which operate on very large data sets on
  clusters of computers.  MapReduce relies on a number of capabilities
  of the underlying system, most related to file operations, but also
  related to process/data colocation.  The Google file system, and
  other global file systems, provide the relevant capabilities, such
  as atomic file renames.  Implementations of MapReduce on these file
  systems can focus on implementing the the dataflow pipeline, which
  is the algorithmic core of the MapReduce framework.

  We have recently implemented MapReduce in SAGA, targeting general
  purpose Grid systems, where the system capabilities required by
  MapReduce are usually not natively supported -- instead, a general
  purpose grid provides a much larger set of lower level operations.
  Some semantics, such as again the atomic file rename, is provided by
  the SAGA API layer, others, such as data/compute colocation are not.
  Our implementation is thus required to interleave the core logic
  with explicit instructions on where processes are to be scheduled
  when operating on specific parts of the data set~\cite{gsoc-saga}.
  Note that some of the required capabilities can be provided by
  higher level grid services -- those are, however, often not
  standardized, and often not available in general purpose Grids.

  The advantage of this approach is obviously that our implementation
  is no longer bound to run on a system providing the appropriate
  semantics originally required by MapReduce, but is portable to
  other, more generic systems as well.  The drawback of the approach
  is obvious as well: our implementation is relatively more complex,
  as it needs to add semantic system capabilities at some level or the
  other, and it is inherently slower, as it is for these capabilities
  very difficult or near impossible to obtain system level performance
  on application level.  But many of these are due to the early-stages
  of the implementation of SAGA, and not a fundamental limitation of
  the design or concept.

  \B{Summary:} A data affine, and compute/data affine environment with
  the capabilities listed above provides a clear separation of
  concerns for MapReduce implementations.


 \up
 \subsection{Example 2: Replica Exchange}

 Replica Exchange~\cite{repex} is a simulation method which improves
 the properties of Monte Carlo simulations: according to certain
 specific measure (here of temperature), two MC simulation instances
 exchange their configuration.  That procedure improves the mixing
 properties of set of MC
 simulations.

 In our specific application case~\cite{saga-migol-paper}, the
 implemented replica-exchange framework relied on the following system
 capabilities: 

  \begin{shortlist}
   \item efficient execution of many jobs
   \item fault tolerant job execution
   \item efficient exchange of small data items (MC temperature)
  \end{shortlist}

  Again, our implementation targets general purpose Grids.
  Typical for Grids, as discussed in Section~\ref{sec:affine},
  no production Grid system available to us provides that
  complete set of capabilities, but all provide a set of lower
  level capabilities, which can be used to implement the
  required ones.

  We deployed an application level fault tolerance service,
  Migol~\cite{migol}, and interleaved checkpoint/restart
  instructions with the original application code, using the
  SAGA CPR package.  Our implementation can thus operate on
  general purpose Grids, as it does not have strong requirements
  on system capabilities, but again the application development
  process is unnecessarily complicated, and definitely not
  focused on the core method implementation.


% \section{Discussion}
% \label{sec:discussion}

\section{Conclusions}
\label{sec:conclusion}


 We have argued above that affinities guide, as well as support, the
 implementation of high level programming abstractions.
 Clouds\footnote{\url{http://cloudcomputing.qrimp.com/}} currently
 however, offer a very finite list of affinities: They are either
 compute centric (e.g. EC2)
 % ~\cite{EC2}, or 
 data centric (e.g. S3)
 % ~\cite{S3}, 
 although some Clouds also offer data-compute colocation.  
 % But that is about all you can find -- 
 There are no Clouds which are communication affine, and would thus
 invite users of tightly-coupled applications, or offer event-driven
 application execution, or pipeline execution modes, etc.  If Clouds
 are to make a broader impact on Scientific Computing, this deficit
 will need to be addressed. 
 % The for that is, simple: the commercial cloud providers see, at the
 % moment, no market for other specialized Clouds.

 Trying to migrate applications to a cloud environment which does not
 offer the appropriate usage modes and affinities is very like a
 painful and tedious endeavor, and will likely show similar
 pain-points to the development of Grid applications.  Also,
 performance and scalability may not be what the user expects.

 On the other hand, a Cloud environment which does offer the suitable
 usage modes and affinities can significantly ease the work of the
 application developer, and likely yield the expected turnaround.

 In many ways, that can be compared to the situation of application
 development for Grids environments: where the higher level grid
 services and abstraction layers are missing in Grid environments,
 application development and deployment is painful, with often limited
 returns.  Clouds can inherently offer these higher abstraction layers
 -- but not always the ones required.

 For scientific computing, the road to successful Cloud deployment
 seems clear: depending on the target application classes, one needs
 to derive the required fundamental affinities the system needs to
 expose -- which may, or may not, differ significantly from those
 offered by commercial Cloud providers.


\up
\section{Acknowledgments}
\label{sec:acks}

 This work is based upon the insight gained from many SAGA projects,
 especially the SAGA-MapReduce and SAGA-Replica Exchange
 implementation projects. We would like to thank Chris and Michael
 Miceli, Andre Luckow, Joohyun Kim, and the wider SAGA team at CCT.

\footnotesize
\bibliographystyle{plain}
\bibliography{cca08}

\end{document}

