Title: Introducing Daton: The fundamental unit of data-intensive
       distributed Computing..

Outline:

 - in publications 1 and 2, we have shown how saga can be used
 to implement data intensive applications such as mapreduce 
 in an infrastructure independent fashion. 

 We showed how performance depends upon a complex interplay of
 i) data-assigned per worker, i) degree-of-distribution (number of
 workers/ number of distributed workers).

 [Q. Should we call i)  data-density or worker-density?]

 - In this paper, we extend the implementation to SAGA-AllPairs.
 Taking a cue from publication 3, we use "image overlap" as the
 comparision metric.  Analagous to the MapReduce framework, we develop
 an All-Pairs framework. We show how an application developer has to
 just provide the function to be used.

 - The user configures which resources to use, and which matrix
 elements get computed on which resource.


 - Performance depends upon: (data-size/worker) per degree-of- distribution
 we call this a Daton. This is a fundamental(!) unit. We provide
 support of programmatically controlling Daton. And adapting this number
 dynamically as system-configurations changes.

 - We repeat the basic performance tests for different comparision
 function. 

 To increase performance, with increasing data size should you:
  -  i)   increase the number of workers uniformally on all resources
      (this increases coordination overhead)
  -  ii)  number of workers on a given resource? 
      (this keeps the local worker-density constant)
  
 The answer to the above two obviously depends whether: 
 -  workloads are homogenous?
 -  workloads are heterogenous?

 -  iii) what if ratio is time-sensitive?
 
 Q: Can we dynamically spawn workers, if the load increases above
 a certain ratio? Is this ratio tunable? Learnable?

 Given a workload, what is the T_c for:
  - 2 workers locally 
  - 2 workers distributed over different systems using local filesystems
  - 2 workers using different systems but using distributed file system

 
