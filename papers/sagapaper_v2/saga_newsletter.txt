The role of computing as the third pillar of investigation in Science
& Engineering (S&E) along with theory and experiment is well
established.  Improvements in algorithms, numerical methods and
Moore's Law have all contributed to major advances in S&E from
computing. In spite of these impressive growths, more computational
capacity will be required for the next-generation of S&E challenges
than is currently or locally available, whilst concomitantly being
able to utilize the increasing computational capacity in novel and
interesting ways.  What is thus required is a balanced yet
sophisticated cyberinfrasturcture --- software layers to build the
application capabilities, tools and services upon the physical layer
(grids, clouds, special purpose architecture and ``vanilla''
high-performance machines/clusters), and scalable approaches to
utilize the cyberinfrastructure.

One way to address the requirements, viz., the need for advanced usage
modes and scaling --- scaling-up (the number of cores), scaling-out
(the number of tasks), as well as scaling the number of resources that
can be utilized, is to federate computing resources.  Federation here
refers not just of machines, but also integrated capabilities,
services and tools.  The federation of previously independent and
individual resources, results in extensible and scalable capabilities
that are more powerful than the simple sum of its parts.  The set of
techniques and technologies used to facilitate resources is
collectively called Grid Computing.

Although federating resources is a great way to scale the number of
resources usable, all too often the resources being federated are
owned, operated and designed for different applications and usage
modes, and thus inherently different.  Thus federation introduces new
challenges of heterogeneity in software stacks, policies and resource
performances, as well greater modes of failures, all of which result
in complexity at both the system and application levels.

Our approach to addressing the persistent and fundamental challenge of
enabling applications and tools to inter-operate across federated and
thereby heterogeneous resources, has been to design and develop a
unified and simplified interface --- known as SAGA, which provides the
most frequently required functionality to construct applications
capable of utilizing distributed federated cyberinfrastructure.

There are three functional levels to SAGA: at the upper level there is
the "public" interface; at the lowest level there are adaptors that
translate the semantics exposed by the public interface to the
specific back-end on which the SAGA API will be used. In between these
two levels is a library that dynamically invokes (loads) the
appropriate adaptors for the desired back-end.

Given the competing design objectives, a trade-off is needed between
the complexity arising from interoperability versus simplicity from
specificity. These adaptors are an important design consideration, as
they ensure that the same public-interface can be utilized on
different back-end resources, whilst ensuring that the complexity is
localized to the adaptor level and not the API or the library
implementing it. Thus the adaptor-based approach is critical as they
ensure interoperability and extensibility to new and diverse
"middleware", by ensuring and insulating the application from any
changes and churn at the middleware layers below.

SAGA is used in a variety of application and CI projects. To name just
a few -- it is used by the KEK project in Japan in their studies of
neutrinos, it has been used by scientists associated with CERN in the
search for fundamental particles such as the Higgs, it is used by
Airbus, and finally by a multitude of scientists using NSF
national-scale production infrastructure such as XSEDE and OSG to meet
the need for scalable resources. Here at Rutgers, as part of a
prestigious $1.6M NSF award, several physical scientists are using
SAGA to provide an efficient way of utilizing advanced computing
infrastructure to implement and refine algorithms emanating in
statistical-physics.  Given the importance of federation of resources
and the inherent complexity in accomplishing it, the use of SAGA by
multiple S&E applications, tools and services is testimony to the
effectiveness of both its design and implementation. 

Given the challenge of supporting large-scale scientific projects and
collaborations, SAGA must be very professionally engineered and
developed. In fact, it is developed to be "industrial strength", also
referred to as supporting production-grade science and engineering.
In addition to a sophisticated software engineering project, SAGA
provides the opportunity to address multiple research challenges, in
fundamental distributed cyberinfrastucture as well as in distributed
applications. For example, traditionally for HPDC applications,
resources have been always pre-assigned to computational tasks, i.e.,
static execution.  But static execution is known to have limitations,
for example, early assignment of resources is known to hinder the
efficient and optimal matchmaking between available resources and
tasks, which can be alleviated by dynamic and late-binding of tasks to
resources. Thus, an important question at the heart of distributed
compuiting is what programming models best support such dynamic
execution?  What abstractions --- conceptual and software are required
to support "flexible and dynamic" model of computing?  How can such
abstractions be implemented to be interoperable and extensible.  Many
of these answers are being addressed as part of the SAGA project.
Last but not least, SAGA is now a global distributed computing
standard.

The SAGA project has been funded by multiple international grants
including from US NSF, EU, UK EPSRC and DoE. For more information on
SAGA and our research visit http://saga-project.org and
http://radical.rutgers.edu

