Improvements in algorithms, numerical methods and Moore's Law have
enabled computing to contribute to major advances in Science &
Engineering (S&E). In spite of impressive growths from each of these
factors however, in order to effectively solve the next-generation of
S&E challenges, more computational capacity will be required than is
typically available locally, whilst concomitantly being able to
utilize the increasing computational capacity in novel and interesting
ways. This suggest the need for a advanced, balanced and comprehensive
cyberinfrastructure (CI) --- comprised of software layers (the
application capabilities, tools and services) and physical layers
(grids, clouds, special purpose architecture and ``vanilla''
high-performance machines/clusters), along with scalable approaches to
utilize the CI.

One way to provide the CI that can support novel usage modes and
scaling --- scaling-out (the number of tasks) as well as scaling the
number of resources, is to federate computing resources.  Federation
here refers not just to machines, but also services and tools.  The
federation of previously independent and individual resources, results
in extensible and scalable capabilities that are more powerful than
the simple sum of its parts.  The set of techniques and technologies
used to federate resources is collectively called Grid Computing.

Although federating resources is a great way to scale the number of
resources usable, all too often the resources being federated are
owned, operated and designed for different applications and usage
modes, and thus inherently different. Federation therefore introduces
the challenge of interoperability, arising from heterogeneity and
complexity in software stacks, policies and resource performance, all
of which result in barriers to interoperation at both the system and
application levels.

Our approach to addressing the persistent and fundamental challenge of
enabling applications and tools to interoperate across federated, and
thereby heterogeneous resources, has been to design and develop a
widely usable yet simplified interface --- known as SAGA, which
provides the most frequently required functionality, such as remote
job-submission, remote file/data transfer, remote coordination, etc.,
to construct applications capable of utilizing distributed federated
CI.

There are three functional levels to SAGA: at the upper-level there is
the "public" interface; at the lowest-level there are adaptors that
translate the semantics exposed by the public interface to the
specific back-end systems on which the SAGA API will be used. In
between these two levels, is a library that dynamically invokes
(loads) the appropriate adaptors for the desired back-end.

Adaptors are an important design consideration, as they ensure that
the same public-interface can be utilized on different back-end
resources, whilst ensuring that the complexity arising from
heterogenety is localized to the adaptor level and not the API or the
library implementing it. The adaptor-based approach is thus critical
as they ensure interoperability and extensibility to new and diverse
"middleware", by insulating the application from any changes and churn
at the middleware layers below.

SAGA is used by a variety of CI projects and a host of applications
seeking to utilize CI in advanced and scalable ways. To name just a
few: it is used by the KEK project in Japan in their studies of
neutrinos, it has been used by scientists associated with CERN in the
search for fundamental particles such as the Higgs, it is used by
Airbus, and finally by a multitude of scientists using NSF
national-scale production infrastructure such as XSEDE and OSG to meet
the need for scalable resources. Here at Rutgers, as part of a
prestigious NSF Cyber-enabled Discovery and Innovation award, several
physical scientists are using SAGA to provide an efficient way of
utilizing advanced computing infrastructure to implement and refine
algorithms emanating in statistical-physics.  Given the importance of
federation of resources and the inherent complexity in accomplishing
it, the use of SAGA by multiple S&E applications, tools and services
is testimony to the effectiveness of both its design and
implementation.

Given the challenge of supporting large-scale scientific projects and
collaborations, SAGA has been developed to be "industrial strength",
i.e., support multiple large-scale and production-grade science and
engineering projects.  In addition to requiring sophisticated software
engineering, SAGA provides the opportunity to address multiple
research challenges in fundamental DCI as well as in distributed
applications. For example, for HPDC applications, computational tasks
have traditionally been assigned far in advance to the resources they
will run on, i.e., static resource determination.  But static
execution is known to have limitations. For example, early assignment
of resources is known to hinder the efficient and optimal matchmaking
between available resources and tasks; this can be alleviated by
dynamic and late-binding of tasks to resources. Thus, an important
question at the heart of distributed compuiting is what programming
models best support such dynamic execution?  What abstractions ---
conceptual and software are required to support a flexible and dynamic
model of executing HPDC application ?  How can such abstractions be
implemented to be interoperable and extensible?  Many of these answers
are being addressed as part of the SAGA project.

Interestingly, but not suprisingly, SAGA is now a global distributed
computing standard and is being used by nearly every major production
DCI the world over.  The SAGA project has been funded by multiple
international grants including from US NSF, EU, UK EPSRC and DoE. For
more information on SAGA and our research visit
http://saga-project.org and http://radical.rutgers.edu

