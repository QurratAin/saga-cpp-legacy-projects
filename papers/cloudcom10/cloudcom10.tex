\documentclass[conference,final]{IEEEtran}

\usepackage{latex8}
\usepackage{times}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}    
\usepackage{multirow}    
\usepackage{listings}   
\usepackage{times}     
\usepackage{paralist}    
\usepackage{wrapfig}    
\usepackage[small,it]{caption}
\usepackage{multirow}
\usepackage{ifpdf}


\usepackage{listings}
\usepackage{keyval}  
\usepackage{color}
\definecolor{listinggray}{gray}{0.95}
\definecolor{darkgray}{gray}{0.7}
\definecolor{commentgreen}{rgb}{0, 0.4, 0}
\definecolor{darkblue}{rgb}{0, 0, 0.4}
\definecolor{middleblue}{rgb}{0, 0, 0.7}
\definecolor{darkred}{rgb}{0.4, 0, 0}
\definecolor{brown}{rgb}{0.5, 0.5, 0}

\lstdefinestyle{myListing}{
  frame=single,   
  backgroundcolor=\color{listinggray},  
  %float=t,
  language=C,       
  basicstyle=\ttfamily \footnotesize,
  breakautoindent=true,
  breaklines=true
  tabsize=2,
  captionpos=b,  
  aboveskip=0em,
  belowskip=-2em,
  %numbers=left, 
  %numberstyle=\tiny
}      

\lstdefinestyle{myPythonListing}{
  frame=single,   
  backgroundcolor=\color{listinggray},  
  %float=t,
  language=Python,       
  basicstyle=\ttfamily \footnotesize,
  breakautoindent=true,
  breaklines=true
  tabsize=2,
  captionpos=b,  
  %numbers=left, 
  %numberstyle=\tiny
}

\newcommand{\up}{\vspace*{-1em}}
\newcommand{\upp}{\vspace*{-0.5em}}
\newcommand{\numrep}{8 }
\newcommand{\samplenum}{4 }
\newcommand{\tmax}{$T_{max}$ }
\newcommand{\tc}{$T_{C}$ }
\newcommand{\bj}{BigJob}

\title{Running Bio-ensemble Simulations on Azure and Hybrid, Heterogeneous Cloud Infrastructures}

\author{
Andr\'e Luckow$^{1}$, Shantenu Jha$^{1,2,3,*}$,\\
  \small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State University, USA}}\\
  \small{\emph{$^{2}$Department of Computer Science, Louisiana State University, USA}}\\
  \small{\emph{$^{3}$e-Science Institute, Edinburgh, UK}}\\
  \small{\emph{$^{*}$Contact Author: \texttt{sjha@cct.lsu.edu}}}\\
}

%\date{}

\def\acknowledgementname{Acknowledgements}
\newenvironment{acknowledgement}%
{\section*{\acknowledgementname}%
\parindent=0pt%
}

\newif\ifdraft
%\drafttrue
\ifdraft
\newcommand{\llnote}[1]{ {\textcolor{green} { ***JK: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***AL: #1 }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\else
\newcommand{\llnote}[1]{}
\newcommand{\alnote}[1]{}
\newcommand{\jhanote}[1]{}
\fi

\begin{document} 

\maketitle    

\begin{abstract}
  
\end{abstract}

\section{Introduction}

Scope:
\begin{itemize}
    \item Running Bio-ensembles on Azure and hybrid Grid/Clouds
    \item Amazon Cluster Compute benchmark
    \item Synchronous ARepex
    \item Fluctuation of runtime on the different VMs (set case for an asynchronous algorithm)
    \item Performance/cost as metric and argument
\end{itemize}


\section{SAGA and SAGA-based Frameworks for Large-Scale and
  Distributed Computation} 

SAGA~\cite{saga_url} provides a simple, POSIX-style API to the most
common Grid functions at a sufficiently high-level of abstraction so
as to be independent of the diverse and dynamic Grid environments. The
SAGA specification defines interfaces for the most common
Grid-programming functions grouped as a set of functional packages
(Fig.~\ref{Fig:SAGA1}). Some key packages are:

\begin{figure}[!ht]
 \begin{center}
     \includegraphics[width=0.45\textwidth]{figures/stci_saga_figures-1.pdf}
 \end{center}
\caption{\small Layered schematic of the different components of the
   SAGA landscape. At the topmost level is the simple integrated API
   which provides the basic functionality for distributed
   computing. Our BigJob abstraction is built upon this SAGA layer
   using Python API bindings.\up} \label{Fig:SAGA1}
\end{figure}

\begin{itemize}
\item File package - provides methods for accessing local and remote
 filesystems, browsing directories, moving, copying, and deleting
 files, setting access permissions, as well as zero-copy reading and
 writing.
\item Job package - provides methods for describing, submitting,
 monitoring, and controlling local and remote jobs. Many parts of
 this package were derived from the largely adopted
 DRMAA specification. 
% ~\cite{drmaa_url} 
\item Other Packages, such as the RPC (remote procedure call), Replica
  package and Stream Package.  
  % remote socket connections with hooks to
  %   support authorization and encryption schemes.
\end{itemize}

In the absence of a formal theoretical taxonomy of distributed
applications, Fig.~\ref{Fig:sagaapps} can act a guide.  Using this
classification system, there are three types of distributed
applications: (i) Applications where local functionality is swapped
for distributed functionality, or where distributed execution modes
are provided.  A simple but illustrative example is %an ensemble of
an application that uses distributed resources for bulk
submission. Here, the application remains unchanged and even unaware
of its distributed execution, and the staging, coordination, and
management are done by external tools or agents. Most applications in
this category are classified as implicitly distributed.  (ii)
Applications that are naturally decomposable or have multiple
components are then aggregated or coordinated by some unifying or
explicit mechanism.  DAG-based workflows are probably the most common
example of applications in this category.  Finally, (iii) applications
that are developed using frameworks, where a framework is a generic
name for a development tool that supports specific application
characteristics (e.g. hierarchical job submission), and recurring
patterns (e.g. MapReduce, data parallelism) and system functionality.
Lazarus~\cite{enkf-gmac09} provides several autonomic features, such as
fault tolerance and dynamic resource selection, specifically for 
Ensemble-Kalman-Filter application scenarios.
SAGA has been used to develop system-level tools and applications for
each of these types.



\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.45\textwidth]{figures/distributed_applications_saga_figure.pdf}
  \end{center}
  \caption{\small Showing the ways in which SAGA can be used to
    develop distributed applications.  The different shaded box
    represent the three different types; frameworks in turn can
    capture either common patterns or common application
    requirements/characteristics. \label{Fig:sagaapps} \up}
\end{figure}

It is important to note that SAGA provides the basic API to implement
distributed functionality required by applications (typically used
directly by the first category of applications), and is also used to
implement higher-level APIs, abstractions, and frameworks that, in
turn, support the development, deployment and execution of distributed
applications~\cite{enkf-gmac09}. Merzky et\,al.~\cite{sagamontage09} 
discusses how SAGA was used to implement a higher-level API to support
workflows. In this paper, we will discuss how SAGA can be used to
implement runtime frameworks to support the efficient execution of the
distributed applications.

\section{Windows Azure}

Azure provides different higher level services, e.\,g.\ the Azure
AppFabric or Azure Storage, that can be accessed via HTTP/REST from
anywhere. Windows Azure offers a platform for on-demand computing and
for hosting generic server-side applications. The so-called Azure
fabric controller automatically monitors alls VMs, automatically
reacts to hardware and software failures and manages application
upgrades.

\subsubsection{Compute}
Windows Azure formalizes different types of virtual machines into
so called roles. Web roles e.\,g.\ are used to host web applications
and frontend code, while worker roles are well suited for background
processing. While these roles target specific scenarios, they are also
highly customizable. Worker roles can e.\,g.\ run native code. The
application must solely implemented a defined entry point, which is
then called by Azure. The Azure fabric controller automatically
manages and monitors applications, handles hardware and software
failures as well as updates to the operating system or to the
application.  Commonly, scientific applications utilize worker roles
for compute- and data-intensive tasks. AzureBlast~\cite{azure_blast}
e.\,g.\ heavily relies on worker roles for computing bio-sequences.

\subsubsection{Storage}
For storing large amounts of data the Azure storage platform
provides three key services: the \emph{Azure Blob Storage} for storing
large objects of raw data, the \emph{Azure Table Storage} for
semi-structured data and the \emph{Azure Queue Storage} for
implementing message queues.  The data is storage replicated across
multiple data centers to protect it against hardware and software
failures. In contrast to other cloud offerings (e.\,g.\ Amazon S3) the
Azure Storage Services provide strong consistency guarantees, i.\,e.\
all changes are immediately visible to all future calls. While
eventual consistency as implemented by S3~\cite{1294281} usually
offers a better performance and scalability, it has some disadvantages
mainly caused by the fact that the complexity is moved to the
application space.

The blob storage can store file up to a size of 1\,TB, which makes it
particularly well suited for data-intensive application. The Amazon S3
service e.\,g.\ restricts the maximum file size to 5\,GB. Further, the
access to the blob storage can be optimized for certain usage modes:
\emph{block blob} can be split into chunks which can be uploaded and
downloaded separately and in parallel.  Thus, block blobs are well
suited for uploading and streaming large amounts of data. \emph{Page
  blob} manage the storage as an array of pages. Each of these pages
can be addressed individually, which makes page blobs a good tool for
random read/write scenarios. \emph{Azure XDrive} provides a durable
NTFS volume, which is backed by a page blob. In particular legacy
applications that heavily utilize file-based storage can simply be
ported to Azure using XDrive.

The Azure Queue Service provides a reliable storage for the delivery
of messages within distributed applications.  The queue service is
ideal to orchestrate the various components of a distributed
applications, e.\,g.\ by distributing work packages or collecting
results, which could be running on Azure or on another resource,
e.\,g.\ a science cloud.

The Azure Table Storage is ideally suited for storing structured
data. Unlike traditional relational database systems, the table
storage is designed with respect to scale-out, low cost and high
performance similar to Google's BigTable~\cite{bigtable2006}
system. For legacy application Azure also provides a SQL-Server based,
relational datastores called SQL Azure. In contrast to Azure tables,
SQL storage supports common relation database features, such as
foreign keys, joins and SQL as query language.


\section{BigJob: SAGA-based Pilot-Job Framework}
%\up

%\jhanote{do we want \emph for bigjob?} \alnote{yes,looks good for introducing the term.}
\emph{BigJob} is a SAGA-based Pilot-Job implementation. In contrast to
other Pilot-Job implementations, e.g., Falkon, BigJob natively
supports parallel applications (e.g. based on MPI) and works
independent of the underlying Grid infrastructure across different
heterogeneous backend, e.\,g.\ Grids and Cloud, reflecting the
advantage of using a SAGA-based approach. Further, the framework is
extensible and provides several hooks that can be used to support
other resource types and Pilot-Job frameworks.
% \jhanote{We have talked about interoperability; need to address extensibility} 
% \alnote{Added sentence before}

As shown in Figure~\ref{fig:figures_distributed_pilot_job}, BigJob currently
provides a unified abstraction to Grids, Condor pools and
Clouds. Using the same API, applications can dynamically allocate
resources via the big-job interface and bind sub-jobs to these
resources. In the next sections, we describe how SAGA interfaces to
three different backends, while exposing the same user-level BigJob interface
and semantics. A tutorial that describes the BigJob API can be found 
at~\cite{bigjob_cloud_tutorial}.

% \jhanote{Should the second level in the upperbox be ``SAGA BigJob
%   API''?}\alnote{changed}
%\up
\begin{figure}[htbp]
    \centering
        \includegraphics[width=0.45\textwidth]{figures/distributed_pilot_job}
        \caption{\textbf{Overview of the SAGA-based Pilot Job:} The
          SAGA Pilot Job API is currently implemented by three
          different backends - one for Grids, Condor and for
          Clouds.\up}
    \label{fig:figures_distributed_pilot_job}
\end{figure}




\subsection{BigJob for Grids}
\up

Figure~\ref{fig:figures_bigjob} shows an overview of the SAGA BigJob
implementation for computational Grids. The Grid BigJob comprises of
three components: (i) the BigJob Manager that provides the Pilot-Job
abstraction and manages the orchestration and scheduling of BigJobs
(which in turn allows the management of both big-job objects and
sub-jobs), (ii) the BigJob Agent that represents the pilot job
\jhanote{Can we use something other than pilot job} and thus, the
application-level resource manager on the respective resource, and
(iii) advert service which is used for communication between the
BigJob Manager and Agent.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/bigjob}
   \caption{BigJob Architecture: The core of the framework, the
      BigJob Manager, orchestrates a set of sub-jobs via a
      BigJob Agent using the SAGA job and file APIs.  The
      BigJob Agent is responsible for managing and monitoring sub-jobs.\up}
   \label{fig:figures_bigjob}
\end{figure}

Applications can utilize the framework via the big-job and sub-job
classes. Both interfaces are syntactically and semantically consistent
with the other SAGA APIs: a sub-job e.\,g.\ is described using 
the standard SAGA job description; job states are expressed
using the SAGA state model. Before running sub-jobs, an application must initialize
a big-job object. The BigJob Manager then queues a job,
% \jhanote{are we confusing the reader by toggling between Pilot-Job
%   and BigJob?} \alnote{ok, change to Grid job}
which actually runs a BigJob Agent on the respective remote
resource. For this agent a specified number of resources is
requested. Subsequently, sub-jobs can be submitted through the BigJob
Manager using the job-id of the BigJob as reference. The BigJob
Manager ensures that sub-jobs are launched onto the correct
resource based upon the specified job-id using the right number of
processes. 

The Grid BigJob is a SAGA-based framework and heavily utilizes the
SAGA job, file and advert APIs. The BigJob Agent is started using the
SAGA job API.  Communication between the BigJob Agent and BigJob
Manager is carried out using the SAGA advert service, a central
key/value store. For each new job, an advert entry is created by the
BigJob Manager. The agent periodically polls for new jobs. If a new
job is found and resources are available, the job is dispatched,
otherwise it is queued.

% \jhanote{But mention that this work is different in that, (i) it
%   establishes the general principles and design, and is used for a
%   range of applications, (ii) extends capabilities to work with
%   Clouds, and (iii) sets the stage for extensible Pilot-Job
%   functionality, e.g., policy-driven pilot-jobs.  Need to mention the
%   general purpose nature: both HTC and HTC of HPC, as well that it is
%   more than just a container of ensemble based applications.  It
%   provides the framework for deploying and supporting
%   application-level runtime environments.}

In this paper we will use BigJob to support HPC applications, however,
BigJob has also been used to support high-throughput runs of HPC
applications on the TeraGrid. Specifically, 
El-Khamra et\,al.~\cite{enkf-gmac09} uses BigJob to support a very
large number of ensembles with highly variable run-times, as well as
heterogenous resource requirements.  In Luckow et\,al.~\cite{repex_ptrs}
although an efficient scale-out behaviour was established, all resources
utilized were TeraGrid resources, and thus accessible using a uniform
Globus interface. Another use cases that have used BigJob on Grid
resources include applications involving parameter sweeps and which
require the execution of a very large number of simulations in
parallel.  Having outlined the general principles and design
objectives of the BigJob framework, and established its use on
production grid infrastructure for a range of applications, we will
focus on extending its capabilities to work with Clouds and other
distributed infrastructure.



\subsection{BigJob for Azure}
\up


Figure~\ref{fig:figures_bigjob_azure} illustrates the architecture of the BigJob
for Azure building blocks. The BigJob Manager (BM) is responsible for accepting 
simulation requests from the end-user and for orchestrating the sub-job runs. 
In the future these capabilities can be extended by supporting more advanced steering and
visualization features. The BM creates work packages for each sub-job and 
distributes them via the Azure Queue Service and
later collects the results stored by the replicas in the Azure
storage.  The BigJob Agents run within Azure worker roles, which are
ideally suited for running background tasks. Azure enables users to
run native code within worker roles, i.\,e.\ the framework will be
able to support numerous MD codes, e.\,g.\ NAMD~\cite{Phillips:2005gd}
and AMBER~\cite{cheatham-5}.  Azure currently does not support MPI
computations across multiple worker roles, thus each MD
simulation is limited to 8 cores.



%\begin{wrapfigure}{R}{0.4\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{figures/bigjob_azure}
    \caption{\textbf{Azure-based BigJob:} The framework
      utilizes the queue storage for distributing work
      packages from the BigJob Manager to the
      BigJob agents running on multiple worker roles.}
    \label{fig:figures_bigjob_azure}
\end{figure}


The worker roles running the BigJob agent are managed by the BigJob Manager using
the Azure Service Management API. In the initial version we will
support the automatic start and stop of hosted services. In the final
version there will be a possibility to automatically deploy agent code
without the need to pre-configuring the VMs. Once the agents are
started, they query the Azure queue for new work items. If a work item
is found, a simulation task is started, e.\,g.\ by running the requested
MD code with right parameters.  The number of
MD jobs per worker role depends on the size of the worker role --
Azure currently supports worker roles up to 8 cores.

% \alnote{Kalman Filter option: we need to convey how we make decisions?
% we need to understand the various trade-offs}

The ensemble use case can greatly benefit from the
capabilities of Azure. If greater accuracy is required or a deadline
must be met, it can seamlessly scale-out to more worker roles.  The
Azure fabric controller monitors all VMs running the worker roles and
automatically restarts the worker roles if necessary. Further, Azure
provides various kinds of reliable and scalable storage options to
express different coordination schemes, e.\,g.\ the master-worker
communication can be conducted via the described message queue.


\section{Usage Modes and Analysis}

The aim of this section is not to perform detailed systematic
performance measures and analysis, but to illustrate the
representative usage modes BigJob can support, and to explain how they
are supported.  We discuss two scenarios that are representative of
how applications would typically use multiple distributed
infrastructures. We focus our attention on replica-based Molecular Dynamics (MD)
simulations and a workload as \numrep replicas of a Hepatitis-C virus
(HCV) model, each replica running for 500 timesteps.  

\subsection{BigJob Startup Times} 
\up

% \alnote{Should we move this section to Usage Model and Analysis?}
% \jhanote{I think not. The performance referred to here is that of just
%   BigJob startup and dispatch time -- which in principle should be
%   independent of any usage mode/scenario. Right?}

Initially, we analyze the overheads resulting from the usage of BigJob across
different infrastructures. Typically, the main overhead when using
BigJob results from the queueing time at individual resources for
Grids and from the VM creation time for Cloud environments.
Figure~\ref{fig:performance_setup_time} compares the average startup times of
the different Pilot-Job backends for Grids, Condor pools and Clouds. Each 
experiment was repeated at least 10 times.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=0.45\textwidth]{performance/setup-times}
    \caption{\textbf{BigJob Startup Times:} In Grids the startup time
      greatly depends upon the queuing time at the local resource
      manager. However, in our experiments also Clouds showed a high
      fluctuation in the queueing time. Azure shows a slightly higher
      startup time than the science and the EC2 clouds.\up}
    \label{fig:performance_setup_time}
\end{figure}

Interestingly for the experiments conducted, the startup times for the
Cloud environments were observed to be larger than queue-waiting time
for the LONI resource Poseidon, which coincidentally was very lightly
loaded during the course of these experiments. Obviously, the startup
of a VM involves higher overheads than spawning a job on an already
running machine: a resource for the VM must be allocated, the VM must
be staged to the target and booted up. The following experiments will
also show that especially the already oversubscribed intra Cloud
network and thus, the staging of the VM can be a bottleneck. Also,
there is a large fluctuation in particular in the EC2 environment
probably caused by the fact that insufficient resources were available
at certain times.

\subsection{Running Replicas on different resource types}

\begin{figure}[htbp]
    \centering
        \includegraphics[width=0.48\textwidth]{performance/namd_run}
    \caption{\textbf{NAMD Runtimes on Different Resource Types: } The
          graph shows that the new EC2 cluster compute instances are 
          able to outperform other Cloud resources as well as traditional
          HPC resources as QB and Poseidon.}
    \label{fig:performance_namd_run}
\end{figure}

We run experiments in different Cloud environments: the Nimbus Science 
Cloud of the University of Chicago and the Amazon EC2 environment is used. In general, each Cloud has 
its characteristics. The Nimbus Cloud is accessed via the WSRF
interface, while for EC2 the Amazon command line client is used. 
Each Nimbus VM provides 2 virtual cores and 3.7\,GB memory. 
Amazon offeres different VM types with up to 8 cores. We used the largest 
VM type (m2.4xlarge) with 8 cores and 68.4\,GB of memory,
the m1.large instance type with 2 cores and 7.5\,GB  and the recently launched
cluster compute instances~\cite{ec2-cc}, which provide a pair of quad-core Intel 
X5570 (Nehalem) processors with 23\,GB memory. In contrast to other instance types,
cluster compute instances are connected using 10 Gbps Ethernet.


\begin{figure}[htbp]
    \centering
        \includegraphics[width=0.48\textwidth]{performance/namd_ec2_azure.pdf}
    \caption{\textbf{NAMD Performance on Azure and EC2:} In particular on smaller VM sizes (1-4 cores) Azure 
    outperforms EC2. The 8 core EC2 VM cc1.xlarge shows a slightly better performance than the Azure 8 core VM,
    however at a higher cost (1.16\,\$ in comparison to 0.96\,\$).}
    \label{fig:performance_namd_ec2_azure}
\end{figure}

Figure~\ref{fig:performance_namd_ec2_azure} presents an performance 
benchmark consisting of a NAMD simulation run on Azure and on comparable 
EC2 resources. For this purpose, an EC2 instance type with a similar core
count and price is chosen. Azure outperforms EC2 in most cases;
which is in noteworthy, since the costs for 2, 4 and 8 core VMs are
drastically lower on Azure. Since the underlying hardware is not known
one can only speculate about the reason. Microsoft controls the
hardware in its data center und optimizes its custom-built Azure
Hypervisor with respect to this hardware~\cite{Krishnan:2010nx}, which
could be a reason for the better performance. In summary, Azure offers
a good price/performance ratio in particular in comparison with EC2.

\subsection{Replica-Exchange on Azure}



\section{Conclusion and Future Directions}



\begin{acknowledgement} 
  \up \footnotesize{Important funding for SAGA has been provided by
    the UK EPSRC grant number GR/D0766171/1 (via OMII-UK) and HPCOPS
    NSF-OCI 0710874. SJ acknowledges the e-Science Institute,
    Edinburgh for supporting the research theme, ``Distributed
    Programming Abstractions'' and theme members for shaping many
    important ideas. This work has also been made possible thanks to
    the internal resources of the Center for Computation \& Technology
    at Louisiana State University and computer resources provided by
    LONI. We thank Yaakoub El Khamra for help with initial experiments
    with the Condor infrastructure. We thank Joohyun Kim (CCT) for
    assistance with the RNA models. We thank FutureGrid and
    ScienceCloud for Nimbus resources.}
\end{acknowledgement}

\up
\bibliographystyle{IEEEtran}
\bibliography{literatur,saga,cloud}
\end{document}



